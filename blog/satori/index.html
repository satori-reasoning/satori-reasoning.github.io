<!doctype html><html lang=en dir=auto><head><meta charset=utf-8><meta http-equiv=X-UA-Compatible content="IE=edge"><meta name=viewport content="width=device-width,initial-scale=1,shrink-to-fit=no"><meta name=robots content="index, follow"><title>Satori: Reinforcement Learning with Chain-of-Action-Thought Enhances LLM Reasoning via Autoregressive Search | Satori</title>
<meta name=keywords content><meta name=description content="Paper
Code
Data
Model

Introduction
Since the release of OpenAI&rsquo;s o1, significant efforts have been made within the research community to enhance open-source LLMs with advanced reasoning capabilities. This includes various approaches, including distillation using a strong teacher model, MCTS, and reward model guided search. This work aims to explore a new research direction: enabling LLMs with autoregressive search capabilities, i.e., a single LLM performs an extended reasoning process with self-reflection and self-exploration of new strategies. To achieve this, we develop a LLM post-training paradigm with several key concepts and ideas inspired by classical reinforcement learning (RL) communities. Our approach results in Satori, a 7B LLM trained on open-source model (Qwen-2.5-Math-7B) and open-source data (OpenMathInstruct-2 and NuminaMath). Key features of Satori include:"><meta name=author content="Satori Team"><link rel=canonical href=https://satori-reasoning.github.io/blog/satori/><link crossorigin=anonymous href=/assets/css/stylesheet.77f6a0e3bb5e5446160c7f304b2059b1a60df2bab26877fae59d14d15e68bac8.css integrity="sha256-d/ag47teVEYWDH8wSyBZsaYN8rqyaHf65Z0U0V5ousg=" rel="preload stylesheet" as=style><link rel=icon href=https://satori-reasoning.github.io/img/logo.png><link rel=icon type=image/png sizes=16x16 href=https://satori-reasoning.github.io/favicon-16x16.png><link rel=icon type=image/png sizes=32x32 href=https://satori-reasoning.github.io/favicon-32x32.png><link rel=apple-touch-icon href=https://satori-reasoning.github.io/apple-touch-icon.png><link rel=mask-icon href=https://satori-reasoning.github.io/safari-pinned-tab.svg><meta name=theme-color content="#2e2e33"><meta name=msapplication-TileColor content="#2e2e33"><link rel=alternate hreflang=en href=https://satori-reasoning.github.io/blog/satori/><noscript><style>#theme-toggle,.top-link{display:none}</style></noscript><script defer crossorigin=anonymous src=/js/custom.df2a5734071a3a99040f5e88e6d16d78358fbdef9a5e7389874ac5f2aa2ca86f.js integrity="sha256-3ypXNAcaOpkED16I5tFteDWPve+aXnOJh0rF8qosqG8="></script><link rel=stylesheet href=https://cdn.jsdelivr.net/npm/katex@0.16.3/dist/katex.min.css integrity=sha384-Juol1FqnotbkyZUT5Z7gUPjQ9gzlwCENvUZTpQBAPxtusdwFLRy382PSDx5UUJ4/ crossorigin=anonymous><script defer src=https://cdn.jsdelivr.net/npm/katex@0.16.3/dist/katex.min.js integrity=sha384-97gW6UIJxnlKemYavrqDHSX3SiygeOwIZhwyOKRfSaf0JWKRVj9hLASHgFTzT+0O crossorigin=anonymous></script><script defer src=https://cdn.jsdelivr.net/npm/katex@0.16.3/dist/contrib/auto-render.min.js integrity=sha384-+VBxd3r6XgURycqtZ117nYw44OOcIax56Z4dCRWbxyPt0Koah1uHoK0o4+/RRE05 crossorigin=anonymous></script><script>document.addEventListener("DOMContentLoaded",function(){renderMathInElement(document.body,{delimiters:[{left:"$$",right:"$$",display:!0},{left:"$",right:"$",display:!1},{left:"\\(",right:"\\)",display:!1},{left:"\\[",right:"\\]",display:!0}],throwOnError:!1})})</script><script async src="https://www.googletagmanager.com/gtag/js?id=G-BP9G7L138H"></script><script>var dnt,doNotTrack=!1;if(!1&&(dnt=navigator.doNotTrack||window.doNotTrack||navigator.msDoNotTrack,doNotTrack=dnt=="1"||dnt=="yes"),!doNotTrack){window.dataLayer=window.dataLayer||[];function gtag(){dataLayer.push(arguments)}gtag("js",new Date),gtag("config","G-BP9G7L138H")}</script><meta property="og:title" content="Satori: Reinforcement Learning with Chain-of-Action-Thought Enhances LLM Reasoning via Autoregressive Search"><meta property="og:description" content="Paper
Code
Data
Model

Introduction
Since the release of OpenAI&rsquo;s o1, significant efforts have been made within the research community to enhance open-source LLMs with advanced reasoning capabilities. This includes various approaches, including distillation using a strong teacher model, MCTS, and reward model guided search. This work aims to explore a new research direction: enabling LLMs with autoregressive search capabilities, i.e., a single LLM performs an extended reasoning process with self-reflection and self-exploration of new strategies. To achieve this, we develop a LLM post-training paradigm with several key concepts and ideas inspired by classical reinforcement learning (RL) communities. Our approach results in Satori, a 7B LLM trained on open-source model (Qwen-2.5-Math-7B) and open-source data (OpenMathInstruct-2 and NuminaMath). Key features of Satori include:"><meta property="og:type" content="article"><meta property="og:url" content="https://satori-reasoning.github.io/blog/satori/"><meta property="og:image" content="https://satori-reasoning.github.io/%3Clink%20or%20path%20of%20image%20for%20opengraph,%20twitter-cards%3E"><meta property="article:section" content="blog"><meta property="article:published_time" content="2025-02-01T12:00:00+08:00"><meta property="article:modified_time" content="2025-02-01T12:00:00+08:00"><meta property="og:site_name" content="Satori"><meta name=twitter:card content="summary_large_image"><meta name=twitter:image content="https://satori-reasoning.github.io/%3Clink%20or%20path%20of%20image%20for%20opengraph,%20twitter-cards%3E"><meta name=twitter:title content="Satori: Reinforcement Learning with Chain-of-Action-Thought Enhances LLM Reasoning via Autoregressive Search"><meta name=twitter:description content="Paper
Code
Data
Model

Introduction
Since the release of OpenAI&rsquo;s o1, significant efforts have been made within the research community to enhance open-source LLMs with advanced reasoning capabilities. This includes various approaches, including distillation using a strong teacher model, MCTS, and reward model guided search. This work aims to explore a new research direction: enabling LLMs with autoregressive search capabilities, i.e., a single LLM performs an extended reasoning process with self-reflection and self-exploration of new strategies. To achieve this, we develop a LLM post-training paradigm with several key concepts and ideas inspired by classical reinforcement learning (RL) communities. Our approach results in Satori, a 7B LLM trained on open-source model (Qwen-2.5-Math-7B) and open-source data (OpenMathInstruct-2 and NuminaMath). Key features of Satori include:"><script type=application/ld+json>{"@context":"https://schema.org","@type":"BreadcrumbList","itemListElement":[{"@type":"ListItem","position":1,"name":"Blog","item":"https://satori-reasoning.github.io/blog/"},{"@type":"ListItem","position":2,"name":"Satori: Reinforcement Learning with Chain-of-Action-Thought Enhances LLM Reasoning via Autoregressive Search","item":"https://satori-reasoning.github.io/blog/satori/"}]}</script><script type=application/ld+json>{"@context":"https://schema.org","@type":"BlogPosting","headline":"Satori: Reinforcement Learning with Chain-of-Action-Thought Enhances LLM Reasoning via Autoregressive Search","name":"Satori: Reinforcement Learning with Chain-of-Action-Thought Enhances LLM Reasoning via Autoregressive Search","description":"Paper Code Data Model\nIntroduction Since the release of OpenAI\u0026rsquo;s o1, significant efforts have been made within the research community to enhance open-source LLMs with advanced reasoning capabilities. This includes various approaches, including distillation using a strong teacher model, MCTS, and reward model guided search. This work aims to explore a new research direction: enabling LLMs with autoregressive search capabilities, i.e., a single LLM performs an extended reasoning process with self-reflection and self-exploration of new strategies. To achieve this, we develop a LLM post-training paradigm with several key concepts and ideas inspired by classical reinforcement learning (RL) communities. Our approach results in Satori, a 7B LLM trained on open-source model (Qwen-2.5-Math-7B) and open-source data (OpenMathInstruct-2 and NuminaMath). Key features of Satori include:\n","keywords":[],"articleBody":"Paper Code Data Model\nIntroduction Since the release of OpenAI’s o1, significant efforts have been made within the research community to enhance open-source LLMs with advanced reasoning capabilities. This includes various approaches, including distillation using a strong teacher model, MCTS, and reward model guided search. This work aims to explore a new research direction: enabling LLMs with autoregressive search capabilities, i.e., a single LLM performs an extended reasoning process with self-reflection and self-exploration of new strategies. To achieve this, we develop a LLM post-training paradigm with several key concepts and ideas inspired by classical reinforcement learning (RL) communities. Our approach results in Satori, a 7B LLM trained on open-source model (Qwen-2.5-Math-7B) and open-source data (OpenMathInstruct-2 and NuminaMath). Key features of Satori include:\nCapable of self-reflection and self-exploration without external guidance. Achieve state-of-the-art reasoning performance mainly through self-improvement (RL). Exhibit transferability of reasoning capabilities on unseen domains beyond math. Our Approach We formulate LLM reasoning as a sequential decision-making problem, where reasoning is a process of constructing and refining an answer step by step. Specifically, the LLM (agent’s policy) starts with an input context (initial state), generates a reasoning step (action), and updates the context (next state). The LLM repeats this process until it reaches a final answer, and receives a reward that evaluates whether the final answer matches the ground truth. With this formulation, we could train the LLM to reason using RL, aiming to generate a sequence of reasoning steps that maximize the expected reward.\nChain-of-Action-Thought reasoning (COAT) The key challenge of achieving autoregressive search is enabling the LLM to determine when to reflect, continue, or explore alternative solutions without external intervention. To enable this, we introduce several special meta-action tokens that guide the LLM’s reasoning process,\nContinue Reasoning (\u003c|continue|\u003e): encourages the LLM to build upon its current reasoning trajectory by generating the next intermediate step. Reflect (\u003c|reflect|\u003e): prompts the model to pause and verify the correctness of prior reasoning steps. Explore Alternative Solution (\u003c|explore|\u003e): signals the model to identify critical flaws in its reasoning and explore a new solution. We refer to this formulation as Chain-of-Action-Thought (COAT) reasoning. Each COAT reasoning step is a sequence of tokens, starting with one of the meta-action tokens.\nOverview of Training Framework A standard LLM is not able to perform COAT reasoning, and achieving this will encounter two key challenges:\nUnawareness of meta-action tokens: Without training, the LLM doesn’t recognize that encountering special meta-action tokens may require reflection or proposing alternative solutions. Long horizon and sparse rewards: Reasoning requires long-term decision-making with rewards only at the end, the LLM must take many correct reasoning steps before receiving rewards, and failures force it to restart from the initial state. This makes learning difficult because rewards are scarce, yet rewards are essential for driving RL progress. To address these two challenges, we propose a two-stage training framework*:\nA small-scale format tuning (FT) stage that helps the base LLM to internalize the COAT reasoning format. A large-scale self-improvement stage that utilizes reinforcement learning with “Restart and Explore” (RAE) techniques. *Note: our concurrent work DeepSeek-R1 adopts a similar training framework of small-scale cold-start SFT followed by large-scale RL training. Although both works coincide in this high-level idea, our work differs from R1 in key methodologies, including the data synthesis framework and RL algorithms.\nFormat Tuning Through Imitation Learning This stage aims to fine-tune a pre-trained base LLM to imitate a few demonstrated reasoning trajectories with COAT reasoning format. To synthesize such COAT trajectories that incorporate trials and errors, we propose a multi-agent data synthesis framework that leverages three LLMs:\nGenerator: Given an input problem, a generator generates multiple reasoning paths for a given input problem using classical CoT techniques. Critic: A critic evaluates the correctness of the reasoning paths generated by the generator, providing feedback to refine the reasoning and address suboptimal steps. Reward Model: A reward model assigns scores to the refined reasoning paths and selects the most effective path as the final demonstration trajectory. These three models collaborate to construct high-quality demonstration trajectories. We observe that a small number (10K) of demonstration trajectories is sufficient for base LLM to follow the COAT reasoning format.\nSelf-improvement via Reinforcement Learning Through format tuning, the LLM has adopted the COAT reasoning style but struggles to generalize to unseen problems. The RL stage aims to incentivize the actual capabilities of leveraging self-reflection to improve reasoning. We start with the format-tuned LLM and further optimize it using the classical PPO algorithm with two additional key strategies,\nRestart and Explore (RAE): Inspired by Go-Explore, we train the LLM policy to reason not only from the problem statement but also from intermediate steps sampled from past trajectories, both correct and incorrect. We also add exploration bonuses to encourage deeper reflection, further increasing opportunities for the policy to arrive at correct answers. Iterative Self-improvement: The policy might converge to a local sub-optimum and cannot further improve. Inspired by Kickstarting, after each round of RL training, we distill the knowledge of the current teacher policy into the student model (base LLM) through supervised fine-tuning. Starting from the newly fine-tuned LLM, we then perform another round of RL training. Benchmarking Performance Satori is evaluated on both in-domain reasoning benchmarks (math reasoning) and out-of-domain benchmarks (general reasoning tasks). All results are reported as the zero-shot pass@1 accuracy with greedy sampling.\nEvaluation Tasks Mathmatics Reasoning Benchmarks: GSM8K, MATH500, AMC2023, AIME2024, and OlympiadBench. Except for GSM8K, all other datasets feature competition-level problems. General Domain Reasoning Benchmarks: Logical Reasoning: FOLIO, BoardgameQA (BGQA). Code Reasoning: CRUXEval. Commonsense Reasoning: StrategyQA (STGQA). Tabular Reasoning: TableBench. Domain-specific Reasoning: MMLUPro STEM subsets (STEM), including physics, chemistry, computer science, engineering, biology, and economics. Math Reasoning Benchmarks Satori-Qwen-7B achieves SOTA performance and outperforms Qwen-2.5-Math-7B-Instruct which uses the same base model (Qwen-2.5-Math-7B). After round 2 training, Satori-Qwen-7B (Round 2) demonstrates even stronger performance on hard tasks.\nScale Model GSM8K MATH500 OlymBench AMC2023 AIME2024 AVG. Large Llama-3.1-70B-Instruct 94.1 68.0 29.4 42.5 13.3 49.5 OpenMath2-Llama3.1-70B 94.1 71.8 30.1 45.0 13.3 50.9 QwQ-32B-Preview 95.5 90.6 61.2 77.5 50.0 75.0 Small Llama-3.1-8b-Instruct 84.4 51.9 15.1 22.5 3.3 35.4 OpenMath2-Llama3.1-8B 90.5 67.8 28.9 37.5 6.7 46.3 NuminaMath-7B-CoT 78.9 54.6 15.9 20.0 10.0 35.9 Qwen-2.5-7B-Instruct 91.6 75.5 35.5 52.5 6.7 52.4 Qwen-2.5-Math-7B-Instruct 95.2 83.6 41.6 62.5 16.7 59.9 Satori-Qwen-7B 93.2 85.6 46.6 67.5 20.0 62.6 Satori-Qwen-7B (Round 2) 93.9 83.6 48.5 72.5 23.3 64.4 General Domain Reasoning Benchmarks Trained only on math datasets, Satori-Qwen-7B exhibits strong transferability across diverse out-of-domain reasoning benchmarks and outperforms Qwen-2.5-Math-7B-Instruct by a large margin. Moreover, despite not being trained in other domains, Satori-Qwen-7B achieves performance comparable to or exceeding other small-scale general instruct models.\nScale Model FOLIO BGQA CRUXEval StrategyQA TableBench STEM Avg. Large Llama-3.1-70B-Instruct 65.0 58.3 59.6 88.8 34.2 61.7 61.3 OpenMath2-Llama3.1-70B 68.5 68.7 35.1 95.6 46.8 15.1 55.0 QwQ-32B-Preview 84.2 71.1 65.2 88.2 51.5 71.3 71.9 Small Llama-3.1-8b-Instruct 63.5 50.3 38.5 92.2 32.4 43.4 53.4 OpenMath2-Llama3.1-8B 57.1 49.0 11.1 84.4 34.2 10.9 41.1 NuminaMath-7B-CoT 53.2 44.6 28.0 77.8 29.1 11.3 40.7 Qwen-2.5-7B-Instruct 72.4 53.0 58.1 91.3 43.2 57.1 62.5 Qwen-2.5-Math-7B-Instruct 68.9 51.3 28.0 85.3 36.2 45.2 52.5 Satori-Qwen-7B 71.4 61.8 42.5 86.3 43.4 56.7 60.4 Satori-Qwen-7B (Round 2) 72.9 58.5 41.1 90.4 44.6 57.4 60.8 Demo Examples Mathematical Reasoning (from AIME2024)\nEvery morning Aya goes for a $9$-kilometer-long walk and stops at a coffee shop afterwards. When she walks at a constant speed of $s$ kilometers per hour, the walk takes her 4 hours, including $t$ minutes spent in the coffee shop. When she walks $s+2$ kilometers per hour, the walk takes her 2 hours and 24 minutes, including $t$ minutes spent in the coffee shop. Suppose Aya walks at $s+\\frac{1}{2}$ kilometers per hour. Find the number of minutes the walk takes her, including the $t$ minutes spent in the coffee shop.\nFirst, we determine the time Aya spends walking when she walks at a speed of $ s $ kilometers per hour. The total time for the walk including the time spent in the coffee shop is 4 hours. Therefore, the time spent walking is: $$ 4 - \\frac{t}{60} \\text{ hours} $$\nSince the distance of the walk is 9 kilometers, we can write the equation:\n$$ s \\left( 4 - \\frac{t}{60} \\right) = 9 $$\nNext, we determine the time Aya spends walking when she walks at a speed of $ s + 2 $ kilometers per hour. The total time for the walk including the time spent in the coffee shop is 2 hours and 24 minutes, which is equivalent to 2.4 hours. Therefore, the time spent walking is:\n$$ 2.4 - \\frac{t}{60} \\text{ hours} $$\nSince the distance of the walk is 9 kilometers, we can write the equation:\n$$ (s + 2) \\left( 2.4 - \\frac{t}{60} \\right) = 9 $$ We now have a system of two equations:\n$$ s \\left( 4 - \\frac{t}{60} \\right) = 9 $$ $$ (s + 2) \\left( 2.4 - \\frac{t}{60} \\right) = 9 $$ Let’s solve these equations step by step. First, expand both equations:\n$$ 4s - \\frac{st}{60} = 9 $$\n$$ 2.4s + 4.8 - \\frac{st}{60} - \\frac{2t}{60} = 9 $$ Subtract the first equation from the second equation to eliminate $ \\frac{st}{60} $:\n$$ (2.4s + 4.8 - \\frac{st}{60} - \\frac{2t}{60}) - (4s - \\frac{st}{60}) = 9 - 9 $$ $$ 2.4s + 4.8 - 4s - \\frac{2t}{60} = 0 $$ $$ -1.6s + 4.8 - \\frac{2t}{60} = 0 $$ $$ -1.6s + 4.8 = \\frac{2t}{60} $$ $$ -96s + 288 = 2t $$$$ t = 144 - 48s $$ Substitute $ t = 144 - 48s $ back into the first equation:\n$$ s \\left( 4 - \\frac{144 - 48s}{60} \\right) = 9 $$ $$ s \\left( 4 - \\frac{144}{60} + \\frac{48s}{60} \\right) = 9 $$ $$ s \\left( 4 - 2.4 + \\frac{4s}{5} \\right) = 9 $$ $$ s \\left( 1.6 + \\frac{4s}{5} \\right) = 9 $$ $$ 1.6s + \\frac{4s^2}{5} = 9 $$ $$ 8s + \\frac{4s^2}{5} = 45 $$ $$ 40s + 4s^2 = 225 $$ $$ 4s^2 + 40s - 225 = 0 $$ Solve the quadratic equation using the quadratic formula $ s = \\frac{-b \\pm \\sqrt{b^2 - 4ac}}{2a} $, where $ a = 4 $, $ b = 40 $, and $ c = -225 $:\n$$ s = \\frac{-40 \\pm \\sqrt{40^2 - 4 \\cdot 4 \\cdot (-225)}}{2 \\cdot 4} $$ $$ s = \\frac{-40 \\pm \\sqrt{1600 + 3600}}{8} $$ $$ s = \\frac{-40 \\pm \\sqrt{5200}}{8} $$ $$ s = \\frac{-40 \\pm 20\\sqrt{13}}{8} $$ $$ s = \\frac{-10 \\pm 5\\sqrt{13}}{2} $$\nSince $ s $ must be positive, we take the positive root:$ s = \\frac{-10 + 5\\sqrt{13}}{2} $ Now, we need to find the time Aya spends walking when she walks at a speed of $ s + \\frac{1}{2} $ kilometers per hour. The time spent walking is:\n$$ \\frac{9}{s + \\frac{1}{2}} $$\nSubstitute $ s = \\frac{-10 + 5\\sqrt{13}}{2} $:\n$$ s + \\frac{1}{2} = \\frac{-10 + 5\\sqrt{13}}{2} + \\frac{1}{2} = \\frac{-9 + 5\\sqrt{13}}{2} $$\nThe time spent walking is:\n$$ \\frac{9}{\\frac{-9 + 5\\sqrt{13}}{2}} = \\frac{18}{-9 + 5\\sqrt{13}} $$Rationalize the denominator:\n$$ \\begin{align*} \u0026\\frac{18}{-9 + 5\\sqrt{13}} \\cdot \\frac{-9 - 5\\sqrt{13}}{-9 - 5\\sqrt{13}} \\ \u0026= \\frac{18(-9 - 5\\sqrt{13})}{81 - 325} \\ \u0026= \\frac{18(-9 - 5\\sqrt{13})}{-244} \\ \u0026= \\frac{9(9 + 5\\sqrt{13})}{122} = \\frac{81 + 45\\sqrt{13}}{122} \\end{align*} $$ The total time for the walk including the time spent in the coffee shop is:\n$$ [ \\frac{81 + 45\\sqrt{13}}{122} + \\frac{144 - 48 \\cdot \\frac{-10 + 5\\sqrt{13}}{2}}{60} ] $$\nAfter simplifying, we find that the total time is 3 hours and 12 minutes. Therefore, the number of minutes the walk takes her, including the $ t $ minutes spent in the coffee shop, is:$$ \\boxed{192} $$\nLet's reassess my previous conclusion. The approach to solving the problem is overly complicated and involves unnecessary steps, such as solving a system of equations with time spent in the coffee shop included. Let's switch to a new solution. First, calculate the walking times for the given speeds without considering the time spent in the coffee shop, and then use the difference in total times to find the time spent walking at the new speed. Let $ t_w $ be the time Aya spends walking when she walks at $ s ) kilometers per hour. We know:\n$$ s \\cdot t_w = 9 $$ $$ t_w = \\frac{9}{s} $$ When she walks at $ s + 2 $ kilometers per hour, the time spent walking is:\n$$ \\frac{9}{s+2} $$ The difference in walking times is:\n$$ \\frac{9}{s} - \\frac{9}{s+2} = 4 - 2.4 = 1.6 $$ Simplify the left side:\n$$ \\frac{9(s+2) - 9s}{s(s+2)} = \\frac{18}{s(s+2)} = 1.6 $$ $$ s(s+2) = \\frac{18}{1.6} = 11.25 $$ $$ s^2 + 2s - 11.25 = 0 $$\nSolve the quadratic equation using the quadratic formula $$ s = \\frac{-b \\pm \\sqrt{b^2 - 4ac}}{2a} $$ :\n$$ s = \\frac{-2 \\pm \\sqrt{4 + 45}}{2} = \\frac{-2 \\pm 7}{2} $$\nSince $ s $ must be positive, we have:\n$$ s = \\frac{5}{2} = 2.5 $$ Now, find the time spent walking at $ s + \\frac{1}{2} = 3 $ kilometers per hour:\n$$ \\text{Time spent walking} = \\frac{9}{3} = 3 \\text{ hours} $$ Convert 3 hours to minutes:$$ 3 \\text{ hours} = 180 \\text{ minutes} $$\nSince $ t $ minutes are spent in the coffee shop, the total time is:$$ 180 + t $$\nFrom the original equations, we know:\n$$ 2.5 \\left( 4 - \\frac{t}{60} \\right) = 9 $$ $$ 10 - \\frac{2.5t}{60} = 9 $$ $$ \\frac{2.5t}{60} = 1 $$ $$ 2.5t = 60 $$ $$ t = 24 $$ Therefore, the total time is:\n$$ 180 + 24 = 204 $$\nThe number of minutes the walk takes her, including the ( t ) minutes spent in the coffee shop, is:\n$$ \\boxed{204} $$\nCommitment to Open-Source Community We aim to contribute to open-source communities by supporting the development of new research methods that can enhance LLM reasoning capabilities. We plan to provide the following resources:\nTechnique details (our research paper) Synthetic data for format tuning Reinforcement learning (RL) training data Code for multi-agent data synthesis Code for RL training Satori Team Members Core Contributors Maohao Shen, MIT Guangtao Zeng, SUTD Zhenting Qi, Harvard Contributors *: Project lead\nZhang-Wei Hong, MIT Zhenfang Chen, MIT-IBM Watson AI Lab Wei Lu, SUTD Gregory W. Wornell, MIT Subhro Das, MIT-IBM Watson AI Lab David Cox, MIT-IBM Watson AI Lab Chuang Gan*, UMass, MIT-IBM Watson AI Lab Contact Information For questions or collaboration, please:\nRaise an issue in our GitHub repository Contact us at: satori2025@outlook.com Citation (TODO)\n@article{TBD, title={Satori: Reinforcement Learning with Chain-of-Action-Thought Enhances LLM Reasoning via Autoregressive Search}, author={TBD}, journal={TBD}, year={2025} } ","wordCount":"2440","inLanguage":"en","datePublished":"2025-02-01T12:00:00+08:00","dateModified":"2025-02-01T12:00:00+08:00","author":{"@type":"Person","name":"Satori Team"},"mainEntityOfPage":{"@type":"WebPage","@id":"https://satori-reasoning.github.io/blog/satori/"},"publisher":{"@type":"Organization","name":"Satori","logo":{"@type":"ImageObject","url":"https://satori-reasoning.github.io/img/logo.png"}}}</script></head><body id=top><script>const hasHeaderBg=!1</script><header class=header><div class=nav-container><nav class=nav><div class=logo><a href=/ accesskey=h title="Satori (Alt + H)"></a><div class=logo-switches></div></div><ul id=menu><li><a href=/ title=Home><span>Home</span></a></li><li><a href=/resources title=Resources><span>Resources</span></a></li><li><a href=/blog/ title=Blog><span>Blog</span></a></li></ul></nav></div></header><div class=hero-container><div class=hero><h1 class=post-title>Satori: Reinforcement Learning with Chain-of-Action-Thought Enhances LLM Reasoning via Autoregressive Search</h1><div class=post-meta><span title='2025-02-01 12:00:00 +0800 +0800'>February 1, 2025</span>&nbsp;·&nbsp;12 min&nbsp;·&nbsp;2440 words&nbsp;·&nbsp;Satori Team</div></div></div><main class=main><article class=post-single><div class=post-content><p><a href class="btn external" target=_blank>Paper</a>
<a href class="btn external" target=_blank>Code</a>
<a href class="btn external" target=_blank>Data</a>
<a href class="btn external" target=_blank>Model</a></p><style>.code-box{max-height:300px;overflow-y:auto;padding:10px;background-color:#f8f8f8;font-family:Consolas,Monaco,courier new,monospace;font-size:14px;line-height:1.5;white-space:pre-wrap;word-wrap:break-word;border-radius:5px}</style><h2 id=introduction><strong>Introduction</strong><a hidden class=anchor aria-hidden=true href=#introduction>#</a></h2><p>Since the release of OpenAI&rsquo;s o1, significant efforts have been made within the research community to enhance open-source LLMs with advanced reasoning capabilities. This includes various approaches, including distillation using a strong teacher model, MCTS, and reward model guided search. This work aims to explore a new research direction: enabling LLMs with autoregressive search capabilities, i.e., a single LLM performs an extended reasoning process with self-reflection and self-exploration of new strategies. To achieve this, we develop a LLM post-training paradigm with several key concepts and ideas inspired by classical reinforcement learning (RL) communities. Our approach results in Satori, a 7B LLM trained on open-source model (Qwen-2.5-Math-7B) and open-source data (OpenMathInstruct-2 and NuminaMath). Key features of Satori include:</p><ul><li>Capable of self-reflection and self-exploration without external guidance.</li><li>Achieve state-of-the-art reasoning performance mainly through self-improvement (RL).</li><li>Exhibit transferability of reasoning capabilities on unseen domains beyond math.</li></ul><h2 id=our-approach><strong>Our Approach</strong><a hidden class=anchor aria-hidden=true href=#our-approach>#</a></h2><p>We formulate LLM reasoning as a sequential decision-making problem, where reasoning is a process of constructing and refining an answer step by step. Specifically, the LLM (agent&rsquo;s policy) starts with an input context (initial state), generates a reasoning step (action), and updates the context (next state). The LLM repeats this process until it reaches a final answer, and receives a reward that evaluates whether the final answer matches the ground truth. With this formulation, we could train the LLM to reason using RL, aiming to generate a sequence of reasoning steps that maximize the expected reward.</p><h3 id=chain-of-action-thought-reasoning-coat><strong>Chain-of-Action-Thought reasoning (COAT)</strong><a hidden class=anchor aria-hidden=true href=#chain-of-action-thought-reasoning-coat>#</a></h3><p>The key challenge of achieving autoregressive search is enabling the LLM to determine when to reflect, continue, or explore alternative solutions without external intervention.
To enable this, we introduce several special meta-action tokens that guide the LLM&rsquo;s reasoning process,</p><ul><li><strong>Continue Reasoning</strong> (&lt;|continue|>): encourages the LLM to build upon its current reasoning trajectory by generating the next intermediate step.</li><li><strong>Reflect</strong> (&lt;|reflect|>): prompts the model to pause and verify the correctness of prior reasoning steps.</li><li><strong>Explore Alternative Solution</strong> (&lt;|explore|>): signals the model to identify critical flaws in its reasoning and explore a new solution.</li></ul><p>We refer to this formulation as Chain-of-Action-Thought (COAT) reasoning. Each COAT reasoning step is a sequence of tokens, starting with one of the meta-action tokens.</p><h3 id=overview-of-training-framework><strong>Overview of Training Framework</strong><a hidden class=anchor aria-hidden=true href=#overview-of-training-framework>#</a></h3><p>A standard LLM is not able to perform COAT reasoning, and achieving this will encounter two key challenges:</p><ul><li><strong>Unawareness of meta-action tokens</strong>: Without training, the LLM doesn&rsquo;t recognize that encountering special meta-action tokens may require reflection or proposing alternative solutions.</li><li><strong>Long horizon and sparse rewards</strong>: Reasoning requires long-term decision-making with rewards only at the end, the LLM must take many correct reasoning steps before receiving rewards, and failures force it to restart from the initial state. This makes learning difficult because rewards are scarce, yet rewards are essential for driving RL progress.</li></ul><p>To address these two challenges, we propose a two-stage training framework*:</p><ol><li>A small-scale format tuning (FT) stage that helps the base LLM to internalize the COAT reasoning format.</li><li>A large-scale self-improvement stage that utilizes reinforcement learning with &ldquo;Restart and Explore&rdquo; (RAE) techniques.
<img loading=lazy src=/img/schematic.png alt=schematic></li></ol><p>*<strong>Note</strong>: <em>our concurrent work <a href=https://arxiv.org/abs/2501.12948>DeepSeek-R1</a> adopts a similar training framework of small-scale cold-start SFT followed by large-scale RL training. Although both works coincide in this high-level idea, our work differs from R1 in key methodologies, including the data synthesis framework and RL algorithms.</em></p><h3 id=format-tuning-through-imitation-learning><strong>Format Tuning Through Imitation Learning</strong><a hidden class=anchor aria-hidden=true href=#format-tuning-through-imitation-learning>#</a></h3><p>This stage aims to fine-tune a pre-trained base LLM to imitate a few demonstrated reasoning trajectories with COAT reasoning format. To synthesize such COAT trajectories that incorporate trials
and errors, we propose a multi-agent data synthesis framework that leverages three LLMs:</p><ul><li><strong>Generator</strong>: Given an input problem, a generator generates multiple reasoning paths for a given input problem using classical CoT techniques.</li><li><strong>Critic</strong>: A critic evaluates the correctness of the reasoning paths generated by the generator, providing feedback to refine the reasoning and address suboptimal steps.</li><li><strong>Reward Model</strong>: A reward model assigns scores to the refined reasoning paths and selects the most effective path as the final demonstration trajectory.</li></ul><p>These three models collaborate to construct high-quality demonstration trajectories. We observe that a small number (10K) of demonstration trajectories is sufficient for base LLM to follow the COAT reasoning format.</p><h3 id=self-improvement-via-reinforcement-learning><strong>Self-improvement via Reinforcement Learning</strong><a hidden class=anchor aria-hidden=true href=#self-improvement-via-reinforcement-learning>#</a></h3><p>Through format tuning, the LLM has adopted the COAT reasoning style but struggles to generalize to unseen problems. The RL stage aims to incentivize the actual capabilities of leveraging self-reflection to improve reasoning. We start with the format-tuned LLM and further optimize it using the classical PPO algorithm with two additional key strategies,</p><ul><li><strong>Restart and Explore (RAE)</strong>: Inspired by <a href=https://arxiv.org/abs/1901.10995>Go-Explore</a>, we train the LLM policy to reason not only from the problem statement but also from intermediate steps sampled from past trajectories, both correct and incorrect. We also add exploration bonuses to encourage deeper reflection, further increasing opportunities for the policy to arrive at correct answers.</li><li><strong>Iterative Self-improvement</strong>: The policy might converge to a local sub-optimum and cannot further improve. Inspired by <a href=https://arxiv.org/abs/1803.03835>Kickstarting</a>, after each round of RL training, we distill the knowledge of the current teacher policy into the student model (base LLM) through supervised fine-tuning. Starting from the newly fine-tuned LLM, we then perform another round of RL training.</li></ul><h2 id=benchmarking-performance><strong>Benchmarking Performance</strong><a hidden class=anchor aria-hidden=true href=#benchmarking-performance>#</a></h2><p>Satori is evaluated on both in-domain reasoning benchmarks (math reasoning) and out-of-domain benchmarks (general reasoning tasks). All results are reported as the zero-shot pass@1 accuracy with greedy sampling.</p><h3 id=evaluation-tasks><strong>Evaluation Tasks</strong><a hidden class=anchor aria-hidden=true href=#evaluation-tasks>#</a></h3><ul><li><strong>Mathmatics Reasoning Benchmarks</strong>: <a href=https://huggingface.co/datasets/openai/gsm8k>GSM8K</a>, <a href=https://arxiv.org/abs/2305.20050>MATH500</a>, AMC2023, AIME2024, and <a href=https://github.com/OpenBMB/OlympiadBench>OlympiadBench</a>. Except for GSM8K, all other datasets feature competition-level problems.</li><li><strong>General Domain Reasoning Benchmarks</strong>:<ul><li>Logical Reasoning: <a href=https://arxiv.org/abs/2209.00840>FOLIO</a>, <a href=https://arxiv.org/abs/2306.07934>BoardgameQA (BGQA)</a>.</li><li>Code Reasoning: <a href=https://arxiv.org/abs/2401.03065>CRUXEval</a>.</li><li>Commonsense Reasoning: <a href=https://arxiv.org/abs/2101.02235>StrategyQA (STGQA)</a>.</li><li>Tabular Reasoning: <a href=https://arxiv.org/abs/2408.09174>TableBench</a>.</li><li>Domain-specific Reasoning: <a href=https://arxiv.org/abs/2406.01574>MMLUPro</a> STEM subsets (STEM), including physics, chemistry, computer science, engineering, biology, and economics.</li></ul></li></ul><h3 id=math-reasoning-benchmarks><strong>Math Reasoning Benchmarks</strong><a hidden class=anchor aria-hidden=true href=#math-reasoning-benchmarks>#</a></h3><p>Satori-Qwen-7B achieves SOTA performance and outperforms Qwen-2.5-Math-7B-Instruct which uses the same base model (Qwen-2.5-Math-7B). After round 2 training, Satori-Qwen-7B (Round 2)
demonstrates even stronger performance on hard tasks.</p><table><thead><tr><th>Scale</th><th>Model</th><th>GSM8K</th><th>MATH500</th><th>OlymBench</th><th>AMC2023</th><th>AIME2024</th><th>AVG.</th></tr></thead><tbody><tr><td><strong>Large</strong></td><td>Llama-3.1-70B-Instruct</td><td>94.1</td><td>68.0</td><td>29.4</td><td>42.5</td><td>13.3</td><td>49.5</td></tr><tr><td></td><td>OpenMath2-Llama3.1-70B</td><td>94.1</td><td>71.8</td><td>30.1</td><td>45.0</td><td>13.3</td><td>50.9</td></tr><tr><td></td><td>QwQ-32B-Preview</td><td>95.5</td><td>90.6</td><td>61.2</td><td>77.5</td><td>50.0</td><td>75.0</td></tr><tr><td><strong>Small</strong></td><td>Llama-3.1-8b-Instruct</td><td>84.4</td><td>51.9</td><td>15.1</td><td>22.5</td><td>3.3</td><td>35.4</td></tr><tr><td></td><td>OpenMath2-Llama3.1-8B</td><td>90.5</td><td>67.8</td><td>28.9</td><td>37.5</td><td>6.7</td><td>46.3</td></tr><tr><td></td><td>NuminaMath-7B-CoT</td><td>78.9</td><td>54.6</td><td>15.9</td><td>20.0</td><td>10.0</td><td>35.9</td></tr><tr><td></td><td>Qwen-2.5-7B-Instruct</td><td>91.6</td><td>75.5</td><td>35.5</td><td>52.5</td><td>6.7</td><td>52.4</td></tr><tr><td></td><td>Qwen-2.5-Math-7B-Instruct</td><td>95.2</td><td>83.6</td><td>41.6</td><td>62.5</td><td>16.7</td><td>59.9</td></tr><tr><td></td><td><strong>Satori-Qwen-7B</strong></td><td>93.2</td><td>85.6</td><td>46.6</td><td>67.5</td><td>20.0</td><td><strong>62.6</strong></td></tr><tr><td></td><td><strong>Satori-Qwen-7B (Round 2)</strong></td><td>93.9</td><td>83.6</td><td>48.5</td><td>72.5</td><td>23.3</td><td><strong>64.4</strong></td></tr></tbody></table><h3 id=general-domain-reasoning-benchmarks><strong>General Domain Reasoning Benchmarks</strong><a hidden class=anchor aria-hidden=true href=#general-domain-reasoning-benchmarks>#</a></h3><p>Trained <strong>only on math</strong> datasets, Satori-Qwen-7B exhibits strong transferability across diverse out-of-domain reasoning benchmarks and outperforms Qwen-2.5-Math-7B-Instruct by a large margin.
Moreover, despite not being trained in other domains, Satori-Qwen-7B achieves performance comparable to or exceeding other small-scale general instruct models.</p><table><thead><tr><th>Scale</th><th>Model</th><th>FOLIO</th><th>BGQA</th><th>CRUXEval</th><th>StrategyQA</th><th>TableBench</th><th>STEM</th><th>Avg.</th></tr></thead><tbody><tr><td><strong>Large</strong></td><td>Llama-3.1-70B-Instruct</td><td>65.0</td><td>58.3</td><td>59.6</td><td>88.8</td><td>34.2</td><td>61.7</td><td>61.3</td></tr><tr><td></td><td>OpenMath2-Llama3.1-70B</td><td>68.5</td><td>68.7</td><td>35.1</td><td>95.6</td><td>46.8</td><td>15.1</td><td>55.0</td></tr><tr><td></td><td>QwQ-32B-Preview</td><td>84.2</td><td>71.1</td><td>65.2</td><td>88.2</td><td>51.5</td><td>71.3</td><td>71.9</td></tr><tr><td><strong>Small</strong></td><td>Llama-3.1-8b-Instruct</td><td>63.5</td><td>50.3</td><td>38.5</td><td>92.2</td><td>32.4</td><td>43.4</td><td>53.4</td></tr><tr><td></td><td>OpenMath2-Llama3.1-8B</td><td>57.1</td><td>49.0</td><td>11.1</td><td>84.4</td><td>34.2</td><td>10.9</td><td>41.1</td></tr><tr><td></td><td>NuminaMath-7B-CoT</td><td>53.2</td><td>44.6</td><td>28.0</td><td>77.8</td><td>29.1</td><td>11.3</td><td>40.7</td></tr><tr><td></td><td>Qwen-2.5-7B-Instruct</td><td>72.4</td><td>53.0</td><td>58.1</td><td>91.3</td><td>43.2</td><td>57.1</td><td><strong>62.5</strong></td></tr><tr><td></td><td>Qwen-2.5-Math-7B-Instruct</td><td>68.9</td><td>51.3</td><td>28.0</td><td>85.3</td><td>36.2</td><td>45.2</td><td>52.5</td></tr><tr><td></td><td><strong>Satori-Qwen-7B</strong></td><td>71.4</td><td>61.8</td><td>42.5</td><td>86.3</td><td>43.4</td><td>56.7</td><td>60.4</td></tr><tr><td></td><td><strong>Satori-Qwen-7B (Round 2)</strong></td><td>72.9</td><td>58.5</td><td>41.1</td><td>90.4</td><td>44.6</td><td>57.4</td><td><strong>60.8</strong></td></tr></tbody></table><h2 id=demo-examples><strong>Demo Examples</strong><a hidden class=anchor aria-hidden=true href=#demo-examples>#</a></h2><p>Mathematical Reasoning (from AIME2024)</p><p>Every morning Aya goes for a $9$-kilometer-long walk and stops at a coffee shop afterwards. When she walks at a constant speed of $s$ kilometers per hour, the walk takes her 4 hours, including $t$ minutes spent in the coffee shop. When she walks $s+2$ kilometers per hour, the walk takes her 2 hours and 24 minutes, including $t$ minutes spent in the coffee shop. Suppose Aya walks at $s+\frac{1}{2}$ kilometers per hour. Find the number of minutes the walk takes her, including the $t$ minutes spent in the coffee shop.</p><div class=code-box>First, we determine the time Aya spends walking when she walks at a speed of $ s $ kilometers per hour. The total time for the walk including the time spent in the coffee shop is 4 hours. Therefore, the time spent walking is:<p>$$ 4 - \frac{t}{60} \text{ hours} $$</p><p>Since the distance of the walk is 9 kilometers, we can write the equation:</p><p>$$ s \left( 4 - \frac{t}{60} \right) = 9 $$</p><p>Next, we determine the time Aya spends walking when she walks at a speed of $ s + 2 $ kilometers per hour. The total time for the walk including the time spent in the coffee shop is 2 hours and 24 minutes, which is equivalent to 2.4 hours. Therefore, the time spent walking is:</p><p>$$ 2.4 - \frac{t}{60} \text{ hours} $$</p><p>Since the distance of the walk is 9 kilometers, we can write the equation:</p><p>$$ (s + 2) \left( 2.4 - \frac{t}{60} \right) = 9 $$
We now have a system of two equations:</p><ol><li>$$ s \left( 4 - \frac{t}{60} \right) = 9 $$</li><li>$$ (s + 2) \left( 2.4 - \frac{t}{60} \right) = 9 $$</li></ol><p>Let&rsquo;s solve these equations step by step. First, expand both equations:</p><ol><li><p>$$ 4s - \frac{st}{60} = 9 $$</p></li><li><p>$$ 2.4s + 4.8 - \frac{st}{60} - \frac{2t}{60} = 9 $$
Subtract the first equation from the second equation to eliminate $ \frac{st}{60} $:</p></li></ol><p>$$ (2.4s + 4.8 - \frac{st}{60} - \frac{2t}{60}) - (4s - \frac{st}{60}) = 9 - 9 $$
$$ 2.4s + 4.8 - 4s - \frac{2t}{60} = 0 $$
$$ -1.6s + 4.8 - \frac{2t}{60} = 0 $$
$$ -1.6s + 4.8 = \frac{2t}{60} $$
$$ -96s + 288 = 2t $$$$ t = 144 - 48s $$
Substitute $ t = 144 - 48s $ back into the first equation:</p><p>$$ s \left( 4 - \frac{144 - 48s}{60} \right) = 9 $$
$$ s \left( 4 - \frac{144}{60} + \frac{48s}{60} \right) = 9 $$
$$ s \left( 4 - 2.4 + \frac{4s}{5} \right) = 9 $$
$$ s \left( 1.6 + \frac{4s}{5} \right) = 9 $$
$$ 1.6s + \frac{4s^2}{5} = 9 $$
$$ 8s + \frac{4s^2}{5} = 45 $$
$$ 40s + 4s^2 = 225 $$
$$ 4s^2 + 40s - 225 = 0 $$
Solve the quadratic equation using the quadratic formula $ s = \frac{-b \pm \sqrt{b^2 - 4ac}}{2a} $, where $ a = 4 $, $ b = 40 $, and $ c = -225 $:</p><p>$$
s = \frac{-40 \pm \sqrt{40^2 - 4 \cdot 4 \cdot (-225)}}{2 \cdot 4} $$
$$ s = \frac{-40 \pm \sqrt{1600 + 3600}}{8} $$
$$ s = \frac{-40 \pm \sqrt{5200}}{8} $$
$$ s = \frac{-40 \pm 20\sqrt{13}}{8} $$
$$ s = \frac{-10 \pm 5\sqrt{13}}{2} $$</p><p>Since $ s $ must be positive, we take the positive root:$ s = \frac{-10 + 5\sqrt{13}}{2} $
Now, we need to find the time Aya spends walking when she walks at a speed of $ s + \frac{1}{2} $ kilometers per hour. The time spent walking is:</p><p>$$ \frac{9}{s + \frac{1}{2}} $$</p><p>Substitute $ s = \frac{-10 + 5\sqrt{13}}{2} $:</p><p>$$ s + \frac{1}{2} = \frac{-10 + 5\sqrt{13}}{2} + \frac{1}{2} = \frac{-9 + 5\sqrt{13}}{2} $$</p><p>The time spent walking is:</p><p>$$ \frac{9}{\frac{-9 + 5\sqrt{13}}{2}} = \frac{18}{-9 + 5\sqrt{13}} $$Rationalize the denominator:</p><p>$$
\begin{align*}
&\frac{18}{-9 + 5\sqrt{13}} \cdot \frac{-9 - 5\sqrt{13}}{-9 - 5\sqrt{13}} \
&= \frac{18(-9 - 5\sqrt{13})}{81 - 325} \
&= \frac{18(-9 - 5\sqrt{13})}{-244} \
&= \frac{9(9 + 5\sqrt{13})}{122} = \frac{81 + 45\sqrt{13}}{122}
\end{align*}
$$
The total time for the walk including the time spent in the coffee shop is:</p><p>$$ [ \frac{81 + 45\sqrt{13}}{122} + \frac{144 - 48 \cdot \frac{-10 + 5\sqrt{13}}{2}}{60} ] $$</p><p>After simplifying, we find that the total time is 3 hours and 12 minutes. Therefore, the number of minutes the walk takes her, including the $ t $ minutes spent in the coffee shop, is:$$ \boxed{192} $$</p><p><span style=color:#b8860b>Let's reassess my previous conclusion. The approach to solving the problem is overly complicated and involves unnecessary steps, such as solving a system of equations with time spent in the coffee shop included.</span></p><p><span style=color:blue>Let's switch to a new solution. First, calculate the walking times for the given speeds without considering the time spent in the coffee shop, and then use the difference in total times to find the time spent walking at the new speed.</span></p><p>Let $ t_w $ be the time Aya spends walking when she walks at $ s ) kilometers per hour. We know:</p><p>$$ s \cdot t_w = 9 $$
$$ t_w = \frac{9}{s} $$
When she walks at $ s + 2 $ kilometers per hour, the time spent walking is:</p><p>$$ \frac{9}{s+2} $$
The difference in walking times is:</p><p>$$ \frac{9}{s} - \frac{9}{s+2} = 4 - 2.4 = 1.6 $$
Simplify the left side:</p><p>$$ \frac{9(s+2) - 9s}{s(s+2)} = \frac{18}{s(s+2)} = 1.6 $$
$$ s(s+2) = \frac{18}{1.6} = 11.25 $$
$$ s^2 + 2s - 11.25 = 0 $$</p><p>Solve the quadratic equation using the quadratic formula $$ s = \frac{-b \pm \sqrt{b^2 - 4ac}}{2a} $$ :</p><p>$$ s = \frac{-2 \pm \sqrt{4 + 45}}{2} = \frac{-2 \pm 7}{2} $$</p><p>Since $ s $ must be positive, we have:</p><p>$$ s = \frac{5}{2} = 2.5 $$
Now, find the time spent walking at $ s + \frac{1}{2} = 3 $ kilometers per hour:</p><p>$$ \text{Time spent walking} = \frac{9}{3} = 3 \text{ hours} $$
Convert 3 hours to minutes:$$ 3 \text{ hours} = 180 \text{ minutes} $$</p><p>Since $ t $ minutes are spent in the coffee shop, the total time is:$$ 180 + t $$</p><p>From the original equations, we know:</p><p>$$ 2.5 \left( 4 - \frac{t}{60} \right) = 9 $$
$$ 10 - \frac{2.5t}{60} = 9 $$
$$ \frac{2.5t}{60} = 1 $$ $$ 2.5t = 60 $$
$$ t = 24 $$ Therefore, the total time is:</p><p>$$ 180 + 24 = 204 $$</p><p>The number of minutes the walk takes her, including the ( t ) minutes spent in the coffee shop, is:</p><p>$$ \boxed{204} $$</p></div><h2 id=commitment-to-open-source-community><strong>Commitment to Open-Source Community</strong><a hidden class=anchor aria-hidden=true href=#commitment-to-open-source-community>#</a></h2><p>We aim to contribute to open-source communities by supporting the development of new research methods that can enhance LLM reasoning capabilities. We plan to provide the following resources:</p><ul><li>Technique details (our research paper)</li><li>Synthetic data for format tuning</li><li>Reinforcement learning (RL) training data</li><li>Code for multi-agent data synthesis</li><li>Code for RL training</li></ul><h2 id=satori-team-members><strong>Satori Team Members</strong><a hidden class=anchor aria-hidden=true href=#satori-team-members>#</a></h2><h3 id=core-contributors><strong>Core Contributors</strong><a hidden class=anchor aria-hidden=true href=#core-contributors>#</a></h3><ul><li><a href=https://maohaos2.github.io/Maohao/>Maohao Shen</a>, MIT</li><li><a href=https://chaoscodes.github.io/>Guangtao Zeng</a>, SUTD</li><li><a href=https://zhentingqi.github.io/>Zhenting Qi</a>, Harvard</li></ul><h3 id=contributors><strong>Contributors</strong><a hidden class=anchor aria-hidden=true href=#contributors>#</a></h3><p>*: Project lead</p><ul><li>Zhang-Wei Hong, MIT</li><li>Zhenfang Chen, MIT-IBM Watson AI Lab</li><li>Wei Lu, SUTD</li><li>Gregory W. Wornell, MIT</li><li>Subhro Das, MIT-IBM Watson AI Lab</li><li>David Cox, MIT-IBM Watson AI Lab</li><li>Chuang Gan*, UMass, MIT-IBM Watson AI Lab</li></ul><h2 id=contact-information><strong>Contact Information</strong><a hidden class=anchor aria-hidden=true href=#contact-information>#</a></h2><p>For questions or collaboration, please:</p><ul><li>Raise an issue in our GitHub repository</li><li>Contact us at:<ul><li><a href=mailto:satori2025@outlook.com>satori2025@outlook.com</a></li></ul></li></ul><h2 id=citation><strong>Citation</strong><a hidden class=anchor aria-hidden=true href=#citation>#</a></h2><p>(TODO)</p><pre tabindex=0><code>@article{TBD,
  title={Satori: Reinforcement Learning with Chain-of-Action-Thought Enhances LLM Reasoning via Autoregressive Search},
  author={TBD},
  journal={TBD},
  year={2025}
}
</code></pre></div></article></main><footer class=footer><span>&copy; 2025 <a href=https://satori-reasoning.github.io/>Satori</a></span>
<span>Powered by
<a href=https://gohugo.io/ rel="noopener noreferrer" target=_blank>Hugo</a>
</span><span>Acknowledgment to
<a href=https://qwenlm.github.io/ rel="noopener noreferrer" target=_blank>Qwen</a></span></footer><a href=#top aria-label="go to top" title="Go to Top (Alt + G)" class=top-link id=top-link accesskey=g><svg viewBox="0 0 12 8" fill="currentcolor"><path d="M12 8H0l6-8z"/></svg>
</a><script>let menu=document.getElementById("menu");menu&&(menu.scrollLeft=localStorage.getItem("menu-scroll-position"),menu.onscroll=function(){localStorage.setItem("menu-scroll-position",menu.scrollLeft)}),document.querySelectorAll('a[href^="#"]').forEach(e=>{e.addEventListener("click",function(e){e.preventDefault();var t=this.getAttribute("href").substr(1);window.matchMedia("(prefers-reduced-motion: reduce)").matches?document.querySelector(`[id='${decodeURIComponent(t)}']`).scrollIntoView():document.querySelector(`[id='${decodeURIComponent(t)}']`).scrollIntoView({behavior:"smooth"}),t==="top"?history.replaceState(null,null," "):history.pushState(null,null,`#${t}`)})})</script><script>var mybutton=document.getElementById("top-link");window.onscroll=function(){document.body.scrollTop>800||document.documentElement.scrollTop>800?(mybutton.style.visibility="visible",mybutton.style.opacity="1"):(mybutton.style.visibility="hidden",mybutton.style.opacity="0")},mybutton.oncontextmenu=e=>{e.preventDefault(),document.querySelectorAll(".example-container").forEach(e=>{e.style.backgroundColor="unset"}),document.querySelectorAll(".example-content").forEach(e=>{e.style.display="block",e.style.backgroundColor="var(--code-bg)",e.style.marginBottom="var(--modal-gap)"}),document.querySelectorAll(".next-button").forEach(e=>{e.style.display="none"})}</script><script>document.querySelectorAll("pre > code").forEach(e=>{const n=e.parentNode.parentNode,t=document.createElement("button");t.classList.add("copy-code"),t.innerHTML="copy";function s(){t.innerHTML="copied!",setTimeout(()=>{t.innerHTML="copy"},2e3)}t.addEventListener("click",t=>{if("clipboard"in navigator){navigator.clipboard.writeText(e.textContent),s();return}const n=document.createRange();n.selectNodeContents(e);const o=window.getSelection();o.removeAllRanges(),o.addRange(n);try{document.execCommand("copy"),s()}catch{}o.removeRange(n)}),n.classList.contains("highlight")?n.appendChild(t):n.parentNode.firstChild==n||(e.parentNode.parentNode.parentNode.parentNode.parentNode.nodeName=="TABLE"?e.parentNode.parentNode.parentNode.parentNode.parentNode.appendChild(t):e.parentNode.appendChild(t))})</script></body></html>