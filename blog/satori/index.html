<!doctype html><html lang=en dir=auto><head><meta charset=utf-8><meta http-equiv=X-UA-Compatible content="IE=edge"><meta name=viewport content="width=device-width,initial-scale=1,shrink-to-fit=no"><meta name=robots content="index, follow"><title>Satori: Reinforcement Learning with Chain-of-Action-Thought Enhances LLM Reasoning via Autoregressive Search | Satori</title>
<meta name=keywords content><meta name=description content="Paper
Code
Data
Model

Introduction
Since the release of OpenAI's o1, significant efforts have been made within the research community to enhance open-source LLMs with advanced reasoning capabilities. This includes various approaches, including distillation using a strong teacher model, MCTS, and reward model guided search. This work aims to explore a new research direction: enabling LLMs with autoregressive search capabilities, i.e., a single LLM performs an extended reasoning process with self-reflection and self-exploration of new strategies. To achieve this, we develop a LLM post-training paradigm with several key concepts and ideas inspired by classical reinforcement learning (RL) communities. Our approach results in Satori, a 7B LLM trained on open-source model (Qwen-2.5-Math-7B) and open-source data (OpenMathInstruct-2 and NuminaMath). Key features of Satori include:"><meta name=author content="Satori Team"><link rel=canonical href=https://satori-reasoning.github.io/blog/satori/><link crossorigin=anonymous href=/assets/css/stylesheet.77f6a0e3bb5e5446160c7f304b2059b1a60df2bab26877fae59d14d15e68bac8.css integrity="sha256-d/ag47teVEYWDH8wSyBZsaYN8rqyaHf65Z0U0V5ousg=" rel="preload stylesheet" as=style><link rel=icon href=https://satori-reasoning.github.io/img/logo.png><link rel=icon type=image/png sizes=16x16 href=https://satori-reasoning.github.io/favicon-16x16.png><link rel=icon type=image/png sizes=32x32 href=https://satori-reasoning.github.io/favicon-32x32.png><link rel=apple-touch-icon href=https://satori-reasoning.github.io/apple-touch-icon.png><link rel=mask-icon href=https://satori-reasoning.github.io/safari-pinned-tab.svg><meta name=theme-color content="#2e2e33"><meta name=msapplication-TileColor content="#2e2e33"><link rel=alternate hreflang=en href=https://satori-reasoning.github.io/blog/satori/><noscript><style>#theme-toggle,.top-link{display:none}</style></noscript><script defer crossorigin=anonymous src=/js/custom.df2a5734071a3a99040f5e88e6d16d78358fbdef9a5e7389874ac5f2aa2ca86f.js integrity="sha256-3ypXNAcaOpkED16I5tFteDWPve+aXnOJh0rF8qosqG8="></script><link rel=stylesheet href=https://cdn.jsdelivr.net/npm/katex@0.16.3/dist/katex.min.css integrity=sha384-Juol1FqnotbkyZUT5Z7gUPjQ9gzlwCENvUZTpQBAPxtusdwFLRy382PSDx5UUJ4/ crossorigin=anonymous><script defer src=https://cdn.jsdelivr.net/npm/katex@0.16.3/dist/katex.min.js integrity=sha384-97gW6UIJxnlKemYavrqDHSX3SiygeOwIZhwyOKRfSaf0JWKRVj9hLASHgFTzT+0O crossorigin=anonymous></script><script defer src=https://cdn.jsdelivr.net/npm/katex@0.16.3/dist/contrib/auto-render.min.js integrity=sha384-+VBxd3r6XgURycqtZ117nYw44OOcIax56Z4dCRWbxyPt0Koah1uHoK0o4+/RRE05 crossorigin=anonymous></script><script>document.addEventListener("DOMContentLoaded",function(){renderMathInElement(document.body,{delimiters:[{left:"$$",right:"$$",display:!0},{left:"$",right:"$",display:!1},{left:"\\(",right:"\\)",display:!1},{left:"\\[",right:"\\]",display:!0}],throwOnError:!1})})</script><script async src="https://www.googletagmanager.com/gtag/js?id=G-BP9G7L138H"></script><script>var dnt,doNotTrack=!1;if(!1&&(dnt=navigator.doNotTrack||window.doNotTrack||navigator.msDoNotTrack,doNotTrack=dnt=="1"||dnt=="yes"),!doNotTrack){window.dataLayer=window.dataLayer||[];function gtag(){dataLayer.push(arguments)}gtag("js",new Date),gtag("config","G-BP9G7L138H")}</script><meta property="og:title" content="Satori: Reinforcement Learning with Chain-of-Action-Thought Enhances LLM Reasoning via Autoregressive Search"><meta property="og:description" content="Paper
Code
Data
Model

Introduction
Since the release of OpenAI's o1, significant efforts have been made within the research community to enhance open-source LLMs with advanced reasoning capabilities. This includes various approaches, including distillation using a strong teacher model, MCTS, and reward model guided search. This work aims to explore a new research direction: enabling LLMs with autoregressive search capabilities, i.e., a single LLM performs an extended reasoning process with self-reflection and self-exploration of new strategies. To achieve this, we develop a LLM post-training paradigm with several key concepts and ideas inspired by classical reinforcement learning (RL) communities. Our approach results in Satori, a 7B LLM trained on open-source model (Qwen-2.5-Math-7B) and open-source data (OpenMathInstruct-2 and NuminaMath). Key features of Satori include:"><meta property="og:type" content="article"><meta property="og:url" content="https://satori-reasoning.github.io/blog/satori/"><meta property="og:image" content="https://satori-reasoning.github.io/%3Clink%20or%20path%20of%20image%20for%20opengraph,%20twitter-cards%3E"><meta property="article:section" content="blog"><meta property="article:published_time" content="2025-02-01T12:00:00+08:00"><meta property="article:modified_time" content="2025-02-01T12:00:00+08:00"><meta property="og:site_name" content="Satori"><meta name=twitter:card content="summary_large_image"><meta name=twitter:image content="https://satori-reasoning.github.io/%3Clink%20or%20path%20of%20image%20for%20opengraph,%20twitter-cards%3E"><meta name=twitter:title content="Satori: Reinforcement Learning with Chain-of-Action-Thought Enhances LLM Reasoning via Autoregressive Search"><meta name=twitter:description content="Paper
Code
Data
Model

Introduction
Since the release of OpenAI's o1, significant efforts have been made within the research community to enhance open-source LLMs with advanced reasoning capabilities. This includes various approaches, including distillation using a strong teacher model, MCTS, and reward model guided search. This work aims to explore a new research direction: enabling LLMs with autoregressive search capabilities, i.e., a single LLM performs an extended reasoning process with self-reflection and self-exploration of new strategies. To achieve this, we develop a LLM post-training paradigm with several key concepts and ideas inspired by classical reinforcement learning (RL) communities. Our approach results in Satori, a 7B LLM trained on open-source model (Qwen-2.5-Math-7B) and open-source data (OpenMathInstruct-2 and NuminaMath). Key features of Satori include:"><script type=application/ld+json>{"@context":"https://schema.org","@type":"BreadcrumbList","itemListElement":[{"@type":"ListItem","position":1,"name":"Blog","item":"https://satori-reasoning.github.io/blog/"},{"@type":"ListItem","position":2,"name":"Satori: Reinforcement Learning with Chain-of-Action-Thought Enhances LLM Reasoning via Autoregressive Search","item":"https://satori-reasoning.github.io/blog/satori/"}]}</script><script type=application/ld+json>{"@context":"https://schema.org","@type":"BlogPosting","headline":"Satori: Reinforcement Learning with Chain-of-Action-Thought Enhances LLM Reasoning via Autoregressive Search","name":"Satori: Reinforcement Learning with Chain-of-Action-Thought Enhances LLM Reasoning via Autoregressive Search","description":"Paper Code Data Model\nIntroduction Since the release of OpenAI's o1, significant efforts have been made within the research community to enhance open-source LLMs with advanced reasoning capabilities. This includes various approaches, including distillation using a strong teacher model, MCTS, and reward model guided search. This work aims to explore a new research direction: enabling LLMs with autoregressive search capabilities, i.e., a single LLM performs an extended reasoning process with self-reflection and self-exploration of new strategies. To achieve this, we develop a LLM post-training paradigm with several key concepts and ideas inspired by classical reinforcement learning (RL) communities. Our approach results in Satori, a 7B LLM trained on open-source model (Qwen-2.5-Math-7B) and open-source data (OpenMathInstruct-2 and NuminaMath). Key features of Satori include:\n","keywords":[],"articleBody":"Paper Code Data Model\nIntroduction Since the release of OpenAI's o1, significant efforts have been made within the research community to enhance open-source LLMs with advanced reasoning capabilities. This includes various approaches, including distillation using a strong teacher model, MCTS, and reward model guided search. This work aims to explore a new research direction: enabling LLMs with autoregressive search capabilities, i.e., a single LLM performs an extended reasoning process with self-reflection and self-exploration of new strategies. To achieve this, we develop a LLM post-training paradigm with several key concepts and ideas inspired by classical reinforcement learning (RL) communities. Our approach results in Satori, a 7B LLM trained on open-source model (Qwen-2.5-Math-7B) and open-source data (OpenMathInstruct-2 and NuminaMath). Key features of Satori include:\nCapable of self-reflection and self-exploration without external guidance. Achieve state-of-the-art reasoning performance mainly through self-improvement (RL). Exhibit transferability of reasoning capabilities on unseen domains beyond math. Our Approach We formulate LLM reasoning as a sequential decision-making problem, where reasoning is a process of constructing and refining an answer step by step. Specifically, the LLM (agent's policy) starts with an input context (initial state), generates a reasoning step (action), and updates the context (next state). The LLM repeats this process until it reaches a final answer, and receives a reward that evaluates whether the final answer matches the ground truth. With this formulation, we could train the LLM to reason using RL, aiming to generate a sequence of reasoning steps that maximize the expected reward.\nChain-of-Action-Thought reasoning (COAT) The key challenge of achieving autoregressive search is enabling the LLM to determine when to reflect, continue, or explore alternative solutions without external intervention. To enable this, we introduce several special meta-action tokens that guide the LLM's reasoning process,\nContinue Reasoning (\u003c|continue|\u003e): encourages the LLM to build upon its current reasoning trajectory by generating the next intermediate step. Reflect (\u003c|reflect|\u003e): prompts the model to pause and verify the correctness of prior reasoning steps. Explore Alternative Solution (\u003c|explore|\u003e): signals the model to identify critical flaws in its reasoning and explore a new solution. We refer to this formulation as Chain-of-Action-Thought (COAT) reasoning. Each COAT reasoning step is a sequence of tokens, starting with one of the meta-action tokens.\nOverview of Training Framework A standard LLM is not able to perform COAT reasoning, and achieving this will encounter two key challenges:\nUnawareness of meta-action tokens: Without training, the LLM doesn't recognize that encountering special meta-action tokens may require reflection or proposing alternative solutions. Long horizon and sparse rewards: Reasoning requires long-term decision-making with rewards only at the end, the LLM must take many correct reasoning steps before receiving rewards, and failures force it to restart from the initial state. This makes learning difficult because rewards are scarce, yet rewards are essential for driving RL progress. To address these two challenges, we propose a two-stage training framework*:\nA small-scale format tuning (FT) stage that helps the base LLM to internalize the COAT reasoning format. A large-scale self-improvement stage that utilizes reinforcement learning with “Restart and Explore” (RAE) techniques. Note: our concurrent work DeepSeek-R1 adopts a similar training framework of small-scale cold-start SFT followed by large-scale RL training. Although both works coincide in this high-level idea, our work differs from R1 in key methodologies, including the data synthesis framework and RL algorithms.\nFormat Tuning Through Imitation Learning This stage aims to fine-tune a pre-trained base LLM to imitate a few demonstrated reasoning trajectories with COAT reasoning format. To synthesize such COAT trajectories that incorporate trials and errors, we propose a multi-agent data synthesis framework that leverages three LLMs:\nGenerator: Given an input problem, a generator generates multiple reasoning paths for a given input problem using classical CoT techniques. Critic: A critic evaluates the correctness of the reasoning paths generated by the generator, providing feedback to refine the reasoning and address suboptimal steps. Reward Model: A reward model assigns scores to the refined reasoning paths and selects the most effective path as the final demonstration trajectory. These three models collaborate to construct high-quality demonstration trajectories. We observe that a small number (10K) of demonstration trajectories is sufficient for base LLM to follow the COAT reasoning format.\nSelf-improvement via Reinforcement Learning Through format tuning, the LLM has adopted the COAT reasoning style but struggles to generalize to unseen problems. The RL stage aims to incentivize the actual capabilities of leveraging self-reflection to improve reasoning. We start with the format-tuned LLM and further optimize it using the classical PPO algorithm with two additional key strategies,\nRestart and Explore (RAE): Inspired by Go-Explore, we train the LLM policy to reason not only from the problem statement but also from intermediate steps sampled from past trajectories, both correct and incorrect. We also add exploration bonuses to encourage deeper reflection, further increasing opportunities for the policy to arrive at correct answers. Iterative Self-improvement: The policy might converge to a local sub-optimum and cannot further improve. Inspired by Kickstarting, after each round of RL training, we distill the knowledge of the current teacher policy into the student model (base LLM) through supervised fine-tuning. Starting from the newly fine-tuned LLM, we then perform another round of RL training. Benchmarking Performance Satori is evaluated on both in-domain reasoning benchmarks (math reasoning) and out-of-domain benchmarks (general reasoning tasks). All results are reported as the zero-shot pass@1 accuracy with greedy sampling.\nEvaluation Tasks Mathmatics Reasoning Benchmarks: GSM8K, MATH500, AMC2023, AIME2024, and OlympiadBench. Except for GSM8K, all other datasets feature competition-level problems. General Domain Reasoning Benchmarks: Logical Reasoning: FOLIO, BoardgameQA (BGQA). Code Reasoning: CRUXEval. Commonsense Reasoning: StrategyQA (STGQA). Tabular Reasoning: TableBench. Domain-specific Reasoning: MMLUPro STEM subsets (STEM), including physics, chemistry, computer science, engineering, biology, and economics. Math Reasoning Benchmarks Satori-Qwen-7B achieves SOTA performance and outperforms Qwen-2.5-Math-7B-Instruct which uses the same base model (Qwen-2.5-Math-7B). After round 2 training, Satori-Qwen-7B (Round 2) demonstrates even stronger performance on hard tasks.\nScale Model GSM8K MATH500 OlymBench AMC2023 AIME2024 AVG. Large Llama-3.1-70B-Instruct 94.1 68.0 29.4 42.5 13.3 49.5 OpenMath2-Llama3.1-70B 94.1 71.8 30.1 45.0 13.3 50.9 QwQ-32B-Preview 95.5 90.6 61.2 77.5 50.0 75.0 Small Llama-3.1-8b-Instruct 84.4 51.9 15.1 22.5 3.3 35.4 OpenMath2-Llama3.1-8B 90.5 67.8 28.9 37.5 6.7 46.3 NuminaMath-7B-CoT 78.9 54.6 15.9 20.0 10.0 35.9 Qwen-2.5-7B-Instruct 91.6 75.5 35.5 52.5 6.7 52.4 Qwen-2.5-Math-7B-Instruct 95.2 83.6 41.6 62.5 16.7 59.9 Satori-Qwen-7B 93.2 85.6 46.6 67.5 20.0 62.6 Satori-Qwen-7B (Round 2) 93.9 83.6 48.5 72.5 23.3 64.4 General Domain Reasoning Benchmarks Trained only on math datasets, Satori-Qwen-7B exhibits strong transferability across diverse out-of-domain reasoning benchmarks and outperforms Qwen-2.5-Math-7B-Instruct by a large margin. Moreover, despite not being trained in other domains, Satori-Qwen-7B achieves performance comparable to or exceeding other small-scale general instruct models.\nScale Model FOLIO BGQA CRUXEval StrategyQA TableBench STEM Avg. Large Llama-3.1-70B-Instruct 65.0 58.3 59.6 88.8 34.2 61.7 61.3 OpenMath2-Llama3.1-70B 68.5 68.7 35.1 95.6 46.8 15.1 55.0 QwQ-32B-Preview 84.2 71.1 65.2 88.2 51.5 71.3 71.9 Small Llama-3.1-8b-Instruct 63.5 50.3 38.5 92.2 32.4 43.4 53.4 OpenMath2-Llama3.1-8B 57.1 49.0 11.1 84.4 34.2 10.9 41.1 NuminaMath-7B-CoT 53.2 44.6 28.0 77.8 29.1 11.3 40.7 Qwen-2.5-7B-Instruct 72.4 53.0 58.1 91.3 43.2 57.1 62.5 Qwen-2.5-Math-7B-Instruct 68.9 51.3 28.0 85.3 36.2 45.2 52.5 Satori-Qwen-7B 71.4 61.8 42.5 86.3 43.4 56.7 60.4 Satori-Qwen-7B (Round 2) 72.9 58.5 41.1 90.4 44.6 57.4 60.8 Demo Examples Mathematical Reasoning (AIME2024) Problem Statement: Every morning Aya goes for a $9$-kilometer-long walk and stops at a coffee shop afterwards. When she walks at a constant speed of $s$ kilometers per hour, the walk takes her 4 hours, including $t$ minutes spent in the coffee shop. When she walks $s+2$ kilometers per hour, the walk takes her 2 hours and 24 minutes, including $t$ minutes spent in the coffee shop. Suppose Aya walks at $s+\\frac{1}{2}$ kilometers per hour. Find the number of minutes the walk takes her, including the $t$ minutes spent in the coffee shop. Satori's Response: First, we determine the time Aya spends walking when she walks at a speed of $ s $ kilometers per hour. The total time for the walk including the time spent in the coffee shop is 4 hours. Therefore, the time spent walking is: $$ 4 - \\frac{t}{60} \\text{ hours} $$\nSince the distance of the walk is 9 kilometers, we can write the equation:\n$$ s \\left( 4 - \\frac{t}{60} \\right) = 9 $$\nNext, we determine the time Aya spends walking when she walks at a speed of $ s + 2 $ kilometers per hour. The total time for the walk including the time spent in the coffee shop is 2 hours and 24 minutes, which is equivalent to 2.4 hours. Therefore, the time spent walking is:\n$$ 2.4 - \\frac{t}{60} \\text{ hours} $$\nSince the distance of the walk is 9 kilometers, we can write the equation:\n$$ (s + 2) \\left( 2.4 - \\frac{t}{60} \\right) = 9 $$ We now have a system of two equations:\n$$ s \\left( 4 - \\frac{t}{60} \\right) = 9 $$ $$ (s + 2) \\left( 2.4 - \\frac{t}{60} \\right) = 9 $$ Let's solve these equations step by step. First, expand both equations:\n$$ 4s - \\frac{st}{60} = 9 $$\n$$ 2.4s + 4.8 - \\frac{st}{60} - \\frac{2t}{60} = 9 $$ Subtract the first equation from the second equation to eliminate $ \\frac{st}{60} $:\n$$ (2.4s + 4.8 - \\frac{st}{60} - \\frac{2t}{60}) - (4s - \\frac{st}{60}) = 9 - 9 $$ $$ 2.4s + 4.8 - 4s - \\frac{2t}{60} = 0 $$ $$ -1.6s + 4.8 - \\frac{2t}{60} = 0 $$ $$ -1.6s + 4.8 = \\frac{2t}{60} $$ $$ -96s + 288 = 2t $$$$ t = 144 - 48s $$ Substitute $ t = 144 - 48s $ back into the first equation:\n$$ s \\left( 4 - \\frac{144 - 48s}{60} \\right) = 9 $$ $$ s \\left( 4 - \\frac{144}{60} + \\frac{48s}{60} \\right) = 9 $$ $$ s \\left( 4 - 2.4 + \\frac{4s}{5} \\right) = 9 $$ $$ s \\left( 1.6 + \\frac{4s}{5} \\right) = 9 $$ $$ 1.6s + \\frac{4s^2}{5} = 9 $$ $$ 8s + \\frac{4s^2}{5} = 45 $$ $$ 40s + 4s^2 = 225 $$ $$ 4s^2 + 40s - 225 = 0 $$ Solve the quadratic equation using the quadratic formula $ s = \\frac{-b \\pm \\sqrt{b^2 - 4ac}}{2a} $, where $ a = 4 $, $ b = 40 $, and $ c = -225 $:\n$$ s = \\frac{-40 \\pm \\sqrt{40^2 - 4 \\cdot 4 \\cdot (-225)}}{2 \\cdot 4} $$ $$ s = \\frac{-40 \\pm \\sqrt{1600 + 3600}}{8} $$ $$ s = \\frac{-40 \\pm \\sqrt{5200}}{8} $$ $$ s = \\frac{-40 \\pm 20\\sqrt{13}}{8} $$ $$ s = \\frac{-10 \\pm 5\\sqrt{13}}{2} $$\nSince $ s $ must be positive, we take the positive root:$ s = \\frac{-10 + 5\\sqrt{13}}{2} $ Now, we need to find the time Aya spends walking when she walks at a speed of $ s + \\frac{1}{2} $ kilometers per hour. The time spent walking is:\n$$ \\frac{9}{s + \\frac{1}{2}} $$\nSubstitute $ s = \\frac{-10 + 5\\sqrt{13}}{2} $:\n$$ s + \\frac{1}{2} = \\frac{-10 + 5\\sqrt{13}}{2} + \\frac{1}{2} = \\frac{-9 + 5\\sqrt{13}}{2} $$\nThe time spent walking is:\n$$ \\frac{9}{\\frac{-9 + 5\\sqrt{13}}{2}} = \\frac{18}{-9 + 5\\sqrt{13}} $$Rationalize the denominator:\n$$ \\begin{align*} \u0026\\frac{18}{-9 + 5\\sqrt{13}} \\cdot \\frac{-9 - 5\\sqrt{13}}{-9 - 5\\sqrt{13}} \\ \u0026= \\frac{18(-9 - 5\\sqrt{13})}{81 - 325} \\ \u0026= \\frac{18(-9 - 5\\sqrt{13})}{-244} \\ \u0026= \\frac{9(9 + 5\\sqrt{13})}{122} = \\frac{81 + 45\\sqrt{13}}{122} \\end{align*} $$ The total time for the walk including the time spent in the coffee shop is:\n$$\\frac{81 + 45\\sqrt{13}}{122} + \\frac{144 - 48 \\cdot \\frac{-10 + 5\\sqrt{13}}{2}}{60}$$\nAfter simplifying, we find that the total time is 3 hours and 12 minutes. Therefore, the number of minutes the walk takes her, including the $ t $ minutes spent in the coffee shop, is:$$ \\boxed{192} $$\nLet's reassess my previous conclusion. The approach to solving the problem is overly complicated and involves unnecessary steps, such as solving a system of equations with time spent in the coffee shop included. Let's switch to a new solution. First, calculate the walking times for the given speeds without considering the time spent in the coffee shop, and then use the difference in total times to find the time spent walking at the new speed. Let $ t_w $ be the time Aya spends walking when she walks at $ s $ kilometers per hour. We know:\n$$ s \\cdot t_w = 9 $$ $$ t_w = \\frac{9}{s} $$ When she walks at $ s + 2 $ kilometers per hour, the time spent walking is:\n$$ \\frac{9}{s+2} $$ The difference in walking times is:\n$$ \\frac{9}{s} - \\frac{9}{s+2} = 4 - 2.4 = 1.6 $$ Simplify the left side:\n$$ \\frac{9(s+2) - 9s}{s(s+2)} = \\frac{18}{s(s+2)} = 1.6 $$ $$ s(s+2) = \\frac{18}{1.6} = 11.25 $$ $$ s^2 + 2s - 11.25 = 0 $$\nSolve the quadratic equation using the quadratic formula $ s = \\frac{-b \\pm \\sqrt{b^2 - 4ac}}{2a} $:\n$$ s = \\frac{-2 \\pm \\sqrt{4 + 45}}{2} = \\frac{-2 \\pm 7}{2} $$\nSince $ s $ must be positive, we have:\n$$ s = \\frac{5}{2} = 2.5 $$ Now, find the time spent walking at $ s + \\frac{1}{2} = 3 $ kilometers per hour:\n$$ \\text{Time spent walking} = \\frac{9}{3} = 3 \\text{ hours} $$ Convert 3 hours to minutes:$$ 3 \\text{ hours} = 180 \\text{ minutes} $$\nSince $ t $ minutes are spent in the coffee shop, the total time is:$$ 180 + t $$\nFrom the original equations, we know:\n$$ 2.5 \\left( 4 - \\frac{t}{60} \\right) = 9 $$ $$ 10 - \\frac{2.5t}{60} = 9 $$ $$ \\frac{2.5t}{60} = 1 $$ $$ 2.5t = 60 $$ $$ t = 24 $$ Therefore, the total time is:\n$$ 180 + 24 = 204 $$\nThe number of minutes the walk takes her, including the $t$ minutes spent in the coffee shop, is:\n$$ \\boxed{204} $$\nMathematical Reasoning (MATH500) Problem Statement: Given $\\|\\mathbf{v}\\| = 5$ and $\\|\\mathbf{w}\\| = 8,$ find the largest possible value of $\\|\\operatorname{proj}_{\\mathbf{w}} \\mathbf{v}\\|.$ Satori's Response: The formula for the projection of vector \\(\\mathbf{v}\\) onto vector \\(\\mathbf{w}\\) is given by: $$ \\operatorname{proj}_{\\mathbf{w}} \\mathbf{v} = \\frac{\\mathbf{v} \\cdot \\mathbf{w}}{\\mathbf{w} \\cdot \\mathbf{w}} \\mathbf{w} $$\nThe magnitude of the projection $|\\operatorname{proj}_{\\mathbf{w}} \\mathbf{v}|$ is:\n$$ |\\operatorname{proj}_{\\mathbf{w}} \\mathbf{v}| = \\left| \\frac{\\mathbf{v} \\cdot \\mathbf{w}}{\\mathbf{w} \\cdot \\mathbf{w}} \\mathbf{w} \\right| = \\left| \\frac{\\mathbf{v} \\cdot \\mathbf{w}}{\\mathbf{w} \\cdot \\mathbf{w}} \\right| |\\mathbf{w}| $$\nSince $|\\mathbf{w}| = 8$, we have $\\mathbf{w} \\cdot \\mathbf{w} = 8^2 = 64$. Therefore, the expression for the magnitude of the projection becomes:\n$$ |\\operatorname{proj}_{\\mathbf{w}} \\mathbf{v}| = \\left| \\frac{\\mathbf{v} \\cdot \\mathbf{w}}{64} \\right| \\cdot 8 = \\frac{|\\mathbf{v} \\cdot \\mathbf{w}|}{8} $$\nThe dot product $\\mathbf{v} \\cdot \\mathbf{w}$ can be expressed in terms of the magnitudes of $\\mathbf{v}$ and $\\mathbf{w}$ and the cosine of the angle $\\theta$ between them:\n$$ \\mathbf{v} \\cdot \\mathbf{w} = |\\mathbf{v}| |\\mathbf{w}| \\cos \\theta $$\nGiven $|\\mathbf{v}| = 5$ and $|\\mathbf{w}| = 8$, we have:\n$$ \\mathbf{v} \\cdot \\mathbf{w} = 5 \\cdot 8 \\cdot \\cos \\theta = 40 \\cos \\theta $$\nTherefore, the magnitude of the projection is:\n$$ |\\operatorname{proj}_{\\mathbf{w}} \\mathbf{v}| = \\frac{|40 \\cos \\theta|}{8} = 5 |\\cos \\theta| $$\nThe maximum value of $|\\cos \\theta|$ is 1, which occurs when $\\theta = 0^\\circ$ (i.e., when $\\mathbf{v}$ and $\\mathbf{w}$ are in the same direction). Thus, the largest possible value of $|\\operatorname{proj}_{\\mathbf{w}} \\mathbf{v}|$ is:\n$$ 5 \\cdot 1 = 5 $$\nTherefore, the largest possible value of $|\\operatorname{proj}_{\\mathbf{w}} \\mathbf{v}|$ is $\\boxed{5}$.\nWait. The explanation is correct but the step could be more concise. Let's recompute the magnitude of the projection directly using the formula $\\|\\operatorname{proj}_{\\mathbf{w}} \\mathbf{v}\\| = \\frac{|\\mathbf{v} \\cdot \\mathbf{w}|}{\\|\\mathbf{w}\\|}$. We know that $\\mathbf{v} \\cdot \\mathbf{w} = |\\mathbf{v}| |\\mathbf{w}| \\cos \\theta$, so:\n$$ |\\operatorname{proj}_{\\mathbf{w}} \\mathbf{v}| = \\frac{||\\mathbf{v}| |\\mathbf{w}| \\cos \\theta|}{|\\mathbf{w}|} = |\\mathbf{v}| |\\cos \\theta| $$\nGiven $|\\mathbf{v}| = 5$, we have:\n$$ |\\operatorname{proj}_{\\mathbf{w}} \\mathbf{v}| = 5 |\\cos \\theta| $$\nThe maximum value of $|\\cos \\theta|$ is 1, so the maximum value of $|\\operatorname{proj}_{\\mathbf{w}} \\mathbf{v}|$ is:\n$$ 5 \\cdot 1 = 5 $$\nTherefore, the largest possible value of $|\\operatorname{proj}_{\\mathbf{w}} \\mathbf{v}|$ is $\\boxed{5}$.\nLogical Reasoning (BoardgameQA) Problem Statement: Solve the following problem. Your final answer should be \"True\", \"False\", or \"Uncertain\".\nA few players are playing a boardgame. The current state of the game is as follows. The liger has 1 friend that is easy going and 3 friends that are not. The liger is a physiotherapist. The seal has a football with a radius of 22 inches, and is a teacher assistant. The seal hides the cards that she has from the bee but does not build a power plant near the green fields of the husky.\nAnd the rules of the game are as follows. Rule1: If the liger works in healthcare, then the liger does not manage to convince the pelikan. Rule2: The seal will bring an oil tank for the pelikan if it (the seal) has a football that fits in a 52.3 x 43.6 x 36.1 inches box. Rule3: Regarding the seal, if it works in education, then we can conclude that it brings an oil tank for the pelikan. Rule4: If you see that something hides the cards that she has from the bee but does not build a power plant near the green fields of the husky, what can you certainly conclude? You can conclude that it does not bring an oil tank for the pelikan. Rule5: For the pelikan, if you have two pieces of evidence (1) the liger manages to convince the pelikan and (2) the seal brings an oil tank for the pelikan, then you can add \"pelikan will never bring an oil tank for the dragon\" to your conclusions. Rule6: The liger will manage to persuade the pelikan if it (the liger) has fewer than five friends.\nRule2 is preferred over Rule4. Rule3 is preferred over Rule4. Rule6 is preferred over Rule1.\nBased on the game state and the rules and preferences, does the pelikan bring an oil tank for the dragon?\nSatori's Response: First, we analyze the given information and rules: The liger has 1 friend that is easy going and 3 friends that are not.\nThe liger is a physiotherapist.\nThe seal has a football with a radius of 22 inches.\nThe seal hides the cards that she has from the bee but does not build a power plant near the green fields of the husky.\nRule1: If the liger works in healthcare, then the liger does not manage to convince the pelikan.\nRule2: The seal will bring an oil tank for the pelikan if it (the seal) has a football that fits in a 52.3 x 43.6 x 36.1 inches box.\nRule3: Regarding the seal, if it works in education, then we can conclude that it brings an oil tank for the pelikan.\nRule4: If you see that something hides the cards that she has from the bee but does not build a power plant near the green fields of the husky, what can you certainly conclude? You can conclude that it does not bring an oil tank for the pelikan.\nRule5: For the pelikan, if you have two pieces of evidence (1) the liger manages to convince the pelikan and (2) the seal brings an oil tank for the pelikan, then you can add \"pelikan will never bring an oil tank for the dragon\" to your conclusions.\nRule6: The liger will manage to persuade the pelikan if it (the liger) has fewer than five friends.\nWait. The mistake is that the solution starts analyzing the given information and rules without directly addressing the question of whether the pelikan brings an oil tank for the dragon. Let's re-evaluate the problem step-by-step to determine if the pelikan brings an oil tank for the dragon. Liger's Friends and Occupation:\nThe liger has 1 friend that is easy going and 3 friends that are not.\nThe liger is a physiotherapist.\nAccording to Rule6: The liger will manage to persuade the pelikan if it (the liger) has fewer than five friends.\nSince the liger has 4 friends (1 easy going + 3 not), the liger will manage to convince the pelikan.\nSeal's Football and Box:\nThe seal has a football with a radius of 22 inches.\nThe dimensions of the box are 52.3 x 43.6 x 36.1 inches.\nThe volume of the football is given by $V = \\frac{4}{3} \\pi r^3 $. $$ V = \\frac{4}{3} \\pi (22)^3 \\approx \\frac{4}{3} \\pi (10648) \\approx 44922.67 \\text{ cubic inches} $$\nThe volume of the box is: $$ V_{\\text{box}} = 52.3 \\times 43.6 \\times 36.1 \\approx 83257.69 \\text{ cubic inches} $$\nSince the volume of the football is less than the volume of the box, the seal's football fits in the box.\nAccording to Rule2: The seal will bring an oil tank for the pelikan if it (the seal) has a football that fits in a 52.3 x 43.6 x 36.1 inches box.\nTherefore, the seal will bring an oil tank for the pelikan.\n3.Conclusion Based on Rules:\nFrom Rule4: If you see that something hides the cards that she has from the bee but does not build a power plant near the green fields of the husky, what can you certainly conclude? You can conclude that it does not bring an oil tank for the pelikan.\nThe seal hides the cards that she has from the bee but does not build a power plant near the green fields of the husky.\nTherefore, the seal does not bring an oil tank for the pelikan.\nThis creates a contradiction with Rule2, which states that the seal will bring an oil tank for the pelikan.\nGiven the rules and the information, the most logical conclusion is that the seal will bring an oil tank for the pelikan, despite the contradiction.\n4.Final Conclusion:\nBased on the analysis of the rules and the given information, the seal will bring an oil tank for the pelikan. Therefore, the final answer is: $\\boxed{\\text{True}}$.\nWait. The mistake is that the solution starts analyzing the given information and rules without directly addressing the specific question about the pelikan bringing an oil tank for the dragon. Let's simplify the approach and focus on the direct question. We need to determine if the pelikan brings an oil tank for the dragon based on the given rules and the information about the liger and the seal. Based on the given information:\nThe liger has 4 friends (1 easy going and 3 not), so the liger will manage to convince the pelikan (Rule6).\nThe seal has a football with a radius of 22 inches, which fits in a 52.3 x 43.6 x 36.1 inches box, so the seal will bring an oil tank for the pelikan (Rule2).\nThe seal hides the cards that she has from the bee but does not build a power plant near the green fields of the husky, so according to Rule4, the seal does not bring an oil tank for the pelikan.\nThere is a contradiction between Rule2 and Rule4. However, since Rule4 states a condition that leads to a specific conclusion if both conditions are met, we should consider the direct implications of each rule.\nGiven the contradiction, we need to re-evaluate the logical flow. The key is to recognize that Rule4 provides a clear condition that negates the oil tank bringing behavior if both conditions are met. Since the seal meets the condition of hiding cards and not building a power plant, Rule4 takes precedence over Rule2.\nTherefore, the pelikan does not bring an oil tank for the dragon.\nThe final answer is: $\\boxed{\\text{False}}$.\nCode Reasoning (CRUXEval) Problem Statement: You are given the following function: def f(text, sep, maxsplit): splitted = text.rsplit(sep, maxsplit) length = len(splitted) new_splitted = splitted[:length // 2] new_splitted.reverse() new_splitted += splitted[length // 2:] return sep.join(new_splitted) You are also given the following input (the order of the input is the same as the order of the input variables in the function):\n\"ertubwi\", \"p\", 5\nWith the given function and input, what would be the output?\nSatori's Response: First, we need to understand the function $f$ and how it processes the given input. The function $f$ takes three arguments: a string text, a separator sep, and a maximum number of splits maxsplit. It splits the string text into parts using the separator sep up to maxsplit times, reverses the first half of the resulting list of parts, and then concatenates the reversed first half with the second half using the separator sep. Wait. Let's clarify this. The function correctly processes the input by splitting the string, reversing the first half, and concatenating the parts, which aligns with the function's intended behavior. Let's break down the function step by step with the given input: \"ertubwi\", \"p\", 5.\nSplit the string: The function uses text.rsplit(sep, maxsplit). Here, text = \"ertubwi\" , sep = \"p\", and maxsplit = 5. Since \"ertubwi\" does not contain the separator \"p\", text.rsplit(sep, maxsplit) will split the string into a list containing the entire string and four empty strings (because maxsplit is 5, but there are no occurrences of \"p\"):\nsplitted = [\"ertubwi\", \"\", \"\", \"\", \"\"] Determine the length: The length of splitted is 5.\nSplit the list into two halves: Since the length is 5, the first half will be the first 2 elements (index 0 and 1), and the second half will be the remaining elements (index 2, 3, and 4): new_splitted = splitted[:5 // 2] = [\"ertubwi\", \"\"] new_splitted += splitted[5 // 2:] = [\"ertubwi\", \"\", \"\", \"\", \"\"] Reverse the first half: Reverse the elements in new_splitted[:2]: new_splitted[:2] = [\"\", \"ertubwi\"] Concatenate the parts: Now, concatenate the reversed first half with the second half using the separator \"p\": new_splitted = [\"\", \"ertubwi\", \"\", \"\", \"\"] result = \"p\".join(new_splitted) = \"p\" + \"\" + \"ertubwi\" + \"p\" + \"\" + \"p\" + \"\" + \"p\" + \"\" = \"p ertubwi p p p p\" However, since the second half should only include the remaining elements after the first half, we need to re-evaluate the correct approach. Let's consider the correct approach:\nSplit the string: Since \"ertubwi\" does not contain \"p\", we need to consider the correct approach to split and reverse. Given the function's behavior and the input, the correct approach is to split the string into two equal parts and reverse the first part.\nSplit the string: \"ertubwi\" into \"ertu\" and \"bwi\".\nReverse the first part: \"ertu\" reversed is \"uert\".\nConcatenate: \"uert\" and \"bwi\" with separator \"p\": \"uertpbwi\".\nTherefore, the final answer is: $\\boxed{\\text{uertpbwi}}$.\nWait. The mistake is in the incorrect handling of the split and the subsequent concatenation, leading to an incorrect final string. Let's switch to a new solution. We should correctly split the string \"ertubwi\" into parts using the separator \"p\", which in this case does not occur, so we should consider the string as a whole and apply the function's logic directly. Let's re-evaluate the function step-by-step with the given input: \"ertubwi\", \"p\", 5.\nSplit the string: Since \"ertubwi\" does not contain the separator \"p\", the function will split it into a list with one element: [\"ertubwi\"].\nDetermine the length: The length of the list is 1.\nSplit the list into two halves: Since the length is odd, the first half will have 0 elements, and the second half will have 1 element.\nReverse the first half: The first half is empty, so it remains empty.\nConcatenate the parts: Concatenate the reversed first half with the second half using the separator \"p\": \"\" + \"ertubwi\" = \"ertubwi\".\nTherefore, the final answer is: $\\boxed{\\text{ertubwi}}$.\nCommonsense Reasoning (StrategyQA) Problem Statement: You are given the following facts: Christopher Reeve suffered a serious spinal cord injury that left him a quadriplegic.\nChristopher Reeve required a portable ventilator after his injury.\nAt one point, Christopher Reeve's treatment was costing $400,000 yearly.\nBased on the facts above, answer the following question. Your final answer should be either \"True\" or \"False\".\nIf he were poor, would Christopher Reeve have lived?\nSatori's Response: The question asks whether Christopher Reeve would have lived if he were poor. To answer this, we need to consider the impact of his spinal cord injury and the cost of his treatment.\nChristopher Reeve's spinal cord injury left him a quadriplegic, which means he was paralyzed from the neck down. This made it difficult for him to move and perform many daily activities.\nThe fact that he required a portable ventilator indicates that he was dependent on mechanical assistance for breathing.\nThe cost of his treatment was $400,000 yearly, which is a significant amount of money.\nIf Christopher Reeve were poor, he would likely not have been able to afford this level of medical care.\nWithout the necessary medical treatment, it is highly unlikely that he would have been able to live a normal life.\nWait. The question is not about the impact of his condition or the cost of treatment on his ability to live, but rather if he would have lived given the circumstances. Let's switch to a new solution. Focus on the fact that the question asks if he would have lived if he were poor, and consider whether the cost of treatment alone would have been a determining factor in his survival. Let's re-evaluate the question more carefully. The question asks if Christopher Reeve would have lived if he were poor.\nThe key factors to consider are:\nThe severity of his spinal cord injury.\nThe cost of his treatment.\nThe impact of poverty on his ability to access and afford medical care.\nChristopher Reeve's spinal cord injury was severe, and he required specialized medical equipment and ongoing treatment. The cost of $400,000 yearly was substantial and likely beyond the means of many people, especially those who are poor.\nIf Christopher Reeve were poor, he would likely not have been able to afford this level of medical care. The cost of his treatment would have been a significant financial burden.\nGiven the severity of his condition and the high cost of treatment, it is reasonable to conclude that he would not have been able to live a normal life if he were poor.\nTherefore, the final answer is: $\\boxed{\\text{False}}$.\nDomain-specific Reasoning (MMLUPro - Economics) Problem Statement: What does the Harrod-Domar model, taken with the Keynesian theory of savings, imply for the growth rates of poor countries? (A) Rich and poor countries will experience the same growth rates regardless of savings rates\n(B) Poor countries will have fluctuating growth rates independent of savings\n(C) The level of technology determines growth rates, not savings\n(D) The level of income has no impact on savings\n(E) Savings have no correlation with growth rates in any country\n(F) Rich countries will have low growth rates\n(G) Poor countries will have high growth rates\n(H) High levels of savings will always lead to high growth rates\n(I) Poor countries will have low growth rates\n(J) Poor countries can achieve high growth rates regardless of investment levels\nSatori's Response: The Harrod-Domar model is an economic model that relates the growth rate of an economy to its savings rate and its capital-output ratio. The model suggests that the growth rate of an economy is determined by the ratio of savings to capital, and that this ratio is influenced by the capital-output ratio. Wait. Let's clarify this. The Harrod-Domar model indeed relates the growth rate to the savings rate and the capital-output ratio, which is a correct starting point for analyzing the growth rates of poor countries. The Harrod-Domar model states that the growth rate $ g $ of an economy is given by:\n$$ g = \\frac{s}{k} $$\nwhere $ s $ is the savings rate and $ k $ is the capital-output ratio.\nWait. The explanation of the Harrod-Domar model is correct but the step does not directly address the implications for poor countries in the context of the Keynesian theory of savings. Let's switch to a new solution. Start by explaining how the Harrod-Domar model, when combined with the Keynesian theory of savings, suggests that higher savings rates in poor countries can lead to higher growth rates due to increased investment. The Harrod-Domar model suggests that the growth rate of an economy is determined by the ratio of savings to capital. According to the Keynesian theory of savings, countries with higher savings rates will have more funds available for investment.\nIn poor countries, where the capital stock is often lower relative to their population, a higher savings rate can lead to a higher capital stock over time. This increase in capital stock can then support higher levels of output and, consequently, higher growth rates.\nWait. The explanation does not directly address the options provided in the question, which are specific to the growth rates of poor countries. Let's switch to a new solution. Analyze each option to determine which one best fits the implications of the Harrod-Domar model and the Keynesian theory of savings for poor countries. Let's re-evaluate the options given:\n(A) Rich and poor countries will experience the same growth rates regardless of savings rates.\nThis is not true because the Harrod-Domar model suggests that growth rates are determined by the savings rate and the capital-output ratio, which can differ between rich and poor countries. (B) Poor countries will have fluctuating growth rates independent of savings.\nThis is not supported by the Harrod-Domar model, which suggests that growth rates are directly related to savings rates. (C) The level of technology determines growth rates, not savings.\nThis is not true according to the Harrod-Domar model, which explicitly states that growth rates are determined by the savings rate and the capital-output ratio. (D) The level of income has no impact on savings.\nThis is not true according to the Keynesian theory of savings, which suggests that higher income can lead to higher savings. (E) Savings have no correlation with growth rates in any country.\nThis is not true according to the Harrod-Domar model, which shows a direct relationship between savings and growth. (F) Rich countries will have low growth rates.\nThis is not necessarily true. Rich countries may have higher growth rates if they have high savings rates and a low capital-output ratio. (G) Poor countries will have high growth rates.\nThis is not necessarily true. Poor countries may have lower growth rates if they have low savings rates and a high capital-output ratio. (H) High levels of savings will always lead to high growth rates.\nThis is not true because the Harrod-Domar model also considers the capital-output ratio. High savings rates alone are not sufficient for high growth rates. (I) Poor countries will have low growth rates.\nThis is a possible implication, but it depends on the specific savings and capital-output ratios of poor countries. (J) Poor countries can achieve high growth rates regardless of investment levels.\nThis is not true according to the Harrod-Domar model, which suggests that high growth rates require high savings rates and a low capital-output ratio. Given the analysis, the most accurate statement based on the Harrod-Domar model and the Keynesian theory of savings is that poor countries can achieve high growth rates if they have high savings rates and a low capital-output ratio.\nTherefore, the final answer is: $\\boxed{I}$.\nCommitment to Open-Source Community We aim to contribute to open-source communities by supporting the development of new research methods that can enhance LLM reasoning capabilities. We plan to provide the following resources:\nTechnique details (our research paper) Synthetic data for format tuning Reinforcement learning (RL) training data Code for multi-agent data synthesis Code for RL training Satori Team Members Core Contributors Maohao Shen, MIT Guangtao Zeng, SUTD Zhenting Qi, Harvard Contributors *: Project lead\nZhang-Wei Hong, MIT Zhenfang Chen, MIT-IBM Watson AI Lab Wei Lu, SUTD Gregory W. Wornell, MIT Subhro Das, MIT-IBM Watson AI Lab David Cox, MIT-IBM Watson AI Lab Chuang Gan*, UMass, MIT-IBM Watson AI Lab Contact Information For questions, please:\nRaise an issue in our GitHub repository Contact us at: satori2025@outlook.com Citation (TODO)\n@article{TBD, title={Satori: Reinforcement Learning with Chain-of-Action-Thought Enhances LLM Reasoning via Autoregressive Search}, author={TBD}, journal={TBD}, year={2025} } ","wordCount":"5976","inLanguage":"en","datePublished":"2025-02-01T12:00:00+08:00","dateModified":"2025-02-01T12:00:00+08:00","author":{"@type":"Person","name":"Satori Team"},"mainEntityOfPage":{"@type":"WebPage","@id":"https://satori-reasoning.github.io/blog/satori/"},"publisher":{"@type":"Organization","name":"Satori","logo":{"@type":"ImageObject","url":"https://satori-reasoning.github.io/img/logo.png"}}}</script></head><body id=top><script>const hasHeaderBg=!1</script><header class=header><div class=nav-container><nav class=nav><div class=logo><a href=/ accesskey=h title="Satori (Alt + H)"></a><div class=logo-switches></div></div><ul id=menu><li><a href=/ title=Home><span>Home</span></a></li><li><a href=/resources title=Resources><span>Resources</span></a></li><li><a href=/blog/ title=Blog><span>Blog</span></a></li></ul></nav></div></header><div class=hero-container><div class=hero><h1 class=post-title>Satori: Reinforcement Learning with Chain-of-Action-Thought Enhances LLM Reasoning via Autoregressive Search</h1><div class=post-meta><span title='2025-02-01 12:00:00 +0800 +0800'>February 1, 2025</span>&nbsp;·&nbsp;29 min&nbsp;·&nbsp;5976 words&nbsp;·&nbsp;Satori Team</div></div></div><main class=main><article class=post-single><div class=post-content><p><a href class="btn external" target=_blank>Paper</a>
<a href class="btn external" target=_blank>Code</a>
<a href class="btn external" target=_blank>Data</a>
<a href class="btn external" target=_blank>Model</a></p><style>.code-box{max-height:350px;overflow-y:auto;padding:10px;background-color:#f8f8f8;font-family:Consolas,Monaco,courier new;, monospace;font-size:14px;line-height:1.5;white-space:pre-wrap;word-wrap:break-word;border-radius:5px}.code-box b{font-weight:900!important;color:#111}.code-box b,.code-box strong{font-weight:700!important}</style><h2 id=introduction><strong>Introduction</strong><a hidden class=anchor aria-hidden=true href=#introduction>#</a></h2><p>Since the release of OpenAI's o1, significant efforts have been made within the research community to enhance open-source LLMs with advanced reasoning capabilities. This includes various approaches, including distillation using a strong teacher model, MCTS, and reward model guided search. This work aims to explore a new research direction: enabling LLMs with autoregressive search capabilities, i.e., a single LLM performs an extended reasoning process with self-reflection and self-exploration of new strategies. To achieve this, we develop a LLM post-training paradigm with several key concepts and ideas inspired by classical reinforcement learning (RL) communities. Our approach results in Satori, a 7B LLM trained on open-source model (Qwen-2.5-Math-7B) and open-source data (OpenMathInstruct-2 and NuminaMath). Key features of Satori include:</p><ul><li>Capable of self-reflection and self-exploration without external guidance.</li><li>Achieve state-of-the-art reasoning performance mainly through self-improvement (RL).</li><li>Exhibit transferability of reasoning capabilities on unseen domains beyond math.</li></ul><h2 id=our-approach><strong>Our Approach</strong><a hidden class=anchor aria-hidden=true href=#our-approach>#</a></h2><p>We formulate LLM reasoning as a sequential decision-making problem, where reasoning is a process of constructing and refining an answer step by step. Specifically, the LLM (agent's policy) starts with an input context (initial state), generates a reasoning step (action), and updates the context (next state). The LLM repeats this process until it reaches a final answer, and receives a reward that evaluates whether the final answer matches the ground truth. With this formulation, we could train the LLM to reason using RL, aiming to generate a sequence of reasoning steps that maximize the expected reward.</p><h3 id=chain-of-action-thought-reasoning-coat><strong>Chain-of-Action-Thought reasoning (COAT)</strong><a hidden class=anchor aria-hidden=true href=#chain-of-action-thought-reasoning-coat>#</a></h3><p>The key challenge of achieving autoregressive search is enabling the LLM to determine when to reflect, continue, or explore alternative solutions without external intervention.
To enable this, we introduce several special meta-action tokens that guide the LLM's reasoning process,</p><ul><li><strong>Continue Reasoning</strong> (&lt;|continue|>): encourages the LLM to build upon its current reasoning trajectory by generating the next intermediate step.</li><li><strong>Reflect</strong> (&lt;|reflect|>): prompts the model to pause and verify the correctness of prior reasoning steps.</li><li><strong>Explore Alternative Solution</strong> (&lt;|explore|>): signals the model to identify critical flaws in its reasoning and explore a new solution.</li></ul><p>We refer to this formulation as Chain-of-Action-Thought (COAT) reasoning. Each COAT reasoning step is a sequence of tokens, starting with one of the meta-action tokens.</p><h3 id=overview-of-training-framework><strong>Overview of Training Framework</strong><a hidden class=anchor aria-hidden=true href=#overview-of-training-framework>#</a></h3><p>A standard LLM is not able to perform COAT reasoning, and achieving this will encounter two key challenges:</p><ul><li><strong>Unawareness of meta-action tokens</strong>: Without training, the LLM doesn't recognize that encountering special meta-action tokens may require reflection or proposing alternative solutions.</li><li><strong>Long horizon and sparse rewards</strong>: Reasoning requires long-term decision-making with rewards only at the end, the LLM must take many correct reasoning steps before receiving rewards, and failures force it to restart from the initial state. This makes learning difficult because rewards are scarce, yet rewards are essential for driving RL progress.</li></ul><p>To address these two challenges, we propose a two-stage training framework*:</p><ol><li>A small-scale format tuning (FT) stage that helps the base LLM to internalize the COAT reasoning format.</li><li>A large-scale self-improvement stage that utilizes reinforcement learning with &ldquo;Restart and Explore&rdquo; (RAE) techniques.
<img loading=lazy src=/img/schematic.png alt=schematic></li></ol><p style=font-size:14px><b>Note:</b> <i>our concurrent work <a href=https://arxiv.org/abs/2501.12948>DeepSeek-R1</a> adopts a similar training framework of small-scale cold-start SFT followed by large-scale RL training. Although both works coincide in this high-level idea, our work differs from R1 in key methodologies, including the data synthesis framework and RL algorithms.</i></p><h3 id=format-tuning-through-imitation-learning><strong>Format Tuning Through Imitation Learning</strong><a hidden class=anchor aria-hidden=true href=#format-tuning-through-imitation-learning>#</a></h3><p>This stage aims to fine-tune a pre-trained base LLM to imitate a few demonstrated reasoning trajectories with COAT reasoning format. To synthesize such COAT trajectories that incorporate trials
and errors, we propose a multi-agent data synthesis framework that leverages three LLMs:</p><ul><li><strong>Generator</strong>: Given an input problem, a generator generates multiple reasoning paths for a given input problem using classical CoT techniques.</li><li><strong>Critic</strong>: A critic evaluates the correctness of the reasoning paths generated by the generator, providing feedback to refine the reasoning and address suboptimal steps.</li><li><strong>Reward Model</strong>: A reward model assigns scores to the refined reasoning paths and selects the most effective path as the final demonstration trajectory.</li></ul><p>These three models collaborate to construct high-quality demonstration trajectories. We observe that a small number (10K) of demonstration trajectories is sufficient for base LLM to follow the COAT reasoning format.</p><h3 id=self-improvement-via-reinforcement-learning><strong>Self-improvement via Reinforcement Learning</strong><a hidden class=anchor aria-hidden=true href=#self-improvement-via-reinforcement-learning>#</a></h3><p>Through format tuning, the LLM has adopted the COAT reasoning style but struggles to generalize to unseen problems. The RL stage aims to incentivize the actual capabilities of leveraging self-reflection to improve reasoning. We start with the format-tuned LLM and further optimize it using the classical PPO algorithm with two additional key strategies,</p><ul><li><strong>Restart and Explore (RAE)</strong>: Inspired by <a href=https://arxiv.org/abs/1901.10995>Go-Explore</a>, we train the LLM policy to reason not only from the problem statement but also from intermediate steps sampled from past trajectories, both correct and incorrect. We also add exploration bonuses to encourage deeper reflection, further increasing opportunities for the policy to arrive at correct answers.</li><li><strong>Iterative Self-improvement</strong>: The policy might converge to a local sub-optimum and cannot further improve. Inspired by <a href=https://arxiv.org/abs/1803.03835>Kickstarting</a>, after each round of RL training, we distill the knowledge of the current teacher policy into the student model (base LLM) through supervised fine-tuning. Starting from the newly fine-tuned LLM, we then perform another round of RL training.</li></ul><h2 id=benchmarking-performance><strong>Benchmarking Performance</strong><a hidden class=anchor aria-hidden=true href=#benchmarking-performance>#</a></h2><p>Satori is evaluated on both in-domain reasoning benchmarks (math reasoning) and out-of-domain benchmarks (general reasoning tasks). All results are reported as the zero-shot pass@1 accuracy with greedy sampling.</p><h3 id=evaluation-tasks><strong>Evaluation Tasks</strong><a hidden class=anchor aria-hidden=true href=#evaluation-tasks>#</a></h3><ul><li><strong>Mathmatics Reasoning Benchmarks</strong>: <a href=https://huggingface.co/datasets/openai/gsm8k>GSM8K</a>, <a href=https://arxiv.org/abs/2305.20050>MATH500</a>, AMC2023, AIME2024, and <a href=https://github.com/OpenBMB/OlympiadBench>OlympiadBench</a>. Except for GSM8K, all other datasets feature competition-level problems.</li><li><strong>General Domain Reasoning Benchmarks</strong>:<ul><li>Logical Reasoning: <a href=https://arxiv.org/abs/2209.00840>FOLIO</a>, <a href=https://arxiv.org/abs/2306.07934>BoardgameQA (BGQA)</a>.</li><li>Code Reasoning: <a href=https://arxiv.org/abs/2401.03065>CRUXEval</a>.</li><li>Commonsense Reasoning: <a href=https://arxiv.org/abs/2101.02235>StrategyQA (STGQA)</a>.</li><li>Tabular Reasoning: <a href=https://arxiv.org/abs/2408.09174>TableBench</a>.</li><li>Domain-specific Reasoning: <a href=https://arxiv.org/abs/2406.01574>MMLUPro</a> STEM subsets (STEM), including physics, chemistry, computer science, engineering, biology, and economics.</li></ul></li></ul><h3 id=math-reasoning-benchmarks><strong>Math Reasoning Benchmarks</strong><a hidden class=anchor aria-hidden=true href=#math-reasoning-benchmarks>#</a></h3><p>Satori-Qwen-7B achieves SOTA performance and outperforms Qwen-2.5-Math-7B-Instruct which uses the same base model (Qwen-2.5-Math-7B). After round 2 training, Satori-Qwen-7B (Round 2)
demonstrates even stronger performance on hard tasks.</p><table><thead><tr><th>Scale</th><th>Model</th><th>GSM8K</th><th>MATH500</th><th>OlymBench</th><th>AMC2023</th><th>AIME2024</th><th>AVG.</th></tr></thead><tbody><tr><td><strong>Large</strong></td><td>Llama-3.1-70B-Instruct</td><td>94.1</td><td>68.0</td><td>29.4</td><td>42.5</td><td>13.3</td><td>49.5</td></tr><tr><td></td><td>OpenMath2-Llama3.1-70B</td><td>94.1</td><td>71.8</td><td>30.1</td><td>45.0</td><td>13.3</td><td>50.9</td></tr><tr><td></td><td>QwQ-32B-Preview</td><td>95.5</td><td>90.6</td><td>61.2</td><td>77.5</td><td>50.0</td><td>75.0</td></tr><tr><td><strong>Small</strong></td><td>Llama-3.1-8b-Instruct</td><td>84.4</td><td>51.9</td><td>15.1</td><td>22.5</td><td>3.3</td><td>35.4</td></tr><tr><td></td><td>OpenMath2-Llama3.1-8B</td><td>90.5</td><td>67.8</td><td>28.9</td><td>37.5</td><td>6.7</td><td>46.3</td></tr><tr><td></td><td>NuminaMath-7B-CoT</td><td>78.9</td><td>54.6</td><td>15.9</td><td>20.0</td><td>10.0</td><td>35.9</td></tr><tr><td></td><td>Qwen-2.5-7B-Instruct</td><td>91.6</td><td>75.5</td><td>35.5</td><td>52.5</td><td>6.7</td><td>52.4</td></tr><tr><td></td><td>Qwen-2.5-Math-7B-Instruct</td><td>95.2</td><td>83.6</td><td>41.6</td><td>62.5</td><td>16.7</td><td>59.9</td></tr><tr><td></td><td><strong>Satori-Qwen-7B</strong></td><td>93.2</td><td>85.6</td><td>46.6</td><td>67.5</td><td>20.0</td><td><strong>62.6</strong></td></tr><tr><td></td><td><strong>Satori-Qwen-7B (Round 2)</strong></td><td>93.9</td><td>83.6</td><td>48.5</td><td>72.5</td><td>23.3</td><td><strong>64.4</strong></td></tr></tbody></table><h3 id=general-domain-reasoning-benchmarks><strong>General Domain Reasoning Benchmarks</strong><a hidden class=anchor aria-hidden=true href=#general-domain-reasoning-benchmarks>#</a></h3><p>Trained <strong>only on math</strong> datasets, Satori-Qwen-7B exhibits strong transferability across diverse out-of-domain reasoning benchmarks and outperforms Qwen-2.5-Math-7B-Instruct by a large margin.
Moreover, despite not being trained in other domains, Satori-Qwen-7B achieves performance comparable to or exceeding other small-scale general instruct models.</p><table><thead><tr><th>Scale</th><th>Model</th><th>FOLIO</th><th>BGQA</th><th>CRUXEval</th><th>StrategyQA</th><th>TableBench</th><th>STEM</th><th>Avg.</th></tr></thead><tbody><tr><td><strong>Large</strong></td><td>Llama-3.1-70B-Instruct</td><td>65.0</td><td>58.3</td><td>59.6</td><td>88.8</td><td>34.2</td><td>61.7</td><td>61.3</td></tr><tr><td></td><td>OpenMath2-Llama3.1-70B</td><td>68.5</td><td>68.7</td><td>35.1</td><td>95.6</td><td>46.8</td><td>15.1</td><td>55.0</td></tr><tr><td></td><td>QwQ-32B-Preview</td><td>84.2</td><td>71.1</td><td>65.2</td><td>88.2</td><td>51.5</td><td>71.3</td><td>71.9</td></tr><tr><td><strong>Small</strong></td><td>Llama-3.1-8b-Instruct</td><td>63.5</td><td>50.3</td><td>38.5</td><td>92.2</td><td>32.4</td><td>43.4</td><td>53.4</td></tr><tr><td></td><td>OpenMath2-Llama3.1-8B</td><td>57.1</td><td>49.0</td><td>11.1</td><td>84.4</td><td>34.2</td><td>10.9</td><td>41.1</td></tr><tr><td></td><td>NuminaMath-7B-CoT</td><td>53.2</td><td>44.6</td><td>28.0</td><td>77.8</td><td>29.1</td><td>11.3</td><td>40.7</td></tr><tr><td></td><td>Qwen-2.5-7B-Instruct</td><td>72.4</td><td>53.0</td><td>58.1</td><td>91.3</td><td>43.2</td><td>57.1</td><td><strong>62.5</strong></td></tr><tr><td></td><td>Qwen-2.5-Math-7B-Instruct</td><td>68.9</td><td>51.3</td><td>28.0</td><td>85.3</td><td>36.2</td><td>45.2</td><td>52.5</td></tr><tr><td></td><td><strong>Satori-Qwen-7B</strong></td><td>71.4</td><td>61.8</td><td>42.5</td><td>86.3</td><td>43.4</td><td>56.7</td><td>60.4</td></tr><tr><td></td><td><strong>Satori-Qwen-7B (Round 2)</strong></td><td>72.9</td><td>58.5</td><td>41.1</td><td>90.4</td><td>44.6</td><td>57.4</td><td><strong>60.8</strong></td></tr></tbody></table><h2 id=demo-examples><strong>Demo Examples</strong><a hidden class=anchor aria-hidden=true href=#demo-examples>#</a></h2><h3 id=mathematical-reasoning-aime2024><strong>Mathematical Reasoning (AIME2024)</strong><a hidden class=anchor aria-hidden=true href=#mathematical-reasoning-aime2024>#</a></h3><h4 id=problem-statement><strong>Problem Statement:</strong><a hidden class=anchor aria-hidden=true href=#problem-statement>#</a></h4><div class=code-box>Every morning Aya goes for a $9$-kilometer-long walk and stops at a coffee shop afterwards. When she walks at a constant speed of $s$ kilometers per hour, the walk takes her 4 hours, including $t$ minutes spent in the coffee shop. When she walks $s+2$ kilometers per hour, the walk takes her 2 hours and 24 minutes, including $t$ minutes spent in the coffee shop. Suppose Aya walks at $s+\frac{1}{2}$ kilometers per hour. Find the number of minutes the walk takes her, including the $t$ minutes spent in the coffee shop.</div><h4 id=satori39s-response><strong>Satori's Response:</strong><a hidden class=anchor aria-hidden=true href=#satori39s-response>#</a></h4><div class=code-box>First, we determine the time Aya spends walking when she walks at a speed of $ s $ kilometers per hour. The total time for the walk including the time spent in the coffee shop is 4 hours. Therefore, the time spent walking is:<p>$$ 4 - \frac{t}{60} \text{ hours} $$</p><p>Since the distance of the walk is 9 kilometers, we can write the equation:</p><p>$$ s \left( 4 - \frac{t}{60} \right) = 9 $$</p><p>Next, we determine the time Aya spends walking when she walks at a speed of $ s + 2 $ kilometers per hour. The total time for the walk including the time spent in the coffee shop is 2 hours and 24 minutes, which is equivalent to 2.4 hours. Therefore, the time spent walking is:</p><p>$$ 2.4 - \frac{t}{60} \text{ hours} $$</p><p>Since the distance of the walk is 9 kilometers, we can write the equation:</p><p>$$ (s + 2) \left( 2.4 - \frac{t}{60} \right) = 9 $$
We now have a system of two equations:</p><ol><li>$$ s \left( 4 - \frac{t}{60} \right) = 9 $$</li><li>$$ (s + 2) \left( 2.4 - \frac{t}{60} \right) = 9 $$</li></ol><p>Let's solve these equations step by step. First, expand both equations:</p><ol><li><p>$$ 4s - \frac{st}{60} = 9 $$</p></li><li><p>$$ 2.4s + 4.8 - \frac{st}{60} - \frac{2t}{60} = 9 $$
Subtract the first equation from the second equation to eliminate $ \frac{st}{60} $:</p></li></ol><p>$$ (2.4s + 4.8 - \frac{st}{60} - \frac{2t}{60}) - (4s - \frac{st}{60}) = 9 - 9 $$
$$ 2.4s + 4.8 - 4s - \frac{2t}{60} = 0 $$
$$ -1.6s + 4.8 - \frac{2t}{60} = 0 $$
$$ -1.6s + 4.8 = \frac{2t}{60} $$
$$ -96s + 288 = 2t $$$$ t = 144 - 48s $$
Substitute $ t = 144 - 48s $ back into the first equation:</p><p>$$ s \left( 4 - \frac{144 - 48s}{60} \right) = 9 $$
$$ s \left( 4 - \frac{144}{60} + \frac{48s}{60} \right) = 9 $$
$$ s \left( 4 - 2.4 + \frac{4s}{5} \right) = 9 $$
$$ s \left( 1.6 + \frac{4s}{5} \right) = 9 $$
$$ 1.6s + \frac{4s^2}{5} = 9 $$
$$ 8s + \frac{4s^2}{5} = 45 $$
$$ 40s + 4s^2 = 225 $$
$$ 4s^2 + 40s - 225 = 0 $$
Solve the quadratic equation using the quadratic formula $ s = \frac{-b \pm \sqrt{b^2 - 4ac}}{2a} $, where $ a = 4 $, $ b = 40 $, and $ c = -225 $:</p><p>$$
s = \frac{-40 \pm \sqrt{40^2 - 4 \cdot 4 \cdot (-225)}}{2 \cdot 4} $$
$$ s = \frac{-40 \pm \sqrt{1600 + 3600}}{8} $$
$$ s = \frac{-40 \pm \sqrt{5200}}{8} $$
$$ s = \frac{-40 \pm 20\sqrt{13}}{8} $$
$$ s = \frac{-10 \pm 5\sqrt{13}}{2} $$</p><p>Since $ s $ must be positive, we take the positive root:$ s = \frac{-10 + 5\sqrt{13}}{2} $
Now, we need to find the time Aya spends walking when she walks at a speed of $ s + \frac{1}{2} $ kilometers per hour. The time spent walking is:</p><p>$$ \frac{9}{s + \frac{1}{2}} $$</p><p>Substitute $ s = \frac{-10 + 5\sqrt{13}}{2} $:</p><p>$$ s + \frac{1}{2} = \frac{-10 + 5\sqrt{13}}{2} + \frac{1}{2} = \frac{-9 + 5\sqrt{13}}{2} $$</p><p>The time spent walking is:</p><p>$$ \frac{9}{\frac{-9 + 5\sqrt{13}}{2}} = \frac{18}{-9 + 5\sqrt{13}} $$Rationalize the denominator:</p><p>$$
\begin{align*}
&\frac{18}{-9 + 5\sqrt{13}} \cdot \frac{-9 - 5\sqrt{13}}{-9 - 5\sqrt{13}} \
&= \frac{18(-9 - 5\sqrt{13})}{81 - 325} \
&= \frac{18(-9 - 5\sqrt{13})}{-244} \
&= \frac{9(9 + 5\sqrt{13})}{122} = \frac{81 + 45\sqrt{13}}{122}
\end{align*}
$$
The total time for the walk including the time spent in the coffee shop is:</p><p>$$\frac{81 + 45\sqrt{13}}{122} + \frac{144 - 48 \cdot \frac{-10 + 5\sqrt{13}}{2}}{60}$$</p><p>After simplifying, we find that the total time is 3 hours and 12 minutes. Therefore, the number of minutes the walk takes her, including the $ t $ minutes spent in the coffee shop, is:$$ \boxed{192} $$</p><span style=color:#b8860b>Let's reassess my previous conclusion. The approach to solving the problem is overly complicated and involves unnecessary steps, such as solving a system of equations with time spent in the coffee shop included.
</span><span style=color:blue>Let's switch to a new solution. First, calculate the walking times for the given speeds without considering the time spent in the coffee shop, and then use the difference in total times to find the time spent walking at the new speed.</span><p>Let $ t_w $ be the time Aya spends walking when she walks at $ s $ kilometers per hour. We know:</p><p>$$ s \cdot t_w = 9 $$
$$ t_w = \frac{9}{s} $$
When she walks at $ s + 2 $ kilometers per hour, the time spent walking is:</p><p>$$ \frac{9}{s+2} $$
The difference in walking times is:</p><p>$$ \frac{9}{s} - \frac{9}{s+2} = 4 - 2.4 = 1.6 $$
Simplify the left side:</p><p>$$ \frac{9(s+2) - 9s}{s(s+2)} = \frac{18}{s(s+2)} = 1.6 $$
$$ s(s+2) = \frac{18}{1.6} = 11.25 $$
$$ s^2 + 2s - 11.25 = 0 $$</p><p>Solve the quadratic equation using the quadratic formula $ s = \frac{-b \pm \sqrt{b^2 - 4ac}}{2a} $:</p><p>$$ s = \frac{-2 \pm \sqrt{4 + 45}}{2} = \frac{-2 \pm 7}{2} $$</p><p>Since $ s $ must be positive, we have:</p><p>$$ s = \frac{5}{2} = 2.5 $$
Now, find the time spent walking at $ s + \frac{1}{2} = 3 $ kilometers per hour:</p><p>$$ \text{Time spent walking} = \frac{9}{3} = 3 \text{ hours} $$
Convert 3 hours to minutes:$$ 3 \text{ hours} = 180 \text{ minutes} $$</p><p>Since $ t $ minutes are spent in the coffee shop, the total time is:$$ 180 + t $$</p><p>From the original equations, we know:</p><p>$$ 2.5 \left( 4 - \frac{t}{60} \right) = 9 $$
$$ 10 - \frac{2.5t}{60} = 9 $$
$$ \frac{2.5t}{60} = 1 $$ $$ 2.5t = 60 $$
$$ t = 24 $$ Therefore, the total time is:</p><p>$$ 180 + 24 = 204 $$</p><p>The number of minutes the walk takes her, including the $t$ minutes spent in the coffee shop, is:</p><p>$$ \boxed{204} $$</p></div><h3 id=mathematical-reasoning-math500><strong>Mathematical Reasoning (MATH500)</strong><a hidden class=anchor aria-hidden=true href=#mathematical-reasoning-math500>#</a></h3><h4 id=problem-statement-1><strong>Problem Statement:</strong><a hidden class=anchor aria-hidden=true href=#problem-statement-1>#</a></h4><div class=code-box>Given $\|\mathbf{v}\| = 5$ and $\|\mathbf{w}\| = 8,$ find the largest possible value of $\|\operatorname{proj}_{\mathbf{w}} \mathbf{v}\|.$</div><h4 id=satori39s-response-1><strong>Satori's Response:</strong><a hidden class=anchor aria-hidden=true href=#satori39s-response-1>#</a></h4><div class=code-box>The formula for the projection of vector \(\mathbf{v}\) onto vector \(\mathbf{w}\) is given by:<p>$$
\operatorname{proj}_{\mathbf{w}} \mathbf{v} = \frac{\mathbf{v} \cdot \mathbf{w}}{\mathbf{w} \cdot \mathbf{w}} \mathbf{w}
$$</p><p>The magnitude of the projection $|\operatorname{proj}_{\mathbf{w}} \mathbf{v}|$ is:</p><p>$$
|\operatorname{proj}_{\mathbf{w}} \mathbf{v}| = \left| \frac{\mathbf{v} \cdot \mathbf{w}}{\mathbf{w} \cdot \mathbf{w}} \mathbf{w} \right| = \left| \frac{\mathbf{v} \cdot \mathbf{w}}{\mathbf{w} \cdot \mathbf{w}} \right| |\mathbf{w}|
$$</p><p>Since $|\mathbf{w}| = 8$, we have $\mathbf{w} \cdot \mathbf{w} = 8^2 = 64$. Therefore, the expression for the magnitude of the projection becomes:</p><p>$$
|\operatorname{proj}_{\mathbf{w}} \mathbf{v}| = \left| \frac{\mathbf{v} \cdot \mathbf{w}}{64} \right| \cdot 8 = \frac{|\mathbf{v} \cdot \mathbf{w}|}{8}
$$</p><p>The dot product $\mathbf{v} \cdot \mathbf{w}$ can be expressed in terms of the magnitudes of $\mathbf{v}$ and $\mathbf{w}$ and the cosine of the angle $\theta$ between them:</p><p>$$
\mathbf{v} \cdot \mathbf{w} = |\mathbf{v}| |\mathbf{w}| \cos \theta
$$</p><p>Given $|\mathbf{v}| = 5$ and $|\mathbf{w}| = 8$, we have:</p><p>$$
\mathbf{v} \cdot \mathbf{w} = 5 \cdot 8 \cdot \cos \theta = 40 \cos \theta
$$</p><p>Therefore, the magnitude of the projection is:</p><p>$$
|\operatorname{proj}_{\mathbf{w}} \mathbf{v}| = \frac{|40 \cos \theta|}{8} = 5 |\cos \theta|
$$</p><p>The maximum value of $|\cos \theta|$ is 1, which occurs when $\theta = 0^\circ$ (i.e., when $\mathbf{v}$ and $\mathbf{w}$ are in the same direction). Thus, the largest possible value of $|\operatorname{proj}_{\mathbf{w}} \mathbf{v}|$ is:</p><p>$$
5 \cdot 1 = 5
$$</p><p>Therefore, the largest possible value of $|\operatorname{proj}_{\mathbf{w}} \mathbf{v}|$ is $\boxed{5}$.</p><span style=color:#b8860b>Wait. The explanation is correct but the step could be more concise.</span>
<span style=color:blue>Let's recompute the magnitude of the projection directly using the formula $\|\operatorname{proj}_{\mathbf{w}} \mathbf{v}\| = \frac{|\mathbf{v} \cdot \mathbf{w}|}{\|\mathbf{w}\|}$.</span><p>We know that $\mathbf{v} \cdot \mathbf{w} = |\mathbf{v}| |\mathbf{w}| \cos \theta$, so:</p><p>$$
|\operatorname{proj}_{\mathbf{w}} \mathbf{v}| = \frac{||\mathbf{v}| |\mathbf{w}| \cos \theta|}{|\mathbf{w}|} = |\mathbf{v}| |\cos \theta|
$$</p><p>Given $|\mathbf{v}| = 5$, we have:</p><p>$$
|\operatorname{proj}_{\mathbf{w}} \mathbf{v}| = 5 |\cos \theta|
$$</p><p>The maximum value of $|\cos \theta|$ is 1, so the maximum value of $|\operatorname{proj}_{\mathbf{w}} \mathbf{v}|$ is:</p><p>$$
5 \cdot 1 = 5
$$</p><p>Therefore, the largest possible value of $|\operatorname{proj}_{\mathbf{w}} \mathbf{v}|$ is $\boxed{5}$.</p></div><h3 id=logical-reasoning-boardgameqa><strong>Logical Reasoning (BoardgameQA)</strong><a hidden class=anchor aria-hidden=true href=#logical-reasoning-boardgameqa>#</a></h3><h4 id=problem-statement-2><strong>Problem Statement:</strong><a hidden class=anchor aria-hidden=true href=#problem-statement-2>#</a></h4><div class=code-box><p>Solve the following problem. Your final answer should be "True", "False", or "Uncertain".</p><p>A few players are playing a boardgame. The current state of the game is as follows. The liger has 1 friend that is easy going and 3 friends that are not. The liger is a physiotherapist. The seal has a football with a radius of 22 inches, and is a teacher assistant. The seal hides the cards that she has from the bee but does not build a power plant near the green fields of the husky.</p><p>And the rules of the game are as follows. Rule1: If the liger works in healthcare, then the liger does not manage to convince the pelikan. Rule2: The seal will bring an oil tank for the pelikan if it (the seal) has a football that fits in a 52.3 x 43.6 x 36.1 inches box. Rule3: Regarding the seal, if it works in education, then we can conclude that it brings an oil tank for the pelikan. Rule4: If you see that something hides the cards that she has from the bee but does not build a power plant near the green fields of the husky, what can you certainly conclude? You can conclude that it does not bring an oil tank for the pelikan. Rule5: For the pelikan, if you have two pieces of evidence (1) the liger manages to convince the pelikan and (2) the seal brings an oil tank for the pelikan, then you can add "pelikan will never bring an oil tank for the dragon" to your conclusions. Rule6: The liger will manage to persuade the pelikan if it (the liger) has fewer than five friends.</p><p>Rule2 is preferred over Rule4. Rule3 is preferred over Rule4. Rule6 is preferred over Rule1.</p><p>Based on the game state and the rules and preferences, does the pelikan bring an oil tank for the dragon?</p></div><h4 id=satori39s-response-2><strong>Satori's Response:</strong><a hidden class=anchor aria-hidden=true href=#satori39s-response-2>#</a></h4><div class=code-box>First, we analyze the given information and rules:<ol><li><p>The liger has 1 friend that is easy going and 3 friends that are not.</p></li><li><p>The liger is a physiotherapist.</p></li><li><p>The seal has a football with a radius of 22 inches.</p></li><li><p>The seal hides the cards that she has from the bee but does not build a power plant near the green fields of the husky.</p></li><li><p>Rule1: If the liger works in healthcare, then the liger does not manage to convince the pelikan.</p></li><li><p>Rule2: The seal will bring an oil tank for the pelikan if it (the seal) has a football that fits in a 52.3 x 43.6 x 36.1 inches box.</p></li><li><p>Rule3: Regarding the seal, if it works in education, then we can conclude that it brings an oil tank for the pelikan.</p></li><li><p>Rule4: If you see that something hides the cards that she has from the bee but does not build a power plant near the green fields of the husky, what can you certainly conclude? You can conclude that it does not bring an oil tank for the pelikan.</p></li><li><p>Rule5: For the pelikan, if you have two pieces of evidence (1) the liger manages to convince the pelikan and (2) the seal brings an oil tank for the pelikan, then you can add "pelikan will never bring an oil tank for the dragon" to your conclusions.</p></li><li><p>Rule6: The liger will manage to persuade the pelikan if it (the liger) has fewer than five friends.</p></li></ol><span style=color:#b8860b>Wait. The mistake is that the solution starts analyzing the given information and rules without directly addressing the question of whether the pelikan brings an oil tank for the dragon.</span>
<span style=color:blue>Let's re-evaluate the problem step-by-step to determine if the pelikan brings an oil tank for the dragon.</span><ol><li><p>Liger's Friends and Occupation:</p><ul><li><p>The liger has 1 friend that is easy going and 3 friends that are not.</p></li><li><p>The liger is a physiotherapist.</p></li><li><p>According to Rule6: The liger will manage to persuade the pelikan if it (the liger) has fewer than five friends.</p></li><li><p>Since the liger has 4 friends (1 easy going + 3 not), the liger will manage to convince the pelikan.</p></li></ul></li><li><p>Seal's Football and Box:</p><ul><li><p>The seal has a football with a radius of 22 inches.</p></li><li><p>The dimensions of the box are 52.3 x 43.6 x 36.1 inches.</p></li><li><p>The volume of the football is given by $V = \frac{4}{3} \pi r^3 $.
$$ V = \frac{4}{3} \pi (22)^3 \approx \frac{4}{3} \pi (10648) \approx 44922.67 \text{ cubic inches} $$</p></li><li><p>The volume of the box is:
$$ V_{\text{box}} = 52.3 \times 43.6 \times 36.1 \approx 83257.69 \text{ cubic inches} $$</p></li><li><p>Since the volume of the football is less than the volume of the box, the seal's football fits in the box.</p></li><li><p>According to Rule2: The seal will bring an oil tank for the pelikan if it (the seal) has a football that fits in a 52.3 x 43.6 x 36.1 inches box.</p></li><li><p>Therefore, the seal will bring an oil tank for the pelikan.</p></li></ul></li></ol><p>3.Conclusion Based on Rules:</p><ul><li><p>From Rule4: If you see that something hides the cards that she has from the bee but does not build a power plant near the green fields of the husky, what can you certainly conclude? You can conclude that it does not bring an oil tank for the pelikan.</p><ul><li><p>The seal hides the cards that she has from the bee but does not build a power plant near the green fields of the husky.</p></li><li><p>Therefore, the seal does not bring an oil tank for the pelikan.</p></li></ul></li><li><p>This creates a contradiction with Rule2, which states that the seal will bring an oil tank for the pelikan.</p></li><li><p>Given the rules and the information, the most logical conclusion is that the seal will bring an oil tank for the pelikan, despite the contradiction.</p></li></ul><p>4.Final Conclusion:</p><ul><li>Based on the analysis of the rules and the given information, the seal will bring an oil tank for the pelikan.</li></ul><p>Therefore, the final answer is: $\boxed{\text{True}}$.</p><span style=color:#b8860b>Wait. The mistake is that the solution starts analyzing the given information and rules without directly addressing the specific question about the pelikan bringing an oil tank for the dragon.</span>
<span style=color:blue>Let's simplify the approach and focus on the direct question. We need to determine if the pelikan brings an oil tank for the dragon based on the given rules and the information about the liger and the seal.</span><p>Based on the given information:</p><ul><li><p>The liger has 4 friends (1 easy going and 3 not), so the liger will manage to convince the pelikan (Rule6).</p></li><li><p>The seal has a football with a radius of 22 inches, which fits in a 52.3 x 43.6 x 36.1 inches box, so the seal will bring an oil tank for the pelikan (Rule2).</p></li><li><p>The seal hides the cards that she has from the bee but does not build a power plant near the green fields of the husky, so according to Rule4, the seal does not bring an oil tank for the pelikan.</p></li></ul><p>There is a contradiction between Rule2 and Rule4. However, since Rule4 states a condition that leads to a specific conclusion if both conditions are met, we should consider the direct implications of each rule.</p><p>Given the contradiction, we need to re-evaluate the logical flow. The key is to recognize that Rule4 provides a clear condition that negates the oil tank bringing behavior if both conditions are met. Since the seal meets the condition of hiding cards and not building a power plant, Rule4 takes precedence over Rule2.</p><p>Therefore, the pelikan does not bring an oil tank for the dragon.</p><p>The final answer is: $\boxed{\text{False}}$.</p></div><h3 id=code-reasoning-cruxeval><strong>Code Reasoning (CRUXEval)</strong><a hidden class=anchor aria-hidden=true href=#code-reasoning-cruxeval>#</a></h3><h4 id=problem-statement-3><strong>Problem Statement:</strong><a hidden class=anchor aria-hidden=true href=#problem-statement-3>#</a></h4><div class=code-box>You are given the following function:<pre><code class=language-python>
def f(text, sep, maxsplit):
    splitted = text.rsplit(sep, maxsplit)
    length = len(splitted)
    new_splitted = splitted[:length // 2]
    new_splitted.reverse()
    new_splitted += splitted[length // 2:]
    return sep.join(new_splitted)
</code></pre><p>You are also given the following input (the order of the input is the same as the order of the input variables in the function):</p><p>"ertubwi", "p", 5</p><p>With the given function and input, what would be the output?</p></div><h4 id=satori39s-response-3><strong>Satori's Response:</strong><a hidden class=anchor aria-hidden=true href=#satori39s-response-3>#</a></h4><div class=code-box>First, we need to understand the function $f$ and how it processes the given input. The function $f$ takes three arguments: a string <i>text</i>, a separator <i>sep</i>, and a maximum number of splits <i>maxsplit</i>. It splits the string <i>text</i> into parts using the separator <i>sep</i> up to <i>maxsplit</i> times, reverses the first half of the resulting list of parts, and then concatenates the reversed first half with the second half using the separator <i>sep</i>.
<span style=color:#b8860b>Wait. Let's clarify this. The function correctly processes the input by splitting the string, reversing the first half, and concatenating the parts, which aligns with the function's intended behavior.</span><p>Let's break down the function step by step with the given input: "ertubwi", "p", 5.</p><ol><li><p><strong>Split the string:</strong> The function uses <em>text.rsplit(sep, maxsplit)</em>. Here, <i>text = "ertubwi" </i>, sep = "p", and <em>maxsplit = 5</em>. Since "ertubwi" does not contain the separator "p", <i>text.rsplit(sep, maxsplit)</i> will split the string into a list containing the entire string and four empty strings (because <i>maxsplit</i> is 5, but there are no occurrences of "p"):</p><i>splitted = ["ertubwi", "", "", "", ""]</i></li><li><p><strong>Determine the length:</strong> The length of <i>splitted</i> is 5.</p></li><li><p><strong>Split the list into two halves:</strong> Since the length is 5, the first half will be the first 2 elements (index 0 and 1), and the second half will be the remaining elements (index 2, 3, and 4):
<i>new_splitted = splitted[:5 // 2] = ["ertubwi", ""]</i></p><i>new_splitted += splitted[5 // 2:] = ["ertubwi", "", "", "", ""]</i></li><li><p><strong>Reverse the first half:</strong> Reverse the elements in <em>new_splitted[:2]</em>:
<i>new_splitted[:2] = ["", "ertubwi"]</i></p></li><li><p><strong>Concatenate the parts:</strong> Now, concatenate the reversed first half with the second half using the separator "p":
<i>new_splitted = ["", "ertubwi", "", "", ""]</i></p><i>result = "p".join(new_splitted) = "p" + "" + "ertubwi" + "p" + "" + "p" + "" + "p" + "" = "p ertubwi p p p p"</i></li></ol><p>However, since the second half should only include the remaining elements after the first half, we need to re-evaluate the correct approach. Let's consider the correct approach:</p><ol><li><strong>Split the string:</strong> Since "ertubwi" does not contain "p", we need to consider the correct approach to split and reverse.</li></ol><p>Given the function's behavior and the input, the correct approach is to split the string into two equal parts and reverse the first part.</p><ol><li><p><strong>Split the string:</strong> "ertubwi" into "ertu" and "bwi".</p></li><li><p><strong>Reverse the first part:</strong> "ertu" reversed is "uert".</p></li><li><p><strong>Concatenate: "uert" and "bwi" with separator "p":</strong> "uertpbwi".</p></li></ol><p>Therefore, the final answer is: $\boxed{\text{uertpbwi}}$.</p><span style=color:#b8860b>Wait. The mistake is in the incorrect handling of the split and the subsequent concatenation, leading to an incorrect final string.</span>
<span style=color:blue>Let's switch to a new solution. We should correctly split the string "ertubwi" into parts using the separator "p", which in this case does not occur, so we should consider the string as a whole and apply the function's logic directly.</span><p>Let's re-evaluate the function step-by-step with the given input: "ertubwi", "p", 5.</p><ol><li><p><strong>Split the string:</strong> Since "ertubwi" does not contain the separator "p", the function will split it into a list with one element: <i>["ertubwi"]</i>.</p></li><li><p><strong>Determine the length:</strong> The length of the list is 1.</p></li><li><p><strong>Split the list into two halves:</strong> Since the length is odd, the first half will have 0 elements, and the second half will have 1 element.</p></li><li><p><strong>Reverse the first half:</strong> The first half is empty, so it remains empty.</p></li><li><p><strong>Concatenate the parts:</strong> Concatenate the reversed first half with the second half using the separator "p": <i>"" + "ertubwi" = "ertubwi"</i>.</p></li></ol><p>Therefore, the final answer is: $\boxed{\text{ertubwi}}$.</p></div><h3 id=commonsense-reasoning-strategyqa><strong>Commonsense Reasoning (StrategyQA)</strong><a hidden class=anchor aria-hidden=true href=#commonsense-reasoning-strategyqa>#</a></h3><h4 id=problem-statement-4><strong>Problem Statement:</strong><a hidden class=anchor aria-hidden=true href=#problem-statement-4>#</a></h4><div class=code-box>You are given the following facts:<ol><li><p>Christopher Reeve suffered a serious spinal cord injury that left him a quadriplegic.</p></li><li><p>Christopher Reeve required a portable ventilator after his injury.</p></li><li><p>At one point, Christopher Reeve's treatment was costing $400,000 yearly.</p></li></ol><p>Based on the facts above, answer the following question. Your final answer should be either "True" or "False".</p><p>If he were poor, would Christopher Reeve have lived?</p></div><h4 id=satori39s-response-4><strong>Satori's Response:</strong><a hidden class=anchor aria-hidden=true href=#satori39s-response-4>#</a></h4><div class=code-box>The question asks whether Christopher Reeve would have lived if he were poor.<p>To answer this, we need to consider the impact of his spinal cord injury and the cost of his treatment.</p><p>Christopher Reeve's spinal cord injury left him a quadriplegic, which means he was paralyzed from the neck down. This made it difficult for him to move and perform many daily activities.</p><p>The fact that he required a portable ventilator indicates that he was dependent on mechanical assistance for breathing.</p><p>The cost of his treatment was $400,000 yearly, which is a significant amount of money.</p><p>If Christopher Reeve were poor, he would likely not have been able to afford this level of medical care.</p><p>Without the necessary medical treatment, it is highly unlikely that he would have been able to live a normal life.</p><span style=color:#b8860b>Wait. The question is not about the impact of his condition or the cost of treatment on his ability to live, but rather if he would have lived given the circumstances.</span>
<span style=color:blue>Let's switch to a new solution. Focus on the fact that the question asks if he would have lived if he were poor, and consider whether the cost of treatment alone would have been a determining factor in his survival.</span><p>Let's re-evaluate the question more carefully. The question asks if Christopher Reeve would have lived if he were poor.</p><p>The key factors to consider are:</p><ol><li><p>The severity of his spinal cord injury.</p></li><li><p>The cost of his treatment.</p></li><li><p>The impact of poverty on his ability to access and afford medical care.</p></li></ol><p>Christopher Reeve's spinal cord injury was severe, and he required specialized medical equipment and ongoing treatment. The cost of $400,000 yearly was substantial and likely beyond the means of many people, especially those who are poor.</p><p>If Christopher Reeve were poor, he would likely not have been able to afford this level of medical care. The cost of his treatment would have been a significant financial burden.</p><p>Given the severity of his condition and the high cost of treatment, it is reasonable to conclude that he would not have been able to live a normal life if he were poor.</p><p>Therefore, the final answer is: $\boxed{\text{False}}$.</p></div><h3 id=domain-specific-reasoning-mmlupro---economics><strong>Domain-specific Reasoning (MMLUPro - Economics)</strong><a hidden class=anchor aria-hidden=true href=#domain-specific-reasoning-mmlupro---economics>#</a></h3><h4 id=problem-statement-5><strong>Problem Statement:</strong><a hidden class=anchor aria-hidden=true href=#problem-statement-5>#</a></h4><div class=code-box>What does the Harrod-Domar model, taken with the Keynesian theory of savings, imply for the growth rates of poor countries?<p>(A) Rich and poor countries will experience the same growth rates regardless of savings rates</p><p>(B) Poor countries will have fluctuating growth rates independent of savings</p><p>(C) The level of technology determines growth rates, not savings</p><p>(D) The level of income has no impact on savings</p><p>(E) Savings have no correlation with growth rates in any country</p><p>(F) Rich countries will have low growth rates</p><p>(G) Poor countries will have high growth rates</p><p>(H) High levels of savings will always lead to high growth rates</p><p>(I) Poor countries will have low growth rates</p><p>(J) Poor countries can achieve high growth rates regardless of investment levels</p></div><h4 id=satori39s-response-5><strong>Satori's Response:</strong><a hidden class=anchor aria-hidden=true href=#satori39s-response-5>#</a></h4><div class=code-box>The Harrod-Domar model is an economic model that relates the growth rate of an economy to its savings rate and its capital-output ratio. The model suggests that the growth rate of an economy is determined by the ratio of savings to capital, and that this ratio is influenced by the capital-output ratio.
<span style=color:#b8860b>Wait. Let's clarify this. The Harrod-Domar model indeed relates the growth rate to the savings rate and the capital-output ratio, which is a correct starting point for analyzing the growth rates of poor countries.</span><p>The Harrod-Domar model states that the growth rate $ g $ of an economy is given by:</p><p>$$ g = \frac{s}{k} $$</p><p>where $ s $ is the savings rate and $ k $ is the capital-output ratio.</p><span style=color:#b8860b>Wait. The explanation of the Harrod-Domar model is correct but the step does not directly address the implications for poor countries in the context of the Keynesian theory of savings.
<span style=color:blue>Let's switch to a new solution. Start by explaining how the Harrod-Domar model, when combined with the Keynesian theory of savings, suggests that higher savings rates in poor countries can lead to higher growth rates due to increased investment.</span><p>The Harrod-Domar model suggests that the growth rate of an economy is determined by the ratio of savings to capital. According to the Keynesian theory of savings, countries with higher savings rates will have more funds available for investment.</p><p>In poor countries, where the capital stock is often lower relative to their population, a higher savings rate can lead to a higher capital stock over time. This increase in capital stock can then support higher levels of output and, consequently, higher growth rates.</p><span style=color:#b8860b>Wait. The explanation does not directly address the options provided in the question, which are specific to the growth rates of poor countries.
<span style=color:blue>Let's switch to a new solution. Analyze each option to determine which one best fits the implications of the Harrod-Domar model and the Keynesian theory of savings for poor countries.</span><p>Let's re-evaluate the options given:</p><p>(A) Rich and poor countries will experience the same growth rates regardless of savings rates.</p><ul><li>This is not true because the Harrod-Domar model suggests that growth rates are determined by the savings rate and the capital-output ratio, which can differ between rich and poor countries.</li></ul><p>(B) Poor countries will have fluctuating growth rates independent of savings.</p><ul><li>This is not supported by the Harrod-Domar model, which suggests that growth rates are directly related to savings rates.</li></ul><p>(C) The level of technology determines growth rates, not savings.</p><ul><li>This is not true according to the Harrod-Domar model, which explicitly states that growth rates are determined by the savings rate and the capital-output ratio.</li></ul><p>(D) The level of income has no impact on savings.</p><ul><li>This is not true according to the Keynesian theory of savings, which suggests that higher income can lead to higher savings.</li></ul><p>(E) Savings have no correlation with growth rates in any country.</p><ul><li>This is not true according to the Harrod-Domar model, which shows a direct relationship between savings and growth.</li></ul><p>(F) Rich countries will have low growth rates.</p><ul><li>This is not necessarily true. Rich countries may have higher growth rates if they have high savings rates and a low capital-output ratio.</li></ul><p>(G) Poor countries will have high growth rates.</p><ul><li>This is not necessarily true. Poor countries may have lower growth rates if they have low savings rates and a high capital-output ratio.</li></ul><p>(H) High levels of savings will always lead to high growth rates.</p><ul><li>This is not true because the Harrod-Domar model also considers the capital-output ratio. High savings rates alone are not sufficient for high growth rates.</li></ul><p>(I) Poor countries will have low growth rates.</p><ul><li>This is a possible implication, but it depends on the specific savings and capital-output ratios of poor countries.</li></ul><p>(J) Poor countries can achieve high growth rates regardless of investment levels.</p><ul><li>This is not true according to the Harrod-Domar model, which suggests that high growth rates require high savings rates and a low capital-output ratio.</li></ul><p>Given the analysis, the most accurate statement based on the Harrod-Domar model and the Keynesian theory of savings is that poor countries can achieve high growth rates if they have high savings rates and a low capital-output ratio.</p><p>Therefore, the final answer is: $\boxed{I}$.</p></div><h2 id=commitment-to-open-source-community><strong>Commitment to Open-Source Community</strong><a hidden class=anchor aria-hidden=true href=#commitment-to-open-source-community>#</a></h2><p>We aim to contribute to open-source communities by supporting the development of new research methods that can enhance LLM reasoning capabilities. We plan to provide the following resources:</p><ul><li>Technique details (our research paper)</li><li>Synthetic data for format tuning</li><li>Reinforcement learning (RL) training data</li><li>Code for multi-agent data synthesis</li><li>Code for RL training</li></ul><h2 id=satori-team-members><strong>Satori Team Members</strong><a hidden class=anchor aria-hidden=true href=#satori-team-members>#</a></h2><h3 id=core-contributors><strong>Core Contributors</strong><a hidden class=anchor aria-hidden=true href=#core-contributors>#</a></h3><ul><li><a href=https://maohaos2.github.io/Maohao/>Maohao Shen</a>, MIT</li><li><a href=https://chaoscodes.github.io/>Guangtao Zeng</a>, SUTD</li><li><a href=https://zhentingqi.github.io/>Zhenting Qi</a>, Harvard</li></ul><h3 id=contributors><strong>Contributors</strong><a hidden class=anchor aria-hidden=true href=#contributors>#</a></h3><p>*: Project lead</p><ul><li>Zhang-Wei Hong, MIT</li><li>Zhenfang Chen, MIT-IBM Watson AI Lab</li><li>Wei Lu, SUTD</li><li>Gregory W. Wornell, MIT</li><li>Subhro Das, MIT-IBM Watson AI Lab</li><li>David Cox, MIT-IBM Watson AI Lab</li><li>Chuang Gan*, UMass, MIT-IBM Watson AI Lab</li></ul><h2 id=contact-information><strong>Contact Information</strong><a hidden class=anchor aria-hidden=true href=#contact-information>#</a></h2><p>For questions, please:</p><ul><li>Raise an issue in our GitHub repository</li><li>Contact us at:<ul><li><a href=mailto:satori2025@outlook.com>satori2025@outlook.com</a></li></ul></li></ul><h2 id=citation><strong>Citation</strong><a hidden class=anchor aria-hidden=true href=#citation>#</a></h2><p>(TODO)</p><pre tabindex=0><code>@article{TBD,
  title={Satori: Reinforcement Learning with Chain-of-Action-Thought Enhances LLM Reasoning via Autoregressive Search},
  author={TBD},
  journal={TBD},
  year={2025}
}
</code></pre></div></article></main><footer class=footer><span>&copy; 2025 <a href=https://satori-reasoning.github.io/>Satori</a></span>
<span>Powered by
<a href=https://gohugo.io/ rel="noopener noreferrer" target=_blank>Hugo</a>
</span><span>Acknowledgment to
<a href=https://qwenlm.github.io/ rel="noopener noreferrer" target=_blank>Qwen</a></span></footer><a href=#top aria-label="go to top" title="Go to Top (Alt + G)" class=top-link id=top-link accesskey=g><svg viewBox="0 0 12 8" fill="currentcolor"><path d="M12 8H0l6-8z"/></svg>
</a><script>let menu=document.getElementById("menu");menu&&(menu.scrollLeft=localStorage.getItem("menu-scroll-position"),menu.onscroll=function(){localStorage.setItem("menu-scroll-position",menu.scrollLeft)}),document.querySelectorAll('a[href^="#"]').forEach(e=>{e.addEventListener("click",function(e){e.preventDefault();var t=this.getAttribute("href").substr(1);window.matchMedia("(prefers-reduced-motion: reduce)").matches?document.querySelector(`[id='${decodeURIComponent(t)}']`).scrollIntoView():document.querySelector(`[id='${decodeURIComponent(t)}']`).scrollIntoView({behavior:"smooth"}),t==="top"?history.replaceState(null,null," "):history.pushState(null,null,`#${t}`)})})</script><script>var mybutton=document.getElementById("top-link");window.onscroll=function(){document.body.scrollTop>800||document.documentElement.scrollTop>800?(mybutton.style.visibility="visible",mybutton.style.opacity="1"):(mybutton.style.visibility="hidden",mybutton.style.opacity="0")},mybutton.oncontextmenu=e=>{e.preventDefault(),document.querySelectorAll(".example-container").forEach(e=>{e.style.backgroundColor="unset"}),document.querySelectorAll(".example-content").forEach(e=>{e.style.display="block",e.style.backgroundColor="var(--code-bg)",e.style.marginBottom="var(--modal-gap)"}),document.querySelectorAll(".next-button").forEach(e=>{e.style.display="none"})}</script><script>document.querySelectorAll("pre > code").forEach(e=>{const n=e.parentNode.parentNode,t=document.createElement("button");t.classList.add("copy-code"),t.innerHTML="copy";function s(){t.innerHTML="copied!",setTimeout(()=>{t.innerHTML="copy"},2e3)}t.addEventListener("click",t=>{if("clipboard"in navigator){navigator.clipboard.writeText(e.textContent),s();return}const n=document.createRange();n.selectNodeContents(e);const o=window.getSelection();o.removeAllRanges(),o.addRange(n);try{document.execCommand("copy"),s()}catch{}o.removeRange(n)}),n.classList.contains("highlight")?n.appendChild(t):n.parentNode.firstChild==n||(e.parentNode.parentNode.parentNode.parentNode.parentNode.nodeName=="TABLE"?e.parentNode.parentNode.parentNode.parentNode.parentNode.appendChild(t):e.parentNode.appendChild(t))})</script></body></html>