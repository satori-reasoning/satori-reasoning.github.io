<!doctype html><html lang=en dir=auto><head><meta charset=utf-8><meta http-equiv=X-UA-Compatible content="IE=edge"><meta name=viewport content="width=device-width,initial-scale=1,shrink-to-fit=no"><meta name=robots content="index, follow"><title>Satori-SWE: Evolutionary Test-Time Scaling for Sample-Efficient Software Engineering | Satori</title>
<meta name=keywords content><meta name=description content="Paper
Github
Huggingface

Introduction
LLMs perform well on coding benchmarks like LiveCodeBench but struggle with real-world software engineering (SWE) tasks (Jimenez et al. 2024). Even large models like Claude reach only around 60% accuracy on SWE-bench, despite using carefully engineered prompting pipelines (Xia et al. 2024). Smaller models (under 100B parameters) perform significantly worse, typically scoring below 10% in zero-shot settings and plateauing around 30% after supervised fine-tuning (SFT) (Pan et al. 2024, Xie et al. 2025) on GitHub issue datasets. Improving the performance of these small models remains a key challenge for practical deployment, where repeatedly querying large models is often too costly or inefficient."><meta name=author content="Satori Team"><link rel=canonical href=https://satori-reasoning.github.io/blog/satori-swe/><link crossorigin=anonymous href=/assets/css/stylesheet.c1b29e6d10edf125f713e4623590febe0d3346e7116bdddce8e286b8c5e46194.css integrity="sha256-wbKebRDt8SX3E+RiNZD+vg0zRucRa93c6OKGuMXkYZQ=" rel="preload stylesheet" as=style><link rel=icon href=https://satori-reasoning.github.io/img/logo.png><link rel=icon type=image/png sizes=16x16 href=https://satori-reasoning.github.io/favicon-16x16.png><link rel=icon type=image/png sizes=32x32 href=https://satori-reasoning.github.io/favicon-32x32.png><link rel=apple-touch-icon href=https://satori-reasoning.github.io/apple-touch-icon.png><link rel=mask-icon href=https://satori-reasoning.github.io/safari-pinned-tab.svg><meta name=theme-color content="#2e2e33"><meta name=msapplication-TileColor content="#2e2e33"><link rel=alternate hreflang=en href=https://satori-reasoning.github.io/blog/satori-swe/><noscript><style>#theme-toggle,.top-link{display:none}</style></noscript><script defer crossorigin=anonymous src=/js/custom.df2a5734071a3a99040f5e88e6d16d78358fbdef9a5e7389874ac5f2aa2ca86f.js integrity="sha256-3ypXNAcaOpkED16I5tFteDWPve+aXnOJh0rF8qosqG8="></script><link rel=stylesheet href=https://cdn.jsdelivr.net/npm/katex@0.16.3/dist/katex.min.css integrity=sha384-Juol1FqnotbkyZUT5Z7gUPjQ9gzlwCENvUZTpQBAPxtusdwFLRy382PSDx5UUJ4/ crossorigin=anonymous><script defer src=https://cdn.jsdelivr.net/npm/katex@0.16.3/dist/katex.min.js integrity=sha384-97gW6UIJxnlKemYavrqDHSX3SiygeOwIZhwyOKRfSaf0JWKRVj9hLASHgFTzT+0O crossorigin=anonymous></script><script defer src=https://cdn.jsdelivr.net/npm/katex@0.16.3/dist/contrib/auto-render.min.js integrity=sha384-+VBxd3r6XgURycqtZ117nYw44OOcIax56Z4dCRWbxyPt0Koah1uHoK0o4+/RRE05 crossorigin=anonymous></script><script>document.addEventListener("DOMContentLoaded",function(){renderMathInElement(document.body,{delimiters:[{left:"$$",right:"$$",display:!0},{left:"$",right:"$",display:!1},{left:"\\(",right:"\\)",display:!1},{left:"\\[",right:"\\]",display:!0}],throwOnError:!1})})</script><script async src="https://www.googletagmanager.com/gtag/js?id=G-BP9G7L138H"></script><script>var dnt,doNotTrack=!1;if(!1&&(dnt=navigator.doNotTrack||window.doNotTrack||navigator.msDoNotTrack,doNotTrack=dnt=="1"||dnt=="yes"),!doNotTrack){window.dataLayer=window.dataLayer||[];function gtag(){dataLayer.push(arguments)}gtag("js",new Date),gtag("config","G-BP9G7L138H")}</script><meta property="og:title" content="Satori-SWE: Evolutionary Test-Time Scaling for Sample-Efficient Software Engineering"><meta property="og:description" content="Paper
Github
Huggingface

Introduction
LLMs perform well on coding benchmarks like LiveCodeBench but struggle with real-world software engineering (SWE) tasks (Jimenez et al. 2024). Even large models like Claude reach only around 60% accuracy on SWE-bench, despite using carefully engineered prompting pipelines (Xia et al. 2024). Smaller models (under 100B parameters) perform significantly worse, typically scoring below 10% in zero-shot settings and plateauing around 30% after supervised fine-tuning (SFT) (Pan et al. 2024, Xie et al. 2025) on GitHub issue datasets. Improving the performance of these small models remains a key challenge for practical deployment, where repeatedly querying large models is often too costly or inefficient."><meta property="og:type" content="article"><meta property="og:url" content="https://satori-reasoning.github.io/blog/satori-swe/"><meta property="og:image" content="https://satori-reasoning.github.io/%3Clink%20or%20path%20of%20image%20for%20opengraph,%20twitter-cards%3E"><meta property="article:section" content="blog"><meta property="article:published_time" content="2025-05-25T12:00:00+08:00"><meta property="article:modified_time" content="2025-05-25T12:00:00+08:00"><meta property="og:site_name" content="Satori"><meta name=twitter:card content="summary_large_image"><meta name=twitter:image content="https://satori-reasoning.github.io/%3Clink%20or%20path%20of%20image%20for%20opengraph,%20twitter-cards%3E"><meta name=twitter:title content="Satori-SWE: Evolutionary Test-Time Scaling for Sample-Efficient Software Engineering"><meta name=twitter:description content="Paper
Github
Huggingface

Introduction
LLMs perform well on coding benchmarks like LiveCodeBench but struggle with real-world software engineering (SWE) tasks (Jimenez et al. 2024). Even large models like Claude reach only around 60% accuracy on SWE-bench, despite using carefully engineered prompting pipelines (Xia et al. 2024). Smaller models (under 100B parameters) perform significantly worse, typically scoring below 10% in zero-shot settings and plateauing around 30% after supervised fine-tuning (SFT) (Pan et al. 2024, Xie et al. 2025) on GitHub issue datasets. Improving the performance of these small models remains a key challenge for practical deployment, where repeatedly querying large models is often too costly or inefficient."><script type=application/ld+json>{"@context":"https://schema.org","@type":"BreadcrumbList","itemListElement":[{"@type":"ListItem","position":1,"name":"Blog","item":"https://satori-reasoning.github.io/blog/"},{"@type":"ListItem","position":2,"name":"Satori-SWE: Evolutionary Test-Time Scaling for Sample-Efficient Software Engineering","item":"https://satori-reasoning.github.io/blog/satori-swe/"}]}</script><script type=application/ld+json>{"@context":"https://schema.org","@type":"BlogPosting","headline":"Satori-SWE: Evolutionary Test-Time Scaling for Sample-Efficient Software Engineering","name":"Satori-SWE: Evolutionary Test-Time Scaling for Sample-Efficient Software Engineering","description":"Paper Github Huggingface\nIntroduction LLMs perform well on coding benchmarks like LiveCodeBench but struggle with real-world software engineering (SWE) tasks (Jimenez et al. 2024). Even large models like Claude reach only around 60% accuracy on SWE-bench, despite using carefully engineered prompting pipelines (Xia et al. 2024). Smaller models (under 100B parameters) perform significantly worse, typically scoring below 10% in zero-shot settings and plateauing around 30% after supervised fine-tuning (SFT) (Pan et al. 2024, Xie et al. 2025) on GitHub issue datasets. Improving the performance of these small models remains a key challenge for practical deployment, where repeatedly querying large models is often too costly or inefficient.\n","keywords":[],"articleBody":"Paper Github Huggingface\nIntroduction LLMs perform well on coding benchmarks like LiveCodeBench but struggle with real-world software engineering (SWE) tasks (Jimenez et al. 2024). Even large models like Claude reach only around 60% accuracy on SWE-bench, despite using carefully engineered prompting pipelines (Xia et al. 2024). Smaller models (under 100B parameters) perform significantly worse, typically scoring below 10% in zero-shot settings and plateauing around 30% after supervised fine-tuning (SFT) (Pan et al. 2024, Xie et al. 2025) on GitHub issue datasets. Improving the performance of these small models remains a key challenge for practical deployment, where repeatedly querying large models is often too costly or inefficient.\nExisting approaches primarily rely on supervised fine-tuning (SFT) with high-quality data, which is expensive to curate at scale. An alternative is test-time scaling: generating multiple outputs, scoring them using a verifier, and selecting the best one. Although effective, this strategy often requires excessive sampling and costly scoring. This work aims to explore a new research direction: sample-efficient test-time scaling methods that can identify correct solutions with fewer samples. We propose Evolutionary Test-Time Scaling (EvoScale) that treats generation as an evolutionary process. By iteratively refining outputs via selection and mutation, EvoScale shifts the output distribution toward higher-scoring regions. Our approach results in Satori-SWE-32B, a 32B model trained on open-source model and data. Key features of Satori-SWE include:\nA new perspective of formulating test-time scaling as an evolutionary process, improving sample efficiency for software engineering tasks. A novel RL training approach that enables self-evolution, eliminating the need for external reward models or verifiers at inference time. Satori-SWE-32B with EvoScale achieves performance comparable to models exceeding 100B parameters, while requiring only a small number of samples. Our Approach 1. Preliminaries We study the problem of using LMs to resolve real-world GitHub issues, where each issue consists of a textual description and a corresponding code repository. This work follows pipeline-based methods that decompose the task into retrieval and editing. Retrieval refers to identifying the files or functions relevant to the issue, while editing involves generating the code changes needed to resolve it.\nFormally, given an issue description $x$, the goal is to produce a code edit (i.e., patch) $y$ that fixes the bug or implements the requested change. A retrieval model selects a subset code context $C(x) \\subseteq \\mathcal{C}$ from the full codebase $\\mathcal{C}$, and an editing model $\\pi$ generates the patch $y = \\pi(x, C(x))$ that modifies the code context $C(x)$. While retrieval has reached around 70% accuracy in prior work, editing remains the main bottleneck. This work focuses on improving editing performance in the pipeline-based setting, using off-the-shelf localization methods in experiments.\nWhy is test-time scaling sample-inefficient in SWE task? Test-time scaling improves model performance during inference without training. For SWE task, correct solutions exist but are rarely sampled. As for hard issues, the model‚Äôs output distribution is not concentrated around high-scoring regions. The figure below shows reward score distribution of outputs from a SFT model, with high-scoring outputs concentrated in the long tail. Given a sample budget $N$, typical test-time scaling methods in SWE (Pan et al. 2024, Wei et al. 2025) draw $N$ outputs (patches) { ${y_i}$ }$_{i=1}^{N}$ from a frozen editor model $\\pi$, score them with a score function $R$ (e.g., reward model or unit tests), and selects the best one $\\arg\\max_{y_i} R(x, y_i)$. While high-scoring outputs near the mode could be sampled easily, the challenge of test-time scaling is to identify high-scoring outputs from the tail of $\\pi(\\cdot \\mid x, C(x))$. However, doing so typically requires a large sample size $N$, making the process sample-inefficient.\n2. Formulation: Patch Generation as Evolution Our goal is to enable sample-efficient test-time scaling, achieving stronger performance with fewer samples. We propose Evolutionary Test-Time Scaling (EvoScale), which iteratively refines generation by using earlier outputs to guide subsequent sampling. We recast patch generation for a GitHub issue as an evolutionary process and use a LLM as a mutation operator (Shen et al. 2023), leveraging its ability to produce syntactically and semantically valid patches. The objective is to explore the patch space with a small number of samples, identify high-scoring patches, and iteratively refine the generated patches. At each iteration $t$, the LM generates a batch of patches $\\mathcal{Y}^{t+1} = \\{y^{t+1}_1, \\dots, y^{t+1}_M\\}$ conditioned on a set of prior patches $\\mathcal{E}^t$: $\\mathcal{Y}^{t+1} \\sim \\pi(\\cdot \\mid x, C(x), \\mathcal{E}^t)$. We refer to $\\mathcal{E}^t$ as conditioning examples consisting of patches generated at iteration $t$. Following the selection step in evolutionary algorithms, $\\mathcal{E}^t$ could be selected as the top-$K$ patches ranked by a scoring function $R$ (i.e., fitness function in evolutionary algorithms). Note that we find that our model after training can self-evolve without this selector. Can a language model naturally perform mutation? Ideally, the mutation operator should generate patches that improve scores. However, we find that models trained with classical SFT‚Äîconditioned only on the issue and code context‚Äîstruggle to refine existing patches. To this end, we propose a two-stage SFT to overcome this limitation.\n3. Small-scale Mutation SFT We introduce a two-stage supervised fine-tuning (SFT) process: classical SFT followed by mutation SFT. The classical SFT model is first trained and then used to generate conditioning examples for training the mutation SFT model.\nClassical SFT: We fine-tune a base model on inputs consisting of the issue description $x$ and code context $C(x)$, with targets that include a chain-of-thought (CoT) trace and the ground-truth patch, jointly denoted as $y^*_{\\text{SFT}}$. The training objective is: $$ \\begin{align} \\max_{\\pi_{\\text{SFT}}} \\; \\mathbb{E}_{x \\sim \\mathcal{D},\\, y^*_{\\text{SFT}} \\sim \\mu(\\cdot \\mid x, C(x))} \\left[ \\log \\pi_{\\text{SFT}}(y^*_{\\text{SFT}} \\mid x, C(x)) \\right]. \\nonumber \\end{align} $$ We refer to the resulting model $\\pi_{\\text{SFT}}$ as the classical SFT model. Mutation SFT: We fine-tune a second model, initialized from the same base model, using inputs $x$, $C(x)$, and a set of conditioning examples $\\mathcal{E}$ consisting of patches sampled from the classical SFT model $\\pi_{\\text{SFT}}$. The target $y^*_{\\text{M-SFT}}$ includes a CoT trace generated by the teacher model $\\mu$ conditioned on $\\mathcal{E}$, along with the ground-truth patch. The training objective is: $$ \\begin{align} \\max_{\\pi_{\\text{M-SFT}}} \\; \\mathbb{E}_{\\substack{x \\sim \\mathcal{D},\\, \\mathcal{E} \\sim \\pi_{\\text{SFT}}(\\cdot \\mid x, C(x)), y^*_{\\text{M-SFT}} \\sim \\mu(\\cdot \\mid x, C(x), \\mathcal{E})}} \\left[ \\log \\pi_{\\text{M-SFT}}(y^*_{\\text{M-SFT}} \\mid x, C(x), \\mathcal{E}) \\right]. \\nonumber \\end{align} $$ We refer to the resulting model $\\pi_{\\text{M-SFT}}$ as the mutation SFT model. Can SFT Model after the two-stage training learns to self-evolve? Self-evolution requires the model to improve low-scoring patches on its own, without relying on reward models to select high-scoring examples. If so, we could eliminate the selection step (Line 3 in Algorithm), reducing scoring costs and sample usage. However, we find that SFT alone cannot enable self-evolution. We then introduce a reinforcement learning approach that trains the model to self-evolve without scoring or filtering.\n4. Large‚ÄëScale RL for Self‚ÄëEvolution To self-evolve, the model must generate patches that maximize a scoring function $R$, given conditioning examples $\\mathcal{E}$ from previous patches. This setup naturally aligns with reinforcement learning (RL), where a policy $\\pi$ is optimized to maximize expected rewards (i.e., scores) over time. Since our goal is to maximize the reward at the final iteration $T$, a na√Øve RL objective is:\n$$ \\begin{align} \\max_{\\pi} \\mathbb{E}_{y^t \\sim \\pi(\\cdot | x, C(x), \\mathcal{E}^{t-1})}\\Bigl[\\sum_{t=0}^T r_t \\Bigr], \\quad \\text{where} \\quad r_t = \\begin{cases} R(x, y^t), \u0026 t = T \\\\ 0, \u0026 \\text{otherwise} \\end{cases}\\nonumber \\end{align} $$ This objective focuses solely on maximizing the final reward. However, it presents two key challenges: Rewards are sparse, with feedback only at iteration $T$, making learning inefficient (Shen et al. 2025, Lee et al. 2024). Generating full $T$-step trajectories is computationally expensive (Snell et al. 2025). We address the sparse reward challenge using potential-based reward shaping (Ng et al. 1999), where the potential function is defined as $\\Phi(y) = R(x, y)$. The potential reward at step-$t$ is:\n$$ \\begin{align} r_t = \\Phi(y^t) - \\Phi(y^{t-1}) = R(x, y^t) - R(x, y^{t-1}). \\nonumber \\end{align} $$ Unlike the na√Øve formulation, this provides non-zero potential rewards at every step, mitigating the sparse reward challenge. The cumulative potential reward forms a telescoping sum: $\\sum_{t=1}^T r_t = R(x, y^T) - R(x, y^0)$. Since $y^0$ is fixed, maximizing this sum is equivalent to maximizing the final score. In practice, we fine-tune the mutation SFT model $\\pi_{\\text{M-SFT}}$ to maximize the expected potential rewards in score between a newly generated patch $y$ and a previous patch $y^\\prime$ drawn from the conditioning examples $\\mathcal{E}$:\n$$ \\begin{align} \\max_{\\pi_{\\text{RL}}} \\mathbb{E}_{\\substack{y \\sim \\pi_{\\text{RL}}(\\cdot \\mid x, C(x), \\mathcal{E}), y' \\sim \\mathcal{E}}} \\bigl[R(x, y) - R(x, y') - \\lambda F(y)\\bigr]. \\nonumber \\end{align} $$ This objective encourages the model to generate patches that consistently improve upon previous ones. To ensure the outputs follow the required syntax, we incorporate a formatting penalty term $F$ (string matching and syntax checking) into the reward function. Benchmarking Performance on SWE‚ÄëBench‚ÄëVerified We use the Qwen2.5-Coder-32B-Instruct model as our base model for training Satori-SWE-32B. Through the two-stage SFT training and RL training, Satori-SWE-32B outperforms all small-scale models under greedy decoding, while achieving comparable performance with current SOTA SWE-RL with smaller model scale (32B v.s. 70B), much fewer training data (30K v.s. million-scale) and test-time scaling samples (50 v.s. 500).\nModel Params Best@N Accuracy (%) GPT‚Äë4o (Agentless)‚Äì138.8 Claude 3.5 (Agentless)‚Äì150.8 DeepSeek‚ÄëV3 (Agentless)‚Äì‚Äì42.0 SWE‚ÄëFixer72‚ÄØB130.2 SWE‚ÄëGym‚Äë32B32‚ÄØB120.6 SWE‚ÄëGym‚Äë32B32‚ÄØB1632.0 Llama‚Äë3 SWE‚ÄëRL70‚ÄØB8037.0 Llama‚Äë3 SWE‚ÄëRL70‚ÄØB50041.0 Satori‚ÄëSWE‚Äë32B32‚ÄØB135.8 Satori‚ÄëSWE‚Äë32B32‚ÄØB1038.9 Satori‚ÄëSWE‚Äë32B32‚ÄØB2540.2 Satori‚ÄëSWE‚Äë32B32‚ÄØB5041.6 Anaysis of SWE-Satori We further present a comprehensive analysis of the proposed EvoScale approach. To simplify our analysis, we use ground-truth localization (retrieval) and focus on the code editing part.\nCan LLMs Iteratively Evolve without Mutation SFT Training? First, we investigate whether the mutation SFT is necessary for LLMs to learn how to iteratively improve their generations. Specifically, we fine-tune base LLMs using either classical SFT (without conditional generation) or mutation SFT. As shown in Figure, models trained with classical SFT fail to naturally improve their outputs when conditioned on previous samples. In contrast, mutation SFT enables the model to iteratively improve under the guidance of a reward model. The performance of the mutation SFT model at later iterations can surpass the classical SFT model by scaling up the samples (e.g., Best@40). Moreover, this iterative refinement capability can be learned effectively even with a small number of training data.\nRL Enables Self-evolve Capability. While mutation SFT model demonstrates evolutionary behavior when guided by a reward model, we further examine whether it can self-evolve without such guidance. Specifically, instead of selecting the top-$K$ candidates to ensure generation quality, we allow the model to generate $M=K=5$ random samples for the next iteration of conditional generation. However, the SFT model fails to learn self-evolution without reward model selection. Interestingly, RL training significantly improves the SFT model in two key aspects. First, RL substantially boosts the model‚Äôs greedy performance, surpassing even the Best@$N$ performance of 30 randomly generated samples from the SFT model. Second, we observe that the RL-trained model exhibits strong self-evolution capability: even when conditioned on its random outputs, the model can self-refine and improve performance across iterations without reward model guidance.\nDo our SFT and RL Models Monotonically Improve Reward Scores over Iterations? We further analyze the evolutionary behavior of the SFT and RL models by measuring the average reward score of the patch samples generated at each iteration. As shown in the Figure, although the SFT model learns to iteratively improve reward scores, it relies on the reward model to select high-quality conditioning examples to achieve significant improvements. In contrast, the RL model trained with potential-based reward, naturally learns to self-evolve without any external guidance. Its reward scores improve monotonically across iterations.\nEvolutionary Test-time Scaling v.s. Other Test-time Scaling Methods. Next, we further compare evolutionary test-time scaling with other test-time scaling methods. Starting from the RL model, we first randomly sample $N={5, 10, 15, 20, 25, 50}$ patch samples and let the reward model and unit tests select the best sample among the subsets. Also starting from the RL model, we let the model perform self-evolution with $K=5$ samples per iteration, up to four iterations ($20$ samples in total). The test-time scaling results presented in the Figure demonstrate both efficiency and effectiveness of evolutionary test-time scaling.\nDemo Examples The core idea of EvoScale is to use a mix of correct and incorrect patches as context to generate a correct patch. In this part, we present an example below to illustrate the capability.\nIssue Description Contains.as_set returns Contains: \u003e\u003e\u003e Contains(x, Reals).as_set() Contains(x, Reals) This is wrong because Contains is not a set (it‚Äôs a boolean).\nIt causes failures elsewhere because Contains lacks as_relational (since it isn‚Äôt a set). For example\n(from https://github.com/sympy/sympy/pull/14965#discussion_r205281989):\n\u003e\u003e\u003e Piecewise((6, Contains(x, Reals)), (7, True)) Traceback (most recent call last): File \"\", line 1, in File \"./sympy/functions/elementary/piecewise.py\", line 136, in __new__ r = cls.eval(*newargs) File \"./sympy/functions/elementary/piecewise.py\", line 185, in eval c = c.as_set().as_relational(x) AttributeError: 'Contains' object has no attribute 'as_relational' Conditional Patches Conditional patch 1 ‚úÖ diff --git a/sympy/sets/contains.py b/sympy/sets/contains.py index 0b1e933..3fd780b 100644 --- a/sympy/sets/contains.py +++ b/sympy/sets/contains.py @@ -45,4 +45,6 @@ class Contains(BooleanFunction): def as_set(self): - raise NotImplementedError() + # The set being contained in is the only set we can return + # from a Contains object in any meaningful way + return self.args[1] Conditional patch 2 ‚úÖ diff --git a/sympy/sets/contains.py b/sympy/sets/contains.py index 0b1e933..4c089aa 100644 --- a/sympy/sets/contains.py +++ b/sympy/sets/contains.py @@ -45,4 +45,5 @@ class Contains(BooleanFunction): def as_set(self): - raise NotImplementedError() + # The set that is being tested for contains + return self.args[1] Conditional patch 3 ‚úÖ diff --git a/sympy/sets/contains.py b/sympy/sets/contains.py index 0b1e933..985c88e 100644 --- a/sympy/sets/contains.py +++ b/sympy/sets/contains.py @@ -45,4 +45,6 @@ class Contains(BooleanFunction): def as_set(self): - raise NotImplementedError() + # The set being contained in is the set representation + # of Contains objects + return self.args[1] Conditional patch 4 ‚úÖ diff --git a/sympy/sets/contains.py b/sympy/sets/contains.py index 0b1e933..e732791 100644 --- a/sympy/sets/contains.py +++ b/sympy/sets/contains.py @@ -45,4 +45,6 @@ class Contains(BooleanFunction): def as_set(self): - raise NotImplementedError() + # Contains(x, S) means \"x is in S\", so as_set returns S + return self.args[1] Conditional patch 5 ‚ùå diff --git a/sympy/sets/contains.py b/sympy/sets/contains.py index 0b1e933..4980364 100644 --- a/sympy/sets/contains.py +++ b/sympy/sets/contains.py @@ -45,4 +45,22 @@ class Contains(BooleanFunction): def as_set(self): + # try to evaluate Contains(x, S) + try: + res = self._eval_evalf() + if res == S.true: + return S.UniversalSet + elif res == S.false: + return S.EmptySet + except (TypeError, AttributeError): + pass + # fallback raise NotImplementedError() Evolve Response Thinking Process 1. Peer Patch Review: Patch 1: This patch correctly identifies that the set being contained in (self.args[1]) is the most meaningful set representation of a Contains object. The implementation is simple and correct.\nPatch 2: Similar to Patch 1 but with slightly different wording in the comment. The functionality is correct but the comment could be more precise about why this is the right approach.\nPatch 3: This patch adds a note about potential invalid sets, which isn‚Äôt strictly necessary since the issue is about the basic behavior. The core implementation is correct but the comment is overly cautious.\nPatch 4: This patch provides a clear explanation of why returning the set (self.args[1]) is the correct behavior - because Contains(x, S) means ‚Äúx is in S‚Äù, so the set representation is just S. This is a correct and well-explained solution.\nPatch 5: This patch takes a different approach by trying to evaluate the Contains expression and return either the universal or empty set based on the result. While creative, this doesn‚Äôt match the simple and correct solution of just returning the set being contained in. The approach is overly complex for this simple issue.\nIssue Analysis:\nThe issue occurs when Contains.as_set() returns the Contains object itself instead of a proper set. This causes problems because other code expects a set with as_relational() method. The correct behavior should be to return the set that x is being tested for containment in (self.args[1]), since that‚Äôs the only meaningful set representation of a Contains object. Solution Approach:\nThe simplest and most correct solution is to modify as_set() to return self.args[1], which is the set being tested for containment. This matches the mathematical interpretation that Contains(x, S) means ‚Äúx is in S‚Äù, so the set representation is just S. No other evaluation or special cases are needed since the set being contained in is the only meaningful set representation of a Contains object. Code Localization:\nThe issue is in sympy/sets/contains.py in the Contains class. The problematic method is as_set() which currently raises NotImplementedError. The fix is to modify this method to return self.args[1]. Evolved Patch ‚úÖ diff --git a/sympy/sets/contains.py b/sympy/sets/contains.py index 0b1e933..e732791 100644 --- a/sympy/sets/contains.py +++ b/sympy/sets/contains.py @@ -45,4 +45,6 @@ class Contains(BooleanFunction): def as_set(self): - raise NotImplementedError() + # Contains(x, S) means \"x is in S\", so as_set returns S + return self.args[1] Commitment to Open-Source Community We plan to provide the following resources:\nüìú Paper with detailed implementation details in Appendix. üß© SFT data with CoT, Mutation SFT data with CoT, and RL data. ‚öôÔ∏è Code for EvoScale Inference Piepline \u0026 RL training framework. üèãÔ∏è‚Äç‚ôÇÔ∏è Model Checkpoints: Code Editing Model (SFT): Satori‚ÄëSWE‚ÄëSFT-32B Code Editing Model (RL): Satori‚ÄëSWE‚ÄëRL-32B Code Editing Reward Model: Satori‚ÄëSWE‚ÄëRM-32B Retrieval Model: Satori‚ÄëSWE‚ÄëRetrieval-32B Retrieval Reward Model: Satori‚ÄëSWE‚ÄëRetrieval-RM-32B Stay tuned on our GitHub.\nSatori Team Members $‚Ä†$: Project lead\nCore Contributors Guangtao Zeng, SUTD Maohao Shen, MIT Zhang-Wei Hong$^‚Ä†$, MIT Contributors Delin Chen, UMass Amherst Zhenting Qi, Harvard Wei Lu, SUTD Gregory W. Wornell, MIT Subhro Das, MIT-IBM Watson AI Lab David Cox, MIT-IBM Watson AI Lab Chuang Gan$^‚Ä†$, UMass, MIT-IBM Watson AI Lab Contact Information For questions, please:\nRaise an issue in our GitHub repository Contact us at: satori2025@outlook.com ","wordCount":"2896","inLanguage":"en","datePublished":"2025-05-25T12:00:00+08:00","dateModified":"2025-05-25T12:00:00+08:00","author":{"@type":"Person","name":"Satori Team"},"mainEntityOfPage":{"@type":"WebPage","@id":"https://satori-reasoning.github.io/blog/satori-swe/"},"publisher":{"@type":"Organization","name":"Satori","logo":{"@type":"ImageObject","url":"https://satori-reasoning.github.io/img/logo.png"}}}</script></head><body id=top><script>const hasHeaderBg=!1</script><header class=header><div class=nav-container><nav class=nav><div class=logo><a href=/ accesskey=h title="Satori (Alt + H)"></a><div class=logo-switches></div></div><ul id=menu><li><a href=/ title=Home><span>Home</span></a></li><li><a href=/blog/ title=Blog><span>Blog</span></a></li></ul></nav></div></header><div class=hero-container><div class=hero><h1 class=post-title>Satori-SWE: Evolutionary Test-Time Scaling for Sample-Efficient Software Engineering</h1><div class=post-meta><span title='2025-05-25 12:00:00 +0800 +0800'>May 25, 2025</span>&nbsp;¬∑&nbsp;14 min&nbsp;¬∑&nbsp;2896 words&nbsp;¬∑&nbsp;Satori Team</div></div></div><main class=main><article class=post-single><div class=post-content><p><a href class="btn external" target=_blank>Paper</a>
<a href=https://github.com/satori-reasoning/Satori-SWE class="btn external" target=_blank>Github</a>
<a href=https://huggingface.co/Satori-reasoning class="btn external" target=_blank>Huggingface</a></p><style>.code-box{max-height:350px;overflow-y:auto;padding:10px;font-family:menlo,Consolas,Monaco,courier new,monospace;font-size:18px;line-height:1.5;white-space:pre-wrap;word-wrap:break-word;border-radius:5px}.issue-box{max-height:350px;overflow-y:auto;padding:10px;background-color:#f8f8f8;font-family:menlo,Consolas,Monaco,courier new,monospace;font-size:14px;line-height:1.5;white-space:pre-wrap;word-wrap:break-word;border-radius:5px}.correct_patch{background-color:#cff8ce}.code-box b{font-weight:600!important;color:#111}.code-box b,.code-box strong{font-weight:700!important}</style><h2 id=introduction><strong>Introduction</strong><a hidden class=anchor aria-hidden=true href=#introduction>#</a></h2><p>LLMs perform well on coding benchmarks like LiveCodeBench but struggle with real-world software engineering (SWE) tasks (<a href="https://openreview.net/pdf?id=VTF8yNQM66">Jimenez et al. 2024</a>). Even large models like Claude reach only around 60% accuracy on SWE-bench, despite using carefully engineered prompting pipelines (<a href=https://arxiv.org/pdf/2407.01489>Xia et al. 2024</a>). Smaller models (under 100B parameters) perform significantly worse, typically scoring below 10% in zero-shot settings and plateauing around 30% after supervised fine-tuning (SFT) (<a href=https://arxiv.org/pdf/2412.21139>Pan et al. 2024</a>, <a href=https://arxiv.org/pdf/2501.05040>Xie et al. 2025</a>) on GitHub issue datasets. Improving the performance of these small models remains a key challenge for practical deployment, where repeatedly querying large models is often too costly or inefficient.</p><p>Existing approaches primarily rely on supervised fine-tuning (SFT) with high-quality data, which is expensive to curate at scale. An alternative is test-time scaling: generating multiple outputs, scoring them using a verifier, and selecting the best one. Although effective, this strategy often requires excessive sampling and costly scoring. This work aims to explore a new research direction: sample-efficient test-time scaling methods that can identify correct solutions with fewer samples. We propose <strong>Evolutionary Test-Time Scaling (EvoScale)</strong> that treats generation as an evolutionary process. By iteratively refining outputs via selection and mutation, EvoScale shifts the output distribution toward higher-scoring regions. Our approach results in <strong>Satori-SWE-32B</strong>, a 32B model trained on open-source model and data. Key features of Satori-SWE include:</p><ul><li>A new perspective of formulating <strong>test-time scaling as an evolutionary process</strong>, improving sample efficiency for software engineering tasks.</li><li>A novel <strong>RL training approach that enables self-evolution</strong>, eliminating the need for external reward models or verifiers at inference time.</li><li>Satori-SWE-32B with EvoScale achieves performance <strong>comparable to models exceeding 100B parameters</strong>, while requiring only a small number of samples.</li></ul><h2 id=our-approach><strong>Our Approach</strong><a hidden class=anchor aria-hidden=true href=#our-approach>#</a></h2><h3 id=1-preliminaries><strong>1. Preliminaries</strong><a hidden class=anchor aria-hidden=true href=#1-preliminaries>#</a></h3><div align=center><img src=/img/swe-satori/overview.png alt=overview style=width:70%></div><p>We study the problem of using LMs to resolve real-world GitHub issues, where each issue consists of a textual description and a corresponding code repository. This work follows pipeline-based methods that decompose the task into retrieval and editing. Retrieval refers to identifying the files or functions relevant to the issue, while editing involves generating the code changes needed to resolve it.</p><p>Formally, given an issue description $x$, the goal is to produce a code edit (i.e., patch) $y$ that fixes the bug or implements the requested change. A retrieval model selects a subset code context $C(x) \subseteq \mathcal{C}$ from the full codebase $\mathcal{C}$, and an editing model $\pi$ generates the patch $y = \pi(x, C(x))$ that modifies the code context $C(x)$. While retrieval has reached around 70% accuracy in prior work, editing remains the main bottleneck. This work focuses on improving editing performance in the pipeline-based setting, using off-the-shelf localization methods in experiments.</p><h3 id=why-is-test-time-scaling-sample-inefficient-in-swe-task><strong>Why is test-time scaling sample-inefficient in SWE task?</strong><a hidden class=anchor aria-hidden=true href=#why-is-test-time-scaling-sample-inefficient-in-swe-task>#</a></h3><p>Test-time scaling improves model performance during inference without training. For SWE task, correct solutions exist but are rarely sampled. As for hard issues, the model‚Äôs output distribution is not concentrated around high-scoring regions. The figure below shows reward score distribution of outputs from a SFT model, with high-scoring outputs concentrated in the long tail. Given a sample budget $N$, typical test-time scaling methods in SWE (<a href=https://arxiv.org/pdf/2412.21139>Pan et al. 2024</a>, <a href=https://arxiv.org/pdf/2502.18449>Wei et al. 2025</a>) draw $N$ outputs (patches) { ${y_i}$ }$_{i=1}^{N}$ from a frozen editor model $\pi$, score them with a score function $R$ (e.g., reward model or unit tests), and selects the best one $\arg\max_{y_i} R(x, y_i)$. While high-scoring outputs near the mode could be sampled easily, the challenge of test-time scaling is to identify high-scoring outputs from the tail of $\pi(\cdot \mid x, C(x))$. <strong>However, doing so typically requires a large sample size $N$, making the process sample-inefficient.</strong></p><div align=center><img src=/img/swe-satori/intro_kde.png alt=intro_kde style=width:40%></div><h3 id=2-formulation-patch-generation-as-evolution><strong>2. Formulation: Patch Generation as Evolution</strong><a hidden class=anchor aria-hidden=true href=#2-formulation-patch-generation-as-evolution>#</a></h3><div align=center><img src=/img/swe-satori/inference.png alt=inference></div>Our goal is to enable sample-efficient test-time scaling, achieving stronger performance with fewer samples. We propose <b>Evolutionary Test-Time Scaling (EvoScale), which iteratively refines generation by using earlier outputs to guide subsequent sampling.</b> We recast patch generation for a GitHub issue as an evolutionary process and use a LLM as a <i>mutation operator</i> (<a href=https://arxiv.org/pdf/2305.00593>Shen et al. 2023</a>), leveraging its ability to produce syntactically and semantically valid patches. The objective is to explore the patch space with a small number of samples, identify high-scoring patches, and iteratively refine the generated patches. At each iteration $t$, the LM generates a batch of patches $\mathcal{Y}^{t+1} = \{y^{t+1}_1, \dots, y^{t+1}_M\}$ conditioned on a set of prior patches $\mathcal{E}^t$: $\mathcal{Y}^{t+1} \sim \pi(\cdot \mid x, C(x), \mathcal{E}^t)$. We refer to $\mathcal{E}^t$ as <i>conditioning examples</i> consisting of patches generated at iteration $t$. Following the selection step in evolutionary algorithms, $\mathcal{E}^t$ could be selected as the top-$K$ patches ranked by a scoring function $R$ (i.e., fitness function in evolutionary algorithms). Note that we find that our model after training can self-evolve without this selector.<h3 id=can-a-language-model-naturally-perform-mutation><strong>Can a language model naturally perform mutation?</strong><a hidden class=anchor aria-hidden=true href=#can-a-language-model-naturally-perform-mutation>#</a></h3><p>Ideally, the mutation operator should generate patches that improve scores. However, we find that models trained with classical SFT‚Äîconditioned only on the issue and code context‚Äîstruggle to refine existing patches. To this end, we propose a <strong>two-stage SFT</strong> to overcome this limitation.</p><div align=center><img src=/img/swe-satori/method_algorithm.png alt=method_algorithm></div><h3 id=3-small-scale-mutation-sft><strong>3. Small-scale Mutation SFT</strong><a hidden class=anchor aria-hidden=true href=#3-small-scale-mutation-sft>#</a></h3><p>We introduce a two-stage supervised fine-tuning (SFT) process: classical SFT followed by mutation SFT. The classical SFT model is first trained and then used to generate conditioning examples for training the mutation SFT model.</p><ol><li><strong>Classical SFT:</strong>
We fine-tune a base model on inputs consisting of the issue description $x$ and code context $C(x)$, with targets that include a chain-of-thought (CoT) trace and the ground-truth patch, jointly denoted as $y^*_{\text{SFT}}$. The training objective is:</li></ol><div class=code-box>$$
\begin{align}
\max_{\pi_{\text{SFT}}} \; \mathbb{E}_{x \sim \mathcal{D},\, y^*_{\text{SFT}} \sim \mu(\cdot \mid x, C(x))} \left[ \log \pi_{\text{SFT}}(y^*_{\text{SFT}} \mid x, C(x)) \right]. \nonumber
\end{align}
$$</div>We refer to the resulting model $\pi_{\text{SFT}}$ as the classical SFT model.<ol start=2><li><strong>Mutation SFT:</strong>
We fine-tune a second model, initialized from the same base model, using inputs $x$, $C(x)$, and a set of conditioning examples $\mathcal{E}$ consisting of patches sampled from the classical SFT model $\pi_{\text{SFT}}$. The target $y^*_{\text{M-SFT}}$ includes a CoT trace generated by the teacher model $\mu$ conditioned on $\mathcal{E}$, along with the ground-truth patch. The training objective is:</li></ol><div class=code-box>$$
\begin{align}
\max_{\pi_{\text{M-SFT}}} \; \mathbb{E}_{\substack{x \sim \mathcal{D},\, \mathcal{E} \sim \pi_{\text{SFT}}(\cdot \mid x, C(x)), y^*_{\text{M-SFT}} \sim \mu(\cdot \mid x, C(x), \mathcal{E})}} \left[ \log \pi_{\text{M-SFT}}(y^*_{\text{M-SFT}} \mid x, C(x), \mathcal{E}) \right]. \nonumber
\end{align}
$$</div>We refer to the resulting model $\pi_{\text{M-SFT}}$ as the mutation SFT model.<h3 id=can-sft-model-after-the-two-stage-training-learns-to-self-evolve><strong>Can SFT Model after the two-stage training learns to self-evolve?</strong><a hidden class=anchor aria-hidden=true href=#can-sft-model-after-the-two-stage-training-learns-to-self-evolve>#</a></h3><p>Self-evolution requires the model to improve low-scoring patches on its own, without relying on reward models to select high-scoring examples. If so, we could eliminate the selection step (Line 3 in Algorithm), reducing scoring costs and sample usage. However, we find that <strong>SFT alone cannot enable self-evolution</strong>. We then introduce a reinforcement learning approach that trains the model to self-evolve without scoring or filtering.</p><h3 id=4-largescale-rl-for-selfevolution><strong>4. Large‚ÄëScale RL for Self‚ÄëEvolution</strong><a hidden class=anchor aria-hidden=true href=#4-largescale-rl-for-selfevolution>#</a></h3><p>To <em>self-evolve</em>, the model must generate patches that maximize a scoring function $R$, given conditioning examples $\mathcal{E}$ from previous patches. This setup naturally aligns with reinforcement learning (RL), where a policy $\pi$ is optimized to maximize expected rewards (i.e., scores) over time. Since our goal is to maximize the reward at the final iteration $T$, a na√Øve RL objective is:</p><div class=code-box>$$
\begin{align}
\max_{\pi} \mathbb{E}_{y^t \sim \pi(\cdot | x, C(x), \mathcal{E}^{t-1})}\Bigl[\sum_{t=0}^T r_t \Bigr], \quad \text{where} \quad r_t =
\begin{cases}
R(x, y^t), & t = T \\
0, & \text{otherwise}
\end{cases}\nonumber
\end{align}
$$</div>This objective focuses solely on maximizing the final reward. However, it presents two key challenges:<ul><li><strong>Rewards are sparse</strong>, with feedback only at iteration $T$, making learning inefficient (<a href=https://arxiv.org/pdf/2502.02508>Shen et al. 2025</a>, <a href=https://proceedings.neurips.cc/paper_files/paper/2024/file/f96af360d2a1b1585c3e3a5b82ba4ef7-Paper-Conference.pdf>Lee et al. 2024</a>).</li><li>Generating full $T$-step trajectories is <strong>computationally expensive</strong> (<a href="https://openreview.net/pdf?id=4FWAwZtd2n">Snell et al. 2025</a>).</li></ul><p>We address the sparse reward challenge using <em><strong>potential-based reward shaping</strong></em> (<a href=https://people.eecs.berkeley.edu/~pabbeel/cs287-fa09/readings/NgHaradaRussell-shaping-ICML1999.pdf>Ng et al. 1999</a>), where the potential function is defined as $\Phi(y) = R(x, y)$. The potential reward at step-$t$ is:</p><div class=code-box>$$
\begin{align}
r_t = \Phi(y^t) - \Phi(y^{t-1}) = R(x, y^t) - R(x, y^{t-1}). \nonumber
\end{align}
$$</div>Unlike the na√Øve formulation, this provides non-zero potential rewards at every step, mitigating the sparse reward challenge. The cumulative potential reward forms a telescoping sum: $\sum_{t=1}^T r_t = R(x, y^T) - R(x, y^0)$. Since $y^0$ is fixed, maximizing this sum is equivalent to maximizing the final score.<p>In practice, we fine-tune the mutation SFT model $\pi_{\text{M-SFT}}$ to maximize the expected potential rewards in score between a newly generated patch $y$ and a previous patch $y^\prime$ drawn from the conditioning examples $\mathcal{E}$:</p><div class=code-box>$$
\begin{align}
\max_{\pi_{\text{RL}}} \mathbb{E}_{\substack{y \sim \pi_{\text{RL}}(\cdot \mid x, C(x), \mathcal{E}), y' \sim \mathcal{E}}} \bigl[R(x, y) - R(x, y') - \lambda F(y)\bigr]. \nonumber
\end{align}
$$</div>This objective encourages the model to generate patches that consistently improve upon previous ones. To ensure the outputs follow the required syntax, we incorporate a formatting penalty term $F$ (<i>string matching</i> and <i>syntax checking</i>) into the reward function.<h2 id=benchmarking-performance-on-swebenchverified><strong>Benchmarking Performance on SWE‚ÄëBench‚ÄëVerified</strong><a hidden class=anchor aria-hidden=true href=#benchmarking-performance-on-swebenchverified>#</a></h2><p>We use the <em>Qwen2.5-Coder-32B-Instruct model</em> as our base model for training Satori-SWE-32B. Through the two-stage SFT training and RL training, <strong>Satori-SWE-32B outperforms all small-scale models</strong> under greedy decoding, while achieving comparable performance with current SOTA SWE-RL with smaller model scale (32B v.s. 70B), much fewer training data (30K v.s. million-scale) and test-time scaling samples (50 v.s. 500).</p><table><thead><tr><th style=width:30%;text-align:center>Model</th><th style=width:15%;text-align:center>Params</th><th style=width:15%;text-align:center>Best@N</th><th style=width:20%;text-align:center>Accuracy (%)</th></tr></thead><tbody><tr><td style=text-align:center>GPT‚Äë4o (Agentless)</td><td style=text-align:center>‚Äì</td><td style=text-align:center>1</td><td style=text-align:center>38.8</td></tr><tr><td style=text-align:center>Claude 3.5 (Agentless)</td><td style=text-align:center>‚Äì</td><td style=text-align:center>1</td><td style=text-align:center>50.8</td></tr><tr><td style=text-align:center>DeepSeek‚ÄëV3 (Agentless)</td><td style=text-align:center>‚Äì</td><td style=text-align:center>‚Äì</td><td style=text-align:center>42.0</td></tr><tr><td style=text-align:center>SWE‚ÄëFixer</td><td style=text-align:center>72‚ÄØB</td><td style=text-align:center>1</td><td style=text-align:center>30.2</td></tr><tr><td style=text-align:center>SWE‚ÄëGym‚Äë32B</td><td style=text-align:center>32‚ÄØB</td><td style=text-align:center>1</td><td style=text-align:center>20.6</td></tr><tr><td style=text-align:center>SWE‚ÄëGym‚Äë32B</td><td style=text-align:center>32‚ÄØB</td><td style=text-align:center>16</td><td style=text-align:center>32.0</td></tr><tr><td style=text-align:center>Llama‚Äë3 SWE‚ÄëRL</td><td style=text-align:center>70‚ÄØB</td><td style=text-align:center>80</td><td style=text-align:center>37.0</td></tr><tr><td style=text-align:center>Llama‚Äë3 SWE‚ÄëRL</td><td style=text-align:center>70‚ÄØB</td><td style=text-align:center>500</td><td style=text-align:center>41.0</td></tr><tr><td style=text-align:center><b>Satori‚ÄëSWE‚Äë32B</b></td><td style=text-align:center><b>32‚ÄØB</b></td><td style=text-align:center><b>1</b></td><td style=text-align:center><b>35.8</b></td></tr><tr><td style=text-align:center><b>Satori‚ÄëSWE‚Äë32B</b></td><td style=text-align:center><b>32‚ÄØB</b></td><td style=text-align:center><b>10</b></td><td style=text-align:center><b>38.9</b></td></tr><tr><td style=text-align:center><b>Satori‚ÄëSWE‚Äë32B</b></td><td style=text-align:center><b>32‚ÄØB</b></td><td style=text-align:center><b>25</b></td><td style=text-align:center><b>40.2</b></td></tr><tr><td style=text-align:center><b>Satori‚ÄëSWE‚Äë32B</b></td><td style=text-align:center><b>32‚ÄØB</b></td><td style=text-align:center><b>50</b></td><td style=text-align:center><b>41.6</b></td></tr></tbody></table><h2 id=anaysis-of-swe-satori><strong>Anaysis of SWE-Satori</strong><a hidden class=anchor aria-hidden=true href=#anaysis-of-swe-satori>#</a></h2><p>We further present a comprehensive analysis of the proposed EvoScale approach. To simplify our analysis, we use ground-truth localization (retrieval) and focus on the code editing part.</p><h3 id=can-llms-iteratively-evolve-without-mutation-sft-training><strong>Can LLMs Iteratively Evolve without Mutation SFT Training?</strong><a hidden class=anchor aria-hidden=true href=#can-llms-iteratively-evolve-without-mutation-sft-training>#</a></h3><p>First, we investigate whether the mutation SFT is necessary for LLMs to learn how to iteratively improve their generations. Specifically, we fine-tune base LLMs using either classical SFT (without conditional generation) or mutation SFT. As shown in Figure, models trained with classical SFT fail to naturally improve their outputs when conditioned on previous samples. In contrast, mutation SFT enables the model to iteratively improve under the guidance of a reward model. The performance of the mutation SFT model at later iterations can surpass the classical SFT model by scaling up the samples (e.g., Best@40). <b>Moreover, this iterative refinement capability can be learned effectively even with a small number of training data.</b></p><div align=center><img src=/img/swe-satori/sft.png alt=sft style=width:60%></div><h3 id=rl-enables-self-evolve-capability><strong>RL Enables Self-evolve Capability.</strong><a hidden class=anchor aria-hidden=true href=#rl-enables-self-evolve-capability>#</a></h3><p>While mutation SFT model demonstrates evolutionary behavior when guided by a reward model, we further examine whether it can self-evolve without such guidance. Specifically, instead of selecting the top-$K$ candidates to ensure generation quality, we allow the model to generate $M=K=5$ random samples for the next iteration of conditional generation. However, the SFT model fails to learn self-evolution without reward model selection. Interestingly, RL training significantly improves the SFT model in two key aspects. First, <strong>RL substantially boosts the model&rsquo;s greedy performance</strong>, surpassing even the Best@$N$ performance of 30 randomly generated samples from the SFT model. Second, we observe that the <strong>RL-trained model exhibits strong self-evolution capability</strong>: even when conditioned on its random outputs, the model can self-refine and improve performance across iterations without reward model guidance.</p><div align=center><img src=/img/swe-satori/ablation_rl_evolution.png alt=ablation_rl_evolution style=width:60%></div><h3 id=do-our-sft-and-rl-models-monotonically-improve-reward-scores-over-iterations><strong>Do our SFT and RL Models Monotonically Improve Reward Scores over Iterations?</strong><a hidden class=anchor aria-hidden=true href=#do-our-sft-and-rl-models-monotonically-improve-reward-scores-over-iterations>#</a></h3><p>We further analyze the evolutionary behavior of the SFT and RL models by measuring the average reward score of the patch samples generated at each iteration. As shown in the Figure, although the SFT model learns to iteratively improve reward scores, it relies on the reward model to select high-quality conditioning examples to achieve significant improvements. In contrast, <strong>the RL model trained with potential-based reward, naturally learns to self-evolve without any external guidance.</strong> Its reward scores improve monotonically across iterations.</p><div align=center><img src=/img/swe-satori/rm_score.png alt=rm_score style=width:60%></div><h3 id=evolutionary-test-time-scaling-vs-other-test-time-scaling-methods><strong>Evolutionary Test-time Scaling v.s. Other Test-time Scaling Methods.</strong><a hidden class=anchor aria-hidden=true href=#evolutionary-test-time-scaling-vs-other-test-time-scaling-methods>#</a></h3><p>Next, we further compare evolutionary test-time scaling with other test-time scaling methods. Starting from the RL model, we first randomly sample $N={5, 10, 15, 20, 25, 50}$ patch samples and let the reward model and unit tests select the best sample among the subsets. Also starting from the RL model, we let the model perform self-evolution with $K=5$ samples per iteration, up to four iterations ($20$ samples in total). The test-time scaling results presented in the Figure demonstrate <strong>both efficiency and effectiveness of evolutionary test-time scaling.</strong></p><div align=center><img src=/img/swe-satori/scaling_methods.png alt=scaling_methods style=width:60%></div><h2 id=demo-examples><strong>Demo Examples</strong><a hidden class=anchor aria-hidden=true href=#demo-examples>#</a></h2><p>The core idea of EvoScale is to use a mix of correct and incorrect patches as context to generate a
correct patch. In this part, we present an example below to illustrate the capability.</p><h3 id=issue-description>Issue Description<a hidden class=anchor aria-hidden=true href=#issue-description>#</a></h3><div class=issue-box markdown=1>Contains.as_set returns Contains:<div class=highlight><pre tabindex=0 class=chroma><code class=language-python data-lang=python><span class=line><span class=cl><span class=o>&gt;&gt;&gt;</span> <span class=n>Contains</span><span class=p>(</span><span class=n>x</span><span class=p>,</span> <span class=n>Reals</span><span class=p>)</span><span class=o>.</span><span class=n>as_set</span><span class=p>()</span>
</span></span><span class=line><span class=cl><span class=n>Contains</span><span class=p>(</span><span class=n>x</span><span class=p>,</span> <span class=n>Reals</span><span class=p>)</span>
</span></span></code></pre></div><p>This is <strong>wrong</strong> because <code>Contains</code> is not a set (it&rsquo;s a boolean).<br>It causes failures elsewhere because <code>Contains</code> lacks <code>as_relational</code>
(since it isn‚Äôt a set). For example<br>(from <a href=https://github.com/sympy/sympy/pull/14965#discussion_r205281989>https://github.com/sympy/sympy/pull/14965#discussion_r205281989</a>):</p><pre tabindex=0><code class=language-pytb data-lang=pytb>&gt;&gt;&gt; Piecewise((6, Contains(x, Reals)), (7, True))
Traceback (most recent call last):
  File &#34;&lt;stdin&gt;&#34;, line 1, in &lt;module&gt;
  File &#34;./sympy/functions/elementary/piecewise.py&#34;, line 136, in __new__
    r = cls.eval(*newargs)
  File &#34;./sympy/functions/elementary/piecewise.py&#34;, line 185, in eval
    c = c.as_set().as_relational(x)
AttributeError: &#39;Contains&#39; object has no attribute &#39;as_relational&#39;
</code></pre></div><h3 id=conditional-patches>Conditional Patches<a hidden class=anchor aria-hidden=true href=#conditional-patches>#</a></h3><h4 id=conditional-patch-1-><strong>Conditional patch 1 ‚úÖ</strong><a hidden class=anchor aria-hidden=true href=#conditional-patch-1->#</a></h4><div class=highlight><pre tabindex=0 class=chroma><code class=language-diff data-lang=diff><span class=line><span class=cl><span class=gh>diff --git a/sympy/sets/contains.py b/sympy/sets/contains.py
</span></span></span><span class=line><span class=cl><span class=gh>index 0b1e933..3fd780b 100644
</span></span></span><span class=line><span class=cl><span class=gh></span><span class=gd>--- a/sympy/sets/contains.py
</span></span></span><span class=line><span class=cl><span class=gd></span><span class=gi>+++ b/sympy/sets/contains.py
</span></span></span><span class=line><span class=cl><span class=gi></span><span class=gu>@@ -45,4 +45,6 @@ class Contains(BooleanFunction):
</span></span></span><span class=line><span class=cl><span class=gu></span>     def as_set(self):
</span></span><span class=line><span class=cl><span class=gd>-        raise NotImplementedError()
</span></span></span><span class=line><span class=cl><span class=gd></span><span class=gi>+        # The set being contained in is the only set we can return
</span></span></span><span class=line><span class=cl><span class=gi>+        # from a Contains object in any meaningful way
</span></span></span><span class=line><span class=cl><span class=gi>+        return self.args[1]
</span></span></span></code></pre></div><h4 id=conditional-patch-2-><strong>Conditional patch 2 ‚úÖ</strong><a hidden class=anchor aria-hidden=true href=#conditional-patch-2->#</a></h4><div class=highlight><pre tabindex=0 class=chroma><code class=language-diff data-lang=diff><span class=line><span class=cl><span class=gh>diff --git a/sympy/sets/contains.py b/sympy/sets/contains.py
</span></span></span><span class=line><span class=cl><span class=gh>index 0b1e933..4c089aa 100644
</span></span></span><span class=line><span class=cl><span class=gh></span><span class=gd>--- a/sympy/sets/contains.py
</span></span></span><span class=line><span class=cl><span class=gd></span><span class=gi>+++ b/sympy/sets/contains.py
</span></span></span><span class=line><span class=cl><span class=gi></span><span class=gu>@@ -45,4 +45,5 @@ class Contains(BooleanFunction):
</span></span></span><span class=line><span class=cl><span class=gu></span>     def as_set(self):
</span></span><span class=line><span class=cl><span class=gd>-        raise NotImplementedError()
</span></span></span><span class=line><span class=cl><span class=gd></span><span class=gi>+        # The set that is being tested for contains
</span></span></span><span class=line><span class=cl><span class=gi>+        return self.args[1]
</span></span></span></code></pre></div><h4 id=conditional-patch-3-><strong>Conditional patch 3 ‚úÖ</strong><a hidden class=anchor aria-hidden=true href=#conditional-patch-3->#</a></h4><div class=highlight><pre tabindex=0 class=chroma><code class=language-diff data-lang=diff><span class=line><span class=cl><span class=gh>diff --git a/sympy/sets/contains.py b/sympy/sets/contains.py
</span></span></span><span class=line><span class=cl><span class=gh>index 0b1e933..985c88e 100644
</span></span></span><span class=line><span class=cl><span class=gh></span><span class=gd>--- a/sympy/sets/contains.py
</span></span></span><span class=line><span class=cl><span class=gd></span><span class=gi>+++ b/sympy/sets/contains.py
</span></span></span><span class=line><span class=cl><span class=gi></span><span class=gu>@@ -45,4 +45,6 @@ class Contains(BooleanFunction):
</span></span></span><span class=line><span class=cl><span class=gu></span>     def as_set(self):
</span></span><span class=line><span class=cl><span class=gd>-        raise NotImplementedError()
</span></span></span><span class=line><span class=cl><span class=gd></span><span class=gi>+        # The set being contained in is the set representation
</span></span></span><span class=line><span class=cl><span class=gi>+        # of Contains objects
</span></span></span><span class=line><span class=cl><span class=gi>+        return self.args[1]
</span></span></span></code></pre></div><h4 id=conditional-patch-4-><strong>Conditional patch 4 ‚úÖ</strong><a hidden class=anchor aria-hidden=true href=#conditional-patch-4->#</a></h4><div class=highlight><pre tabindex=0 class=chroma><code class=language-diff data-lang=diff><span class=line><span class=cl><span class=gh>diff --git a/sympy/sets/contains.py b/sympy/sets/contains.py
</span></span></span><span class=line><span class=cl><span class=gh>index 0b1e933..e732791 100644
</span></span></span><span class=line><span class=cl><span class=gh></span><span class=gd>--- a/sympy/sets/contains.py
</span></span></span><span class=line><span class=cl><span class=gd></span><span class=gi>+++ b/sympy/sets/contains.py
</span></span></span><span class=line><span class=cl><span class=gi></span><span class=gu>@@ -45,4 +45,6 @@ class Contains(BooleanFunction):
</span></span></span><span class=line><span class=cl><span class=gu></span>     def as_set(self):
</span></span><span class=line><span class=cl><span class=gd>-        raise NotImplementedError()
</span></span></span><span class=line><span class=cl><span class=gd></span><span class=gi>+        # Contains(x, S) means &#34;x is in S&#34;, so as_set returns S
</span></span></span><span class=line><span class=cl><span class=gi>+        return self.args[1]
</span></span></span></code></pre></div><h4 id=conditional-patch-5-><strong>Conditional patch 5 ‚ùå</strong><a hidden class=anchor aria-hidden=true href=#conditional-patch-5->#</a></h4><div class=highlight><pre tabindex=0 class=chroma><code class=language-diff data-lang=diff><span class=line><span class=cl><span class=gh>diff --git a/sympy/sets/contains.py b/sympy/sets/contains.py
</span></span></span><span class=line><span class=cl><span class=gh>index 0b1e933..4980364 100644
</span></span></span><span class=line><span class=cl><span class=gh></span><span class=gd>--- a/sympy/sets/contains.py
</span></span></span><span class=line><span class=cl><span class=gd></span><span class=gi>+++ b/sympy/sets/contains.py
</span></span></span><span class=line><span class=cl><span class=gi></span><span class=gu>@@ -45,4 +45,22 @@ class Contains(BooleanFunction):
</span></span></span><span class=line><span class=cl><span class=gu></span>     def as_set(self):
</span></span><span class=line><span class=cl><span class=gi>+        # try to evaluate Contains(x, S)
</span></span></span><span class=line><span class=cl><span class=gi>+        try:
</span></span></span><span class=line><span class=cl><span class=gi>+            res = self._eval_evalf()
</span></span></span><span class=line><span class=cl><span class=gi>+            if res == S.true:
</span></span></span><span class=line><span class=cl><span class=gi>+                return S.UniversalSet
</span></span></span><span class=line><span class=cl><span class=gi>+            elif res == S.false:
</span></span></span><span class=line><span class=cl><span class=gi>+                return S.EmptySet
</span></span></span><span class=line><span class=cl><span class=gi>+        except (TypeError, AttributeError):
</span></span></span><span class=line><span class=cl><span class=gi>+            pass
</span></span></span><span class=line><span class=cl><span class=gi>+        # fallback
</span></span></span><span class=line><span class=cl><span class=gi></span>     raise NotImplementedError()
</span></span></code></pre></div><h3 id=evolve-response>Evolve Response<a hidden class=anchor aria-hidden=true href=#evolve-response>#</a></h3><h4 id=thinking-process><strong>Thinking Process</strong><a hidden class=anchor aria-hidden=true href=#thinking-process>#</a></h4><div class=issue-box>1. Peer Patch Review:<ul><li><p>Patch 1: This patch correctly identifies that the set being contained in (self.args[1]) is the most meaningful set representation of a Contains object. The implementation is simple and correct.</p></li><li><p>Patch 2: Similar to Patch 1 but with slightly different wording in the comment. The functionality is correct but the comment could be more precise about why this is the right approach.</p></li><li><p>Patch 3: This patch adds a note about potential invalid sets, which isn&rsquo;t strictly necessary since the issue is about the basic behavior. The core implementation is correct but the comment is overly cautious.</p></li><li><p>Patch 4: This patch provides a clear explanation of why returning the set (self.args[1]) is the correct behavior - because Contains(x, S) means &ldquo;x is in S&rdquo;, so the set representation is just S. This is a correct and well-explained solution.</p></li><li><p>Patch 5: This patch takes a different approach by trying to evaluate the Contains expression and return either the universal or empty set based on the result. While creative, this doesn&rsquo;t match the simple and correct solution of just returning the set being contained in. The approach is overly complex for this simple issue.</p></li></ul><ol start=2><li><p>Issue Analysis:</p><ul><li>The issue occurs when Contains.as_set() returns the Contains object itself instead of a proper set.</li><li>This causes problems because other code expects a set with as_relational() method.</li><li>The correct behavior should be to return the set that x is being tested for containment in (self.args[1]), since that&rsquo;s the only meaningful set representation of a Contains object.</li></ul></li><li><p>Solution Approach:</p><ul><li>The simplest and most correct solution is to modify as_set() to return self.args[1], which is the set being tested for containment.</li><li>This matches the mathematical interpretation that Contains(x, S) means &ldquo;x is in S&rdquo;, so the set representation is just S.</li><li>No other evaluation or special cases are needed since the set being contained in is the only meaningful set representation of a Contains object.</li></ul></li><li><p>Code Localization:</p><ul><li>The issue is in sympy/sets/contains.py in the Contains class.</li><li>The problematic method is as_set() which currently raises NotImplementedError.</li><li>The fix is to modify this method to return self.args[1].</li></ul></li></ol></div><h4 id=evolved-patch-><strong>Evolved Patch</strong> ‚úÖ<a hidden class=anchor aria-hidden=true href=#evolved-patch->#</a></h4><div class=highlight><pre tabindex=0 class=chroma><code class=language-diff data-lang=diff><span class=line><span class=cl><span class=gh>diff --git a/sympy/sets/contains.py b/sympy/sets/contains.py
</span></span></span><span class=line><span class=cl><span class=gh>index 0b1e933..e732791 100644
</span></span></span><span class=line><span class=cl><span class=gh></span><span class=gd>--- a/sympy/sets/contains.py
</span></span></span><span class=line><span class=cl><span class=gd></span><span class=gi>+++ b/sympy/sets/contains.py
</span></span></span><span class=line><span class=cl><span class=gi></span><span class=gu>@@ -45,4 +45,6 @@ class Contains(BooleanFunction):
</span></span></span><span class=line><span class=cl><span class=gu></span>     def as_set(self):
</span></span><span class=line><span class=cl><span class=gd>-        raise NotImplementedError()
</span></span></span><span class=line><span class=cl><span class=gd></span><span class=gi>+        # Contains(x, S) means &#34;x is in S&#34;, so as_set returns S
</span></span></span><span class=line><span class=cl><span class=gi>+        return self.args[1]
</span></span></span></code></pre></div><hr><h2 id=commitment-to-open-source-community><strong>Commitment to Open-Source Community</strong><a hidden class=anchor aria-hidden=true href=#commitment-to-open-source-community>#</a></h2><p>We plan to provide the following resources:</p><ul><li>üìú <strong>Paper</strong> with detailed implementation details in Appendix.</li><li>üß© <a href=https://huggingface.co/datasets/Satori-reasoning/Satori-SWE-two-stage-SFT-data>SFT data with CoT, Mutation SFT data with CoT</a>, and <a href=https://huggingface.co/datasets/Satori-reasoning/SWE-Satori-RL-data>RL data</a>.</li><li>‚öôÔ∏è Code for EvoScale Inference Piepline & RL training framework.</li><li>üèãÔ∏è‚Äç‚ôÇÔ∏è Model Checkpoints:<ul><li>Code Editing Model (SFT): <code>Satori‚ÄëSWE‚ÄëSFT-32B</code></li><li>Code Editing Model (RL): <code>Satori‚ÄëSWE‚ÄëRL-32B</code></li><li>Code Editing Reward Model: <code>Satori‚ÄëSWE‚ÄëRM-32B</code></li><li>Retrieval Model: <code>Satori‚ÄëSWE‚ÄëRetrieval-32B</code></li><li>Retrieval Reward Model: <code>Satori‚ÄëSWE‚ÄëRetrieval-RM-32B</code></li></ul></li></ul><p>Stay tuned on our <a href=https://github.com/satori-reasoning/Satori-SWE>GitHub</a>.</p><hr><h2 id=satori-team-members><strong>Satori Team Members</strong><a hidden class=anchor aria-hidden=true href=#satori-team-members>#</a></h2><p><span>$‚Ä†$</span>: Project lead</p><h3 id=core-contributors><strong>Core Contributors</strong><a hidden class=anchor aria-hidden=true href=#core-contributors>#</a></h3><ul><li><a href=https://chaoscodes.github.io/>Guangtao Zeng, SUTD</a></li><li><a href=https://maohaos2.github.io/Maohao/>Maohao Shen, MIT</a></li><li><a href=https://williamd4112.github.io/>Zhang-Wei Hong<span>$^‚Ä†$</span>, MIT</a></li></ul><h3 id=contributors><strong>Contributors</strong><a hidden class=anchor aria-hidden=true href=#contributors>#</a></h3><ul><li>Delin Chen, UMass Amherst</li><li>Zhenting Qi, Harvard</li><li>Wei Lu, SUTD</li><li>Gregory W. Wornell, MIT</li><li>Subhro Das, MIT-IBM Watson AI Lab</li><li>David Cox, MIT-IBM Watson AI Lab</li><li>Chuang Gan<span>$^‚Ä†$</span>, UMass, MIT-IBM Watson AI Lab</li></ul><hr><h2 id=contact-information><strong>Contact Information</strong><a hidden class=anchor aria-hidden=true href=#contact-information>#</a></h2><p>For questions, please:</p><ul><li>Raise an issue in our GitHub repository</li><li>Contact us at: <a href=mailto:satori2025@outlook.com>satori2025@outlook.com</a></li></ul><hr></div></article></main><footer class=footer><span>&copy; 2025 <a href=https://satori-reasoning.github.io/>Satori</a></span>
<span>Powered by
<a href=https://gohugo.io/ rel="noopener noreferrer" target=_blank>Hugo</a>
</span><span>Acknowledgment to
<a href=https://qwenlm.github.io/ rel="noopener noreferrer" target=_blank>Qwen</a></span></footer><a href=#top aria-label="go to top" title="Go to Top (Alt + G)" class=top-link id=top-link accesskey=g><svg viewBox="0 0 12 8" fill="currentcolor"><path d="M12 8H0l6-8z"/></svg>
</a><script>let menu=document.getElementById("menu");menu&&(menu.scrollLeft=localStorage.getItem("menu-scroll-position"),menu.onscroll=function(){localStorage.setItem("menu-scroll-position",menu.scrollLeft)}),document.querySelectorAll('a[href^="#"]').forEach(e=>{e.addEventListener("click",function(e){e.preventDefault();var t=this.getAttribute("href").substr(1);window.matchMedia("(prefers-reduced-motion: reduce)").matches?document.querySelector(`[id='${decodeURIComponent(t)}']`).scrollIntoView():document.querySelector(`[id='${decodeURIComponent(t)}']`).scrollIntoView({behavior:"smooth"}),t==="top"?history.replaceState(null,null," "):history.pushState(null,null,`#${t}`)})})</script><script>var mybutton=document.getElementById("top-link");window.onscroll=function(){document.body.scrollTop>800||document.documentElement.scrollTop>800?(mybutton.style.visibility="visible",mybutton.style.opacity="1"):(mybutton.style.visibility="hidden",mybutton.style.opacity="0")},mybutton.oncontextmenu=e=>{e.preventDefault(),document.querySelectorAll(".example-container").forEach(e=>{e.style.backgroundColor="unset"}),document.querySelectorAll(".example-content").forEach(e=>{e.style.display="block",e.style.backgroundColor="var(--code-bg)",e.style.marginBottom="var(--modal-gap)"}),document.querySelectorAll(".next-button").forEach(e=>{e.style.display="none"})}</script><script>document.querySelectorAll("pre > code").forEach(e=>{const n=e.parentNode.parentNode,t=document.createElement("button");t.classList.add("copy-code"),t.innerHTML="copy";function s(){t.innerHTML="copied!",setTimeout(()=>{t.innerHTML="copy"},2e3)}t.addEventListener("click",t=>{if("clipboard"in navigator){navigator.clipboard.writeText(e.textContent),s();return}const n=document.createRange();n.selectNodeContents(e);const o=window.getSelection();o.removeAllRanges(),o.addRange(n);try{document.execCommand("copy"),s()}catch{}o.removeRange(n)}),n.classList.contains("highlight")?n.appendChild(t):n.parentNode.firstChild==n||(e.parentNode.parentNode.parentNode.parentNode.parentNode.nodeName=="TABLE"?e.parentNode.parentNode.parentNode.parentNode.parentNode.appendChild(t):e.parentNode.appendChild(t))})</script></body></html>